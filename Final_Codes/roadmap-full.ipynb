{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final ROADMAP Train - Test Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flipped images correction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm # Progress bar\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Path to the CORRUPTED testing videos\n",
    "TEST_DATA_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\n",
    "\n",
    "# Path where we will save the CLEANED videos\n",
    "CLEAN_DATA_DIR = '/kaggle/working/cleaned_testing_videos'\n",
    "\n",
    "MODEL_PATH = 'Final_models\/rotnet_model.pth'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =================================================\n",
    "\n",
    "def clean_dataset():\n",
    "    print(f\"Processing on: {DEVICE}\")\n",
    "    \n",
    "    # 1. Load the Trained RotNet\n",
    "    model = models.resnet18(pretrained=False) # No need to download weights again\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2) # Matches our binary training\n",
    "    \n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Standard transform for the model input\n",
    "    # Note: We do NOT augment here, just resize/norm\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 2. Find all images\n",
    "    # We walk through the directory to keep structure\n",
    "    image_paths = sorted(glob.glob(os.path.join(TEST_DATA_DIR, '**', '*.jpg'), recursive=True))\n",
    "    print(f\"Found {len(image_paths)} frames to process.\")\n",
    "    \n",
    "    # 3. Processing Loop\n",
    "    flip_count = 0\n",
    "    \n",
    "    for img_path in tqdm(image_paths, desc=\"Cleaning\"):\n",
    "        # A. Setup paths\n",
    "        # Get relative path (e.g., \"01/frame_0001.jpg\") to maintain structure\n",
    "        rel_path = os.path.relpath(img_path, TEST_DATA_DIR)\n",
    "        save_path = os.path.join(CLEAN_DATA_DIR, rel_path)\n",
    "        \n",
    "        # Create folder if not exists\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        # B. Predict Rotation\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        input_tensor = preprocess(image).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            label = predicted.item()\n",
    "            \n",
    "        # C. Fix and Save\n",
    "        # Label 0 = Upright (Keep as is)\n",
    "        # Label 1 = Flipped (Needs 180 rotation to fix)\n",
    "        \n",
    "        if label == 1:\n",
    "            # It was detected as Upside Down, so we rotate it -180 (or 180) to fix\n",
    "            fixed_image = image.transpose(Image.FLIP_TOP_BOTTOM) \n",
    "            flip_count += 1\n",
    "        else:\n",
    "            fixed_image = image\n",
    "            \n",
    "        # Save the fixed image\n",
    "        fixed_image.save(save_path)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Cleaning Complete!\")\n",
    "    print(f\"Total Images: {len(image_paths)}\")\n",
    "    print(f\"Images Flipped/Fixed: {flip_count}\")\n",
    "    print(f\"Cleaned dataset saved to: {CLEAN_DATA_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise remover using FastDVDnet (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# 1. WHERE ARE YOUR NOISY FRAMES?\n",
    "# Adjust this to the root folder containing '01', '02', etc.\n",
    "INPUT_ROOT = \"/kaggle/working/cleaned_testing_videos\" \n",
    "\n",
    "# 2. WHERE TO SAVE CLEAN FRAMES?\n",
    "OUTPUT_ROOT = \"/kaggle/working/denoised_dataset_test\"\n",
    "\n",
    "# 3. SETTINGS (The Winning Formula)\n",
    "NOISE_SIGMA = 40 / 255.0  \n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "# =================================================\n",
    "\n",
    "# --- UTILS ---\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
    "\n",
    "def install_and_setup():\n",
    "    if not os.path.exists(\"fastdvdnet\"):\n",
    "        print(\"üõ†Ô∏è Cloning FastDVDnet...\")\n",
    "        os.system(\"git clone https://github.com/m-tassano/fastdvdnet.git\")\n",
    "        os.system(\"pip install tensorboardX\")\n",
    "    \n",
    "    if not os.path.exists(\"fastdvdnet/model/model.pth\"):\n",
    "        os.makedirs(\"fastdvdnet/model\", exist_ok=True)\n",
    "        os.system(\"wget -O fastdvdnet/model/model.pth https://github.com/m-tassano/fastdvdnet/raw/master/model.pth\")\n",
    "\n",
    "# --- DATASET ---\n",
    "class FrameSequenceDataset(Dataset):\n",
    "    def __init__(self, frame_paths):\n",
    "        self.frame_paths = frame_paths\n",
    "        self.total = len(frame_paths)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Sliding Window of 5 frames\n",
    "        indices = [max(0, min(self.total - 1, idx + offset)) for offset in range(-2, 3)]\n",
    "        \n",
    "        frames = []\n",
    "        for i in indices:\n",
    "            path = self.frame_paths[i]\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                img = np.zeros((360, 640, 3), dtype=np.uint8) # Fallback size\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            frames.append(img)\n",
    "            \n",
    "        stack = np.concatenate(frames, axis=2) # (H, W, 15)\n",
    "        tensor = torch.from_numpy(stack).permute(2, 0, 1) # (15, H, W)\n",
    "        return tensor\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "def run_mass_cleaning():\n",
    "    install_and_setup()\n",
    "    \n",
    "    # Import Model\n",
    "    sys.path.append(\"fastdvdnet\")\n",
    "    try:\n",
    "        from models import FastDVDnet\n",
    "    except ImportError:\n",
    "        from fastdvdnet.models import FastDVDnet\n",
    "\n",
    "    # Find all video folders (01, 02, ... 21)\n",
    "    video_folders = sorted(glob.glob(os.path.join(INPUT_ROOT, \"*\")))\n",
    "    # Filter to ensure they are directories\n",
    "    video_folders = [f for f in video_folders if os.path.isdir(f)]\n",
    "    \n",
    "    print(f\"üåç Found {len(video_folders)} videos to clean.\")\n",
    "\n",
    "    # Setup Model Once\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FastDVDnet(num_input_frames=5)\n",
    "    \n",
    "    state_dict = torch.load(\"fastdvdnet/model/model.pth\", map_location=device)\n",
    "    new_state = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"üî• Dual GPU Active\")\n",
    "        model = nn.DataParallel(model)\n",
    "        \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # --- LOOP OVER VIDEOS ---\n",
    "    for vid_path in video_folders:\n",
    "        vid_id = os.path.basename(vid_path)\n",
    "        print(f\"\\nüé¨ Processing Video: {vid_id}\")\n",
    "        \n",
    "        # 1. Get Frames\n",
    "        files = glob.glob(os.path.join(vid_path, \"*\"))\n",
    "        files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        files.sort(key=lambda x: natural_sort_key(os.path.basename(x)))\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"‚ö†Ô∏è Skipping {vid_id} (No images found)\")\n",
    "            continue\n",
    "            \n",
    "        # 2. Setup Output Folder\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, vid_id)\n",
    "        if os.path.exists(save_dir): shutil.rmtree(save_dir)\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "        # 3. Process\n",
    "        dataset = FrameSequenceDataset(files)\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(tqdm(loader, desc=f\"Cleaning {vid_id}\")):\n",
    "                data = data.to(device)\n",
    "                B, C, H, W = data.shape\n",
    "                \n",
    "                noise_sigma = torch.full((B, 1, H, W), NOISE_SIGMA).to(device)\n",
    "                \n",
    "                clean_batch = model(data, noise_sigma)\n",
    "                clean_batch = clean_batch.permute(0, 2, 3, 1).cpu().numpy()\n",
    "                \n",
    "                for i in range(B):\n",
    "                    img = np.clip(clean_batch[i] * 255, 0, 255).astype(np.uint8)\n",
    "                    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Standardized Name: frame_0000.jpg\n",
    "                    global_idx = batch_idx * BATCH_SIZE + i\n",
    "                    save_name = f\"frame_{global_idx:04d}.jpg\"\n",
    "                    \n",
    "                    cv2.imwrite(os.path.join(save_dir, save_name), img_bgr)\n",
    "                    \n",
    "    print(f\"\\n‚úÖ‚úÖ‚úÖ ALL VIDEOS CLEANED! Saved to: {OUTPUT_ROOT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_mass_cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise remover using FastDVDnet (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# 1. WHERE ARE YOUR NOISY FRAMES?\n",
    "# Adjust this to the root folder containing '01', '02', etc.\n",
    "INPUT_ROOT = \"/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos\" \n",
    "\n",
    "# 2. WHERE TO SAVE CLEAN FRAMES?\n",
    "OUTPUT_ROOT = \"/kaggle/working/denoised_dataset_train\"\n",
    "\n",
    "# 3. SETTINGS (The Winning Formula)\n",
    "NOISE_SIGMA = 40 / 255.0  \n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "# =================================================\n",
    "\n",
    "# --- UTILS ---\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
    "\n",
    "def install_and_setup():\n",
    "    if not os.path.exists(\"fastdvdnet\"):\n",
    "        print(\"üõ†Ô∏è Cloning FastDVDnet...\")\n",
    "        os.system(\"git clone https://github.com/m-tassano/fastdvdnet.git\")\n",
    "        os.system(\"pip install tensorboardX\")\n",
    "    \n",
    "    if not os.path.exists(\"fastdvdnet/model/model.pth\"):\n",
    "        os.makedirs(\"fastdvdnet/model\", exist_ok=True)\n",
    "        os.system(\"wget -O fastdvdnet/model/model.pth https://github.com/m-tassano/fastdvdnet/raw/master/model.pth\")\n",
    "\n",
    "# --- DATASET ---\n",
    "class FrameSequenceDataset(Dataset):\n",
    "    def __init__(self, frame_paths):\n",
    "        self.frame_paths = frame_paths\n",
    "        self.total = len(frame_paths)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Sliding Window of 5 frames\n",
    "        indices = [max(0, min(self.total - 1, idx + offset)) for offset in range(-2, 3)]\n",
    "        \n",
    "        frames = []\n",
    "        for i in indices:\n",
    "            path = self.frame_paths[i]\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                img = np.zeros((360, 640, 3), dtype=np.uint8) # Fallback size\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            frames.append(img)\n",
    "            \n",
    "        stack = np.concatenate(frames, axis=2) # (H, W, 15)\n",
    "        tensor = torch.from_numpy(stack).permute(2, 0, 1) # (15, H, W)\n",
    "        return tensor\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "def run_mass_cleaning():\n",
    "    install_and_setup()\n",
    "    \n",
    "    # Import Model\n",
    "    sys.path.append(\"fastdvdnet\")\n",
    "    try:\n",
    "        from models import FastDVDnet\n",
    "    except ImportError:\n",
    "        from fastdvdnet.models import FastDVDnet\n",
    "\n",
    "    # Find all video folders (01, 02, ... 21)\n",
    "    video_folders = sorted(glob.glob(os.path.join(INPUT_ROOT, \"*\")))\n",
    "    # Filter to ensure they are directories\n",
    "    video_folders = [f for f in video_folders if os.path.isdir(f)]\n",
    "    \n",
    "    print(f\"üåç Found {len(video_folders)} videos to clean.\")\n",
    "\n",
    "    # Setup Model Once\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FastDVDnet(num_input_frames=5)\n",
    "    \n",
    "    state_dict = torch.load(\"fastdvdnet/model/model.pth\", map_location=device)\n",
    "    new_state = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"üî• Dual GPU Active\")\n",
    "        model = nn.DataParallel(model)\n",
    "        \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # --- LOOP OVER VIDEOS ---\n",
    "    for vid_path in video_folders:\n",
    "        vid_id = os.path.basename(vid_path)\n",
    "        print(f\"\\nüé¨ Processing Video: {vid_id}\")\n",
    "        \n",
    "        # 1. Get Frames\n",
    "        files = glob.glob(os.path.join(vid_path, \"*\"))\n",
    "        files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        files.sort(key=lambda x: natural_sort_key(os.path.basename(x)))\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"‚ö†Ô∏è Skipping {vid_id} (No images found)\")\n",
    "            continue\n",
    "            \n",
    "        # 2. Setup Output Folder\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, vid_id)\n",
    "        if os.path.exists(save_dir): shutil.rmtree(save_dir)\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "        # 3. Process\n",
    "        dataset = FrameSequenceDataset(files)\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(tqdm(loader, desc=f\"Cleaning {vid_id}\")):\n",
    "                data = data.to(device)\n",
    "                B, C, H, W = data.shape\n",
    "                \n",
    "                noise_sigma = torch.full((B, 1, H, W), NOISE_SIGMA).to(device)\n",
    "                \n",
    "                clean_batch = model(data, noise_sigma)\n",
    "                clean_batch = clean_batch.permute(0, 2, 3, 1).cpu().numpy()\n",
    "                \n",
    "                for i in range(B):\n",
    "                    img = np.clip(clean_batch[i] * 255, 0, 255).astype(np.uint8)\n",
    "                    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Standardized Name: frame_0000.jpg\n",
    "                    global_idx = batch_idx * BATCH_SIZE + i\n",
    "                    save_name = f\"frame_{global_idx:04d}.jpg\"\n",
    "                    \n",
    "                    cv2.imwrite(os.path.join(save_dir, save_name), img_bgr)\n",
    "                    \n",
    "    print(f\"\\n‚úÖ‚úÖ‚úÖ ALL VIDEOS CLEANED! Saved to: {OUTPUT_ROOT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_mass_cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training scrpit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "TRAIN_DIR = '/kaggle/working/denoised_dataset_train'\n",
    "SAVE_PATH = 'roadmap_lite_model.pth'\n",
    "\n",
    "BATCH_SIZE = 16          \n",
    "IMG_SIZE = 256           \n",
    "EPOCHS = 30\n",
    "LR = 0.0003\n",
    "T_STEPS = 4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =================================================\n",
    "\n",
    "# --- 1. COMPONENTS ---\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.shortcut = nn.Conv2d(in_c, out_c, 1, stride, 0)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out = out + self.shortcut(x) \n",
    "        return F.relu(out)\n",
    "\n",
    "class ConvGRUCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, 2 * hidden_dim, kernel_size, 1, self.padding, bias=bias)\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        if cur_state is None:\n",
    "            cur_state = torch.zeros(input_tensor.size(0), self.hidden_dim, input_tensor.size(2), input_tensor.size(3)).to(input_tensor.device)\n",
    "        combined = torch.cat([input_tensor, cur_state], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        gamma, beta = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        reset_gate = torch.sigmoid(gamma)\n",
    "        update_gate = torch.sigmoid(beta)\n",
    "        new_state = (1 - update_gate) * cur_state + update_gate * torch.tanh(input_tensor)\n",
    "        return new_state\n",
    "\n",
    "# --- 2. ROADMAP LITE ---\n",
    "class ROADMAP_LITE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Conv2d(3, 64, 3, 1, 1); self.enc2 = ResBlock(64, 128, 2)\n",
    "        self.enc3 = ResBlock(128, 256, 2); self.enc4 = ResBlock(256, 512, 2)\n",
    "        self.gru1 = ConvGRUCell(512, 512, 3, True)\n",
    "        self.gru2 = ConvGRUCell(256, 256, 3, True)\n",
    "        self.gru3 = ConvGRUCell(128, 128, 3, True)\n",
    "        self.dec1 = ResBlock(512 + 256, 256, 1); self.dec2 = ResBlock(256 + 128, 128, 1)\n",
    "        self.dec3 = ResBlock(128 + 64, 64, 1); self.final = nn.Conv2d(64, 3, 3, 1, 1)\n",
    "    def forward(self, frames):\n",
    "        batch, t_steps, c, h, w = frames.size()\n",
    "        h1, h2, h3 = None, None, None\n",
    "        for t in range(t_steps):\n",
    "            xt = frames[:, t]\n",
    "            f1 = F.relu(self.enc1(xt)); f2 = self.enc2(f1)\n",
    "            f3 = self.enc3(f2); f4 = self.enc4(f3)\n",
    "            h1 = self.gru1(f4, h1); h2 = self.gru2(f3, h2); h3 = self.gru3(f2, h3)\n",
    "        up1 = F.interpolate(h1, scale_factor=2); cat1 = torch.cat([up1, h2], dim=1); d1 = self.dec1(cat1)\n",
    "        up2 = F.interpolate(d1, scale_factor=2); cat2 = torch.cat([up2, h3], dim=1); d2 = self.dec2(cat2)\n",
    "        up3 = F.interpolate(d2, scale_factor=2); cat3 = torch.cat([up3, f1], dim=1); d3 = self.dec3(cat3)\n",
    "        return torch.tanh(self.final(d3))\n",
    "\n",
    "# --- 3. LOSSES ---\n",
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16(pretrained=True).features\n",
    "        for m in vgg.modules():\n",
    "            if isinstance(m, nn.ReLU): m.inplace = False\n",
    "        self.blocks = nn.ModuleList([vgg[:4], vgg[4:9], vgg[9:16], vgg[16:23]])\n",
    "        for param in self.parameters(): param.requires_grad = False\n",
    "    def forward(self, x, y):\n",
    "        loss = 0\n",
    "        for block in self.blocks:\n",
    "            x, y = block(x), block(y)\n",
    "            loss += torch.mean(torch.abs(x - y))\n",
    "        return loss\n",
    "\n",
    "# --- 4. TRAINING ---\n",
    "class RoadmapDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.clips = []\n",
    "        self.transform = transform\n",
    "        for vid in sorted(os.listdir(root)):\n",
    "            path = os.path.join(root, vid)\n",
    "            frames = sorted(glob.glob(os.path.join(path, '*.jpg')))\n",
    "            if len(frames) < T_STEPS + 1: continue\n",
    "            for i in range(len(frames) - T_STEPS):\n",
    "                self.clips.append((frames[i:i+T_STEPS], frames[i+T_STEPS]))\n",
    "    def __len__(self): return len(self.clips)\n",
    "    def __getitem__(self, idx):\n",
    "        in_paths, tgt_path = self.clips[idx]\n",
    "        in_imgs = [self.transform(Image.open(p).convert('RGB')) for p in in_paths]\n",
    "        tgt_img = self.transform(Image.open(tgt_path).convert('RGB'))\n",
    "        return torch.stack(in_imgs), tgt_img\n",
    "\n",
    "def train():\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    print(f\"Training ROADMAP LITE on {torch.cuda.device_count()} GPUs\")\n",
    "    \n",
    "    tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "    loader = DataLoader(RoadmapDataset(TRAIN_DIR, tf), batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    \n",
    "    model = ROADMAP_LITE()\n",
    "    if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    vgg_loss = VGGLoss().to(DEVICE)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    l1_loss = nn.L1Loss(); l2_loss = nn.MSELoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    try:\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            loop = tqdm(loader, desc=f\"Ep {epoch+1}\")\n",
    "            \n",
    "            # Epoch Accumulators\n",
    "            epoch_loss = 0.0\n",
    "            epoch_int = 0.0\n",
    "            epoch_grad = 0.0\n",
    "            epoch_noise = 0.0\n",
    "            count = 0\n",
    "            \n",
    "            for inputs, target in loop:\n",
    "                inputs, target = inputs.to(DEVICE), target.to(DEVICE)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    pred = model(inputs)\n",
    "                    \n",
    "                    # Calculate Individual Losses\n",
    "                    loss_int = l2_loss(pred, target)\n",
    "                    loss_grad = l1_loss(pred, target)\n",
    "                    loss_noise = vgg_loss(pred, target)\n",
    "                    \n",
    "                    total_loss = loss_int + loss_grad + loss_noise\n",
    "\n",
    "                scaler.scale(total_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                # Update Accumulators\n",
    "                b_size = inputs.size(0)\n",
    "                epoch_loss += total_loss.item() * b_size\n",
    "                epoch_int += loss_int.item() * b_size\n",
    "                epoch_grad += loss_grad.item() * b_size\n",
    "                epoch_noise += loss_noise.item() * b_size\n",
    "                count += b_size\n",
    "                \n",
    "                # Update Progress Bar with Breakdown\n",
    "                loop.set_postfix(\n",
    "                    T=total_loss.item(), \n",
    "                    Int=loss_int.item(), \n",
    "                    Grad=loss_grad.item(), \n",
    "                    Noise=loss_noise.item()\n",
    "                )\n",
    "            \n",
    "            # Print Epoch Averages\n",
    "            avg_loss = epoch_loss / count\n",
    "            avg_int = epoch_int / count\n",
    "            avg_grad = epoch_grad / count\n",
    "            avg_noise = epoch_noise / count\n",
    "            print(f\"--> Ep {epoch+1} Avg: Total={avg_loss:.4f} | Int={avg_int:.4f} | Grad={avg_grad:.4f} | Noise={avg_noise:.4f}\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n[!] Keyboard Interrupt detected. Stopping training...\")\n",
    "\n",
    "    # Save logic works for both successful finish AND Interrupt\n",
    "    state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "    torch.save(state, SAVE_PATH)\n",
    "    print(f\"Model saved safely to {SAVE_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Visualiser Script\n",
    "use the weight from above training script or replace by the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# 1. Path to CLEANED TEST VIDEOS\n",
    "CLEAN_TEST_DIR = '/kaggle/working/denoised_dataset_test' \n",
    "\n",
    "# 2. Path to your saved LITE model\n",
    "MODEL_PATH = 'Final_Models/roadmap_lite_model.pth'\n",
    "\n",
    "IMG_SIZE = 256\n",
    "T_STEPS = 4  # Must match what you trained with\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =================================================\n",
    "\n",
    "# --- 1. RE-DEFINE ARCHITECTURE (Must match training exactly) ---\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.shortcut = nn.Conv2d(in_c, out_c, 1, stride, 0)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out = out + self.shortcut(x) \n",
    "        return F.relu(out)\n",
    "\n",
    "class ConvGRUCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, 2 * hidden_dim, kernel_size, 1, self.padding, bias=bias)\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        if cur_state is None:\n",
    "            cur_state = torch.zeros(input_tensor.size(0), self.hidden_dim, input_tensor.size(2), input_tensor.size(3)).to(input_tensor.device)\n",
    "        combined = torch.cat([input_tensor, cur_state], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        gamma, beta = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        reset_gate = torch.sigmoid(gamma)\n",
    "        update_gate = torch.sigmoid(beta)\n",
    "        new_state = (1 - update_gate) * cur_state + update_gate * torch.tanh(input_tensor)\n",
    "        return new_state\n",
    "\n",
    "class ROADMAP_LITE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Conv2d(3, 64, 3, 1, 1); self.enc2 = ResBlock(64, 128, 2)\n",
    "        self.enc3 = ResBlock(128, 256, 2); self.enc4 = ResBlock(256, 512, 2)\n",
    "        \n",
    "        # Lite Paths (Direct GRU)\n",
    "        self.gru1 = ConvGRUCell(512, 512, 3, True)\n",
    "        self.gru2 = ConvGRUCell(256, 256, 3, True)\n",
    "        self.gru3 = ConvGRUCell(128, 128, 3, True)\n",
    "        \n",
    "        self.dec1 = ResBlock(512 + 256, 256, 1); self.dec2 = ResBlock(256 + 128, 128, 1)\n",
    "        self.dec3 = ResBlock(128 + 64, 64, 1); self.final = nn.Conv2d(64, 3, 3, 1, 1)\n",
    "\n",
    "    def forward(self, frames):\n",
    "        batch, t_steps, c, h, w = frames.size()\n",
    "        h1, h2, h3 = None, None, None\n",
    "        for t in range(t_steps):\n",
    "            xt = frames[:, t]\n",
    "            f1 = F.relu(self.enc1(xt)); f2 = self.enc2(f1)\n",
    "            f3 = self.enc3(f2); f4 = self.enc4(f3)\n",
    "            h1 = self.gru1(f4, h1); h2 = self.gru2(f3, h2); h3 = self.gru3(f2, h3)\n",
    "            \n",
    "        up1 = F.interpolate(h1, scale_factor=2); cat1 = torch.cat([up1, h2], dim=1); d1 = self.dec1(cat1)\n",
    "        up2 = F.interpolate(d1, scale_factor=2); cat2 = torch.cat([up2, h3], dim=1); d2 = self.dec2(cat2)\n",
    "        up3 = F.interpolate(d2, scale_factor=2); cat3 = torch.cat([up3, f1], dim=1); d3 = self.dec3(cat3)\n",
    "        return torch.tanh(self.final(d3))\n",
    "\n",
    "# --- 2. PSNR CALCULATION FUNCTION ---\n",
    "def calc_psnr(pred, target):\n",
    "    # Denormalize first: tanh output is [-1, 1] -> [0, 1]\n",
    "    pred = (pred + 1) / 2\n",
    "    target = (target + 1) / 2\n",
    "    \n",
    "    mse = torch.mean((pred - target) ** 2, dim=[1, 2, 3]) # Mean per image\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "# --- 3. MAIN VISUALIZATION FUNCTION ---\n",
    "def visualize_video(vid_id):\n",
    "    print(f\"Visualizing Video: {vid_id}\")\n",
    "    \n",
    "    # Locate Video\n",
    "    vid_path = os.path.join(CLEAN_TEST_DIR, str(vid_id).zfill(2)) # Handle \"1\" vs \"01\"\n",
    "    if not os.path.exists(vid_path):\n",
    "        # Try finding folder regardless of leading zero\n",
    "        candidates = [d for d in os.listdir(CLEAN_TEST_DIR) if str(int(d)) == str(int(vid_id))]\n",
    "        if not candidates:\n",
    "            print(f\"Error: Video {vid_id} not found in {CLEAN_TEST_DIR}\")\n",
    "            return\n",
    "        vid_path = os.path.join(CLEAN_TEST_DIR, candidates[0])\n",
    "        \n",
    "    frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n",
    "    print(f\"Found {len(frames)} frames.\")\n",
    "    \n",
    "    # Load Model\n",
    "    model = ROADMAP_LITE()\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        state = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "        # Handle DataParallel cleaning\n",
    "        new_state = {k.replace('module.', ''): v for k, v in state.items()}\n",
    "        model.load_state_dict(new_state)\n",
    "    else:\n",
    "        print(\"Model file not found!\")\n",
    "        return\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Transform\n",
    "    tf = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ])\n",
    "    \n",
    "    psnr_list = []\n",
    "    frame_indices = []\n",
    "    \n",
    "    print(\"Running Inference...\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(frames) - T_STEPS):\n",
    "            # Input: T frames\n",
    "            clip = frames[i : i + T_STEPS]\n",
    "            # Target: The NEXT frame (T+1)\n",
    "            target_path = frames[i + T_STEPS]\n",
    "            \n",
    "            input_imgs = [tf(Image.open(p).convert('RGB')) for p in clip]\n",
    "            target_img = tf(Image.open(target_path).convert('RGB')).unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            input_tensor = torch.stack(input_imgs).unsqueeze(0).to(DEVICE) # (1, T, 3, H, W)\n",
    "            \n",
    "            # Predict\n",
    "            pred_img = model(input_tensor)\n",
    "            \n",
    "            # Calculate Quality (PSNR)\n",
    "            psnr = calc_psnr(pred_img, target_img)\n",
    "            psnr_list.append(psnr)\n",
    "            frame_indices.append(i + T_STEPS) # Mapping to the predicted frame index\n",
    "            \n",
    "    # --- 4. NORMALIZE TO ANOMALY SCORE ---\n",
    "    # Paper Formula: S_t = 1 - (R_t - min(R)) / (max(R) - min(R)) \n",
    "    psnr_array = np.array(psnr_list)\n",
    "    min_r = np.min(psnr_array)\n",
    "    max_r = np.max(psnr_array)\n",
    "    \n",
    "    # Inverse: Low PSNR (Bad prediction) -> High Anomaly Score\n",
    "    anomaly_scores = 1.0 - ((psnr_array - min_r) / (max_r - min_r))\n",
    "    \n",
    "    # --- 5. PLOT ---\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot the signal\n",
    "    plt.plot(frame_indices, anomaly_scores, color='#e74c3c', linewidth=2, label='Anomaly Score')\n",
    "    \n",
    "    # Add styling\n",
    "    plt.title(f'Anomaly Detection: Video {vid_id} (ROADMAP Lite)', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Frame Number', fontsize=12)\n",
    "    plt.ylabel('Anomaly Score (0=Normal, 1=Abnormal)', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, label='Threshold Hint')\n",
    "    \n",
    "    # Fill under curve for emphasis\n",
    "    plt.fill_between(frame_indices, anomaly_scores, color='#e74c3c', alpha=0.1)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save and Show\n",
    "    save_name = f'anomaly_plot_video_{vid_id}.png'\n",
    "    plt.savefig(save_name)\n",
    "    print(f\"Plot saved to {save_name}\")\n",
    "    plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# ENTER VIDEO NUMBER HERE\n",
    "# ==========================================\n",
    "target_video = \"10\"  # <--- Change this to whatever video you wish (e.g., \"01\", \"12\")\n",
    "visualize_video(target_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission script generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:03:11.589235Z",
     "iopub.status.busy": "2026-01-01T19:03:11.589057Z",
     "iopub.status.idle": "2026-01-01T19:03:19.723075Z",
     "shell.execute_reply": "2026-01-01T19:03:19.722118Z",
     "shell.execute_reply.started": "2026-01-01T19:03:11.589216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# 1. Path to CLEANED TEST VIDEOS (frame_0000.jpg style)\n",
    "CLEAN_TEST_DIR = '/kaggle/working/denoised_dataset_test' \n",
    "\n",
    "# 2. Path to ORIGINAL NOISY VIDEOS (frame_00939.jpg style) - For ID mapping\n",
    "ORIGINAL_TEST_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\n",
    "\n",
    "# 3. Model Weights\n",
    "MODEL_PATH = '/kaggle/input/vlg-roadmap/pytorch/default/1/roadmap_lite_model.pth'\n",
    "\n",
    "SUBMISSION_FILE = 'submission_roadmap.csv'\n",
    "IMG_SIZE = 256\n",
    "T_STEPS = 4  # Sequence Length\n",
    "BATCH_SIZE = 32 # Can be lower if OOM occurs (ROADMAP is heavier than U-Net)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =================================================\n",
    "\n",
    "# --- 1. MODEL ARCHITECTURE ---\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.shortcut = nn.Conv2d(in_c, out_c, 1, stride, 0)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out = out + self.shortcut(x) \n",
    "        return F.relu(out)\n",
    "\n",
    "class ConvGRUCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, 2 * hidden_dim, kernel_size, 1, self.padding, bias=bias)\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        if cur_state is None:\n",
    "            cur_state = torch.zeros(input_tensor.size(0), self.hidden_dim, input_tensor.size(2), input_tensor.size(3)).to(input_tensor.device)\n",
    "        combined = torch.cat([input_tensor, cur_state], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        gamma, beta = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        reset_gate = torch.sigmoid(gamma)\n",
    "        update_gate = torch.sigmoid(beta)\n",
    "        new_state = (1 - update_gate) * cur_state + update_gate * torch.tanh(input_tensor)\n",
    "        return new_state\n",
    "\n",
    "class ROADMAP_LITE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Conv2d(3, 64, 3, 1, 1); self.enc2 = ResBlock(64, 128, 2)\n",
    "        self.enc3 = ResBlock(128, 256, 2); self.enc4 = ResBlock(256, 512, 2)\n",
    "        self.gru1 = ConvGRUCell(512, 512, 3, True)\n",
    "        self.gru2 = ConvGRUCell(256, 256, 3, True)\n",
    "        self.gru3 = ConvGRUCell(128, 128, 3, True)\n",
    "        self.dec1 = ResBlock(512 + 256, 256, 1); self.dec2 = ResBlock(256 + 128, 128, 1)\n",
    "        self.dec3 = ResBlock(128 + 64, 64, 1); self.final = nn.Conv2d(64, 3, 3, 1, 1)\n",
    "\n",
    "    def forward(self, frames):\n",
    "        # frames shape: (Batch, T, C, H, W)\n",
    "        batch, t_steps, c, h, w = frames.size()\n",
    "        h1, h2, h3 = None, None, None\n",
    "        \n",
    "        # We only need the prediction for the LAST step, but we must run through T steps\n",
    "        # Optimization: We don't need to store intermediates for backprop (inference mode)\n",
    "        for t in range(t_steps):\n",
    "            xt = frames[:, t]\n",
    "            f1 = F.relu(self.enc1(xt)); f2 = self.enc2(f1)\n",
    "            f3 = self.enc3(f2); f4 = self.enc4(f3)\n",
    "            h1 = self.gru1(f4, h1); h2 = self.gru2(f3, h2); h3 = self.gru3(f2, h3)\n",
    "            \n",
    "        # Decode only the final state\n",
    "        up1 = F.interpolate(h1, scale_factor=2); cat1 = torch.cat([up1, h2], dim=1); d1 = self.dec1(cat1)\n",
    "        up2 = F.interpolate(d1, scale_factor=2); cat2 = torch.cat([up2, h3], dim=1); d2 = self.dec2(cat2)\n",
    "        up3 = F.interpolate(d2, scale_factor=2); cat3 = torch.cat([up3, f1], dim=1); d3 = self.dec3(cat3)\n",
    "        return torch.tanh(self.final(d3))\n",
    "\n",
    "# --- 2. DATASET ---\n",
    "class RoadMapDataset(Dataset):\n",
    "    def __init__(self, vid_id, root_dir, t_steps=4, img_size=256):\n",
    "        self.t_steps = t_steps\n",
    "        vid_path = os.path.join(root_dir, vid_id)\n",
    "        self.frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        self.samples = []\n",
    "        # We need T frames to predict T+1\n",
    "        if len(self.frames) > t_steps:\n",
    "            for i in range(len(self.frames) - t_steps):\n",
    "                self.samples.append(i)\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Input: T frames [0, 1, 2, 3]\n",
    "        in_paths = self.frames[idx : idx + self.t_steps]\n",
    "        # Target: Frame [4] (The one we are predicting)\n",
    "        tgt_path = self.frames[idx + self.t_steps]\n",
    "        \n",
    "        imgs = [self.transform(Image.open(p).convert('RGB')) for p in in_paths]\n",
    "        # Stack: (T, C, H, W)\n",
    "        input_seq = torch.stack(imgs, dim=0) \n",
    "        target = self.transform(Image.open(tgt_path).convert('RGB'))\n",
    "        \n",
    "        return input_seq, target\n",
    "\n",
    "# --- UTILS ---\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
    "\n",
    "def extract_frame_id(filename):\n",
    "    match = re.search(r'(\\d+)', os.path.splitext(filename)[0])\n",
    "    if match: return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "# --- 3. MAIN GENERATION ---\n",
    "def generate_roadmap_submission():\n",
    "    print(\"üöÄ Generating ROADMAP Submission...\")\n",
    "    \n",
    "    # 1. Load Model\n",
    "    model = ROADMAP_LITE()\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"‚ùå Model weights not found!\"); return\n",
    "        \n",
    "    st = torch.load(MODEL_PATH, map_location='cpu')\n",
    "    if 'module.' in list(st.keys())[0]: st = {k.replace('module.', ''): v for k, v in st.items()}\n",
    "    model.load_state_dict(st)\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"üî• Using {torch.cuda.device_count()} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model.eval()\n",
    "    \n",
    "    # 2. Iterate Videos\n",
    "    video_folders = sorted([f for f in os.listdir(CLEAN_TEST_DIR) if os.path.isdir(os.path.join(CLEAN_TEST_DIR, f))])\n",
    "    all_results = []\n",
    "    \n",
    "    for vid_id in video_folders:\n",
    "        print(f\"üé¨ Processing Video {vid_id}...\", end=\" \")\n",
    "        \n",
    "        # --- A. Get Original IDs ---\n",
    "        orig_vid_path = os.path.join(ORIGINAL_TEST_DIR, vid_id)\n",
    "        if not os.path.exists(orig_vid_path):\n",
    "             # Fuzzy match for '2' vs '02'\n",
    "             candidates = [d for d in os.listdir(ORIGINAL_TEST_DIR) if str(int(d)) == str(int(vid_id))]\n",
    "             if candidates: orig_vid_path = os.path.join(ORIGINAL_TEST_DIR, candidates[0])\n",
    "        \n",
    "        orig_files = glob.glob(os.path.join(orig_vid_path, '*'))\n",
    "        orig_files = [f for f in orig_files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        orig_files.sort(key=lambda x: natural_sort_key(os.path.basename(x)))\n",
    "        \n",
    "        real_frame_ids = []\n",
    "        for f in orig_files:\n",
    "            fid = extract_frame_id(os.path.basename(f))\n",
    "            if fid is not None: real_frame_ids.append(fid)\n",
    "            \n",
    "        if not real_frame_ids: continue\n",
    "            \n",
    "        # --- B. Inference ---\n",
    "        ds = RoadMapDataset(vid_id, CLEAN_TEST_DIR, T_STEPS, IMG_SIZE)\n",
    "        if len(ds) == 0: continue\n",
    "            \n",
    "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "        \n",
    "        psnr_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in loader:\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                \n",
    "                # Forward\n",
    "                preds = model(inputs) # (B, 3, H, W)\n",
    "                \n",
    "                # Denormalize [-1, 1] -> [0, 1] for PSNR calculation\n",
    "                preds = (preds + 1) / 2\n",
    "                targets = (targets + 1) / 2\n",
    "                \n",
    "                # PSNR Calculation per item in batch\n",
    "                mse = torch.mean((preds - targets) ** 2, dim=[1, 2, 3])\n",
    "                psnr = 10 * torch.log10(1 / mse)\n",
    "                \n",
    "                psnr_list.extend(psnr.cpu().numpy())\n",
    "        \n",
    "        # --- C. Normalization (Paper Formula) ---\n",
    "        psnr_array = np.array(psnr_list)\n",
    "        if len(psnr_array) > 0:\n",
    "            min_r = np.min(psnr_array)\n",
    "            max_r = np.max(psnr_array)\n",
    "            \n",
    "            # Inverse: Low PSNR = High Anomaly\n",
    "            if max_r > min_r:\n",
    "                scores_norm = 1.0 - ((psnr_array - min_r) / (max_r - min_r))\n",
    "            else:\n",
    "                scores_norm = np.zeros_like(psnr_array)\n",
    "        else:\n",
    "            scores_norm = []\n",
    "            \n",
    "        # --- D. Map & Save ---\n",
    "        # Logic: First T_STEPS (4) frames have no score -> 0.0\n",
    "        # Prediction 0 corresponds to Real Frame 4 (5th frame)\n",
    "        \n",
    "        clean_vid_num = int(vid_id)\n",
    "        \n",
    "        for i, real_id in enumerate(real_frame_ids):\n",
    "            if i < T_STEPS:\n",
    "                score = 0.0\n",
    "            else:\n",
    "                pred_idx = i - T_STEPS\n",
    "                if pred_idx < len(scores_norm):\n",
    "                    score = scores_norm[pred_idx]\n",
    "                else:\n",
    "                    score = 0.0\n",
    "            \n",
    "            merged_id = f\"{clean_vid_num}_{real_id}\"\n",
    "            all_results.append({'ID': merged_id, 'Prediction': float(score)})\n",
    "            \n",
    "        print(f\"Done. ({len(real_frame_ids)} frames)\")\n",
    "        \n",
    "    # Save\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Sort\n",
    "    df['temp_vid'] = df['ID'].apply(lambda x: int(x.split('_')[0]))\n",
    "    df['temp_frame'] = df['ID'].apply(lambda x: int(x.split('_')[1]))\n",
    "    df.sort_values(by=['temp_vid', 'temp_frame'], inplace=True)\n",
    "    df.drop(columns=['temp_vid', 'temp_frame'], inplace=True)\n",
    "    \n",
    "    df.to_csv(SUBMISSION_FILE, index=False)\n",
    "    print(f\"\\n‚úÖ‚úÖ‚úÖ ROADMAP SUBMISSION SAVED: {SUBMISSION_FILE}\")\n",
    "    print(df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_roadmap_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid Normalising (good for visualisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "INPUT_CSV = '/kaggle/working/submission_roadmap.csv'\n",
    "OUTPUT_CSV = 'submission_processed.csv'\n",
    "\n",
    "# The \"Center\" of your sigmoid (The threshold between Normal vs Anomaly)\n",
    "# You observed normals are < 0.1, so 0.1 is the tipping point.\n",
    "CENTER = 0.07\n",
    "\n",
    "# How hard to push values to 0 and 1.\n",
    "# Low (e.g. 10) = Gentle S-curve\n",
    "# High (e.g. 50) = Hard Step Function (Almost binary)\n",
    "STEEPNESS = 15\n",
    "# =================================================\n",
    "\n",
    "def sigmoid_transform(x, center, k):\n",
    "    \"\"\"\n",
    "    Custom Sigmoid:\n",
    "    x: Input score array\n",
    "    center: The value that maps to 0.5 (tipping point)\n",
    "    k: Steepness coefficient\n",
    "    \"\"\"\n",
    "    # 1. Shift x so the center is at 0\n",
    "    shifted = x - center\n",
    "    \n",
    "    # 2. Scale by steepness\n",
    "    scaled = k * shifted\n",
    "    \n",
    "    # 3. Apply standard sigmoid: 1 / (1 + e^-x)\n",
    "    return 1 / (1 + np.exp(-scaled))\n",
    "\n",
    "def process_and_visualize():\n",
    "    print(f\"üìÇ Loading {INPUT_CSV}...\")\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    \n",
    "    original_scores = df['Predicted'].values\n",
    "    \n",
    "    # --- APPLY TRANSFORM ---\n",
    "    print(f\"‚ö° Applying Sigmoid (Center={CENTER}, Steepness={STEEPNESS})...\")\n",
    "    new_scores = sigmoid_transform(original_scores, CENTER, STEEPNESS)\n",
    "    \n",
    "    df['Predicted'] = new_scores\n",
    "    \n",
    "    # --- VISUALIZATION (THE \"U\" GRAPH) ---\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Plot 1: Original Distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(original_scores, bins=50, color='blue', alpha=0.7)\n",
    "    plt.axvline(CENTER, color='red', linestyle='--', label=f'Center ({CENTER})')\n",
    "    plt.title(\"Original Scores (Before)\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Processed Distribution (Should look like a U)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(new_scores, bins=50, color='green', alpha=0.7)\n",
    "    plt.title(f\"Processed Scores (After)\\nSteepness: {STEEPNESS}\")\n",
    "    plt.xlabel(\"Score (0=Normal, 1=Anomaly)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"score_distribution_u_graph.png\")\n",
    "    print(\"üì∏ Saved visualization to 'score_distribution_u_graph.png'\")\n",
    "    plt.show()\n",
    "    \n",
    "    # --- SAVE ---\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"‚úÖ Saved processed scores to {OUTPUT_CSV}\")\n",
    "    print(df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_and_visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly window detection (smoothning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Use the file that gave you 0.66\n",
    "INPUT_CSV = 'submission_processed.csv' \n",
    "OUTPUT_CSV = 'submission_boosted_roadmap.csv'\n",
    "\n",
    "# DILATION WINDOW\n",
    "# Avenue is 25fps. Events last ~1-2 seconds.\n",
    "# A window of 25 means we look +/- 12 frames around a peak.\n",
    "WINDOW_SIZE = 20\n",
    "# =================================================\n",
    "\n",
    "def boost_score():\n",
    "    print(f\"Reading {INPUT_CSV}...\")\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    \n",
    "    df['VideoID'] = df['ID'].apply(lambda x: x.split('_')[0])\n",
    "    \n",
    "    final_preds = []\n",
    "    \n",
    "    print(f\"Applying Peak Widening (Window={WINDOW_SIZE})...\")\n",
    "    \n",
    "    for vid, group in df.groupby('VideoID', sort=False):\n",
    "        raw = group['Predicted'].copy()\n",
    "        \n",
    "        # 1. ROLLING MAX (Dilation)\n",
    "        # This spreads the '1.0' scores to their neighbors.\n",
    "        # \"If frame T is bad, then frame T+1 is probably bad too.\"\n",
    "        dilated = raw.rolling(window=WINDOW_SIZE, center=True, min_periods=1).max()\n",
    "        \n",
    "        # 2. LIGHT SMOOTHING (Average)\n",
    "        # Rolling max creates \"blocky\" steps. We smooth the edges slightly.\n",
    "        smoothed = dilated.rolling(window=10, center=True, min_periods=1).mean()\n",
    "        \n",
    "        \n",
    "        \n",
    "        final_preds.extend(smoothed.values)\n",
    "        \n",
    "    df['Predicted'] = final_preds\n",
    "    \n",
    "    # Final Normalize (Safety)\n",
    "    df['Predicted'] = np.clip(df['Predicted'], 0.0, 1.0)\n",
    "    \n",
    "    # --- VISUALIZATION ---\n",
    "    # Let's see how much \"fatter\" the detection became\n",
    "    v05 = df[df['ID'].str.startswith('05_')]\n",
    "    if not v05.empty:\n",
    "        x = v05['ID'].apply(lambda x: int(x.split('_')[1]))\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # We need to reload original to compare\n",
    "        orig = pd.read_csv(INPUT_CSV)\n",
    "        orig_v05 = orig[orig['ID'].str.startswith('05_')]\n",
    "        \n",
    "        plt.plot(x, orig_v05['Predicted'], color='gray', alpha=0.5, label='Original (0.66)')\n",
    "        plt.plot(x, v05['Predicted'], color='green', linewidth=2, label='Widened (Boosted)')\n",
    "        \n",
    "        plt.title(\"Peak Widening Effect (Video 05)\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Save\n",
    "    out = df[['ID', 'Predicted']]\n",
    "    out.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"Saved boosted scores to {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    boost_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is used in final essemble code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final visualiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "INPUT_CSV = '/kaggle/working/submission_boosted_roadmap.csv'\n",
    "THRESHOLD = 0.5 \n",
    "# =================================================\n",
    "\n",
    "def visualize_inline():\n",
    "    print(f\"üìÇ Loading {INPUT_CSV}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: File not found. Did you run the previous step?\")\n",
    "        return\n",
    "\n",
    "    # 1. Parse 'ID' (e.g., '1_939') back into Video and Frame columns\n",
    "    print(\"üîÑ Parsing data...\")\n",
    "    split_data = df['ID'].str.split('_', expand=True)\n",
    "    df['video_id'] = split_data[0].astype(int)\n",
    "    df['frame_id'] = split_data[1].astype(int)\n",
    "    \n",
    "    # 2. Get unique videos\n",
    "    videos = sorted(df['video_id'].unique())\n",
    "    print(f\"üåç Visualizing {len(videos)} videos...\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 3. Loop and Plot Inline\n",
    "    for vid in videos:\n",
    "        # Filter data for this video\n",
    "        vid_df = df[df['video_id'] == vid].sort_values(by='frame_id')\n",
    "        \n",
    "        frames = vid_df['frame_id'].values\n",
    "        scores = vid_df['Predicted'].values\n",
    "        \n",
    "        # Create Plot\n",
    "        plt.figure(figsize=(14, 4)) # Wide and short for better scrolling\n",
    "        \n",
    "        # Plot the Score Line\n",
    "        plt.plot(frames, scores, color='#007acc', linewidth=2, label='Anomaly Score')\n",
    "        \n",
    "        # Fill area under curve for anomalies (Red Zones)\n",
    "        plt.fill_between(frames, scores, THRESHOLD, where=(scores >= THRESHOLD), \n",
    "                         interpolate=True, color='red', alpha=0.3, label='Anomaly Detected')\n",
    "        \n",
    "        # Add Threshold Line\n",
    "        plt.axhline(y=THRESHOLD, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Styling\n",
    "        plt.title(f\"Video {vid}: Anomaly Profile\", fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Frame Number\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(loc='upper right')\n",
    "        \n",
    "        # SHOW PLOT\n",
    "        plt.show()\n",
    "        print(\"\\n\") # Spacing\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_inline()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15067517,
     "sourceId": 126766,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
