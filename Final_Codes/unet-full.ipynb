{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Multi-Scale Unet Model Train - Test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm # Progress bar\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Path to the CORRUPTED testing videos\n",
    "TEST_DATA_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\n",
    "\n",
    "# Path where we will save the CLEANED videos\n",
    "CLEAN_DATA_DIR = '/kaggle/working/cleaned_testing_videos'\n",
    "\n",
    "MODEL_PATH = 'Final_models\\rotnet_model.pth'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =================================================\n",
    "\n",
    "def clean_dataset():\n",
    "    print(f\"Processing on: {DEVICE}\")\n",
    "    \n",
    "    # 1. Load the Trained RotNet\n",
    "    model = models.resnet18(pretrained=False) # No need to download weights again\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2) # Matches our binary training\n",
    "    \n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Standard transform for the model input\n",
    "    # Note: We do NOT augment here, just resize/norm\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 2. Find all images\n",
    "    # We walk through the directory to keep structure\n",
    "    image_paths = sorted(glob.glob(os.path.join(TEST_DATA_DIR, '**', '*.jpg'), recursive=True))\n",
    "    print(f\"Found {len(image_paths)} frames to process.\")\n",
    "    \n",
    "    # 3. Processing Loop\n",
    "    flip_count = 0\n",
    "    \n",
    "    for img_path in tqdm(image_paths, desc=\"Cleaning\"):\n",
    "        # A. Setup paths\n",
    "        # Get relative path (e.g., \"01/frame_0001.jpg\") to maintain structure\n",
    "        rel_path = os.path.relpath(img_path, TEST_DATA_DIR)\n",
    "        save_path = os.path.join(CLEAN_DATA_DIR, rel_path)\n",
    "        \n",
    "        # Create folder if not exists\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        # B. Predict Rotation\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        input_tensor = preprocess(image).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            label = predicted.item()\n",
    "            \n",
    "        # C. Fix and Save\n",
    "        # Label 0 = Upright (Keep as is)\n",
    "        # Label 1 = Flipped (Needs 180 rotation to fix)\n",
    "        \n",
    "        if label == 1:\n",
    "            # It was detected as Upside Down, so we rotate it -180 (or 180) to fix\n",
    "            fixed_image = image.transpose(Image.FLIP_TOP_BOTTOM) \n",
    "            flip_count += 1\n",
    "        else:\n",
    "            fixed_image = image\n",
    "            \n",
    "        # Save the fixed image\n",
    "        fixed_image.save(save_path)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Cleaning Complete!\")\n",
    "    print(f\"Total Images: {len(image_paths)}\")\n",
    "    print(f\"Images Flipped/Fixed: {flip_count}\")\n",
    "    print(f\"Cleaned dataset saved to: {CLEAN_DATA_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise remover using FastDVDnet (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# 1. WHERE ARE YOUR NOISY FRAMES?\n",
    "# Adjust this to the root folder containing '01', '02', etc.\n",
    "INPUT_ROOT = \"/kaggle/working/cleaned_testing_videos\" \n",
    "\n",
    "# 2. WHERE TO SAVE CLEAN FRAMES?\n",
    "OUTPUT_ROOT = \"/kaggle/working/denoised_dataset_test\"\n",
    "\n",
    "# 3. SETTINGS (The Winning Formula)\n",
    "NOISE_SIGMA = 40 / 255.0  \n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "# =================================================\n",
    "\n",
    "# --- UTILS ---\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
    "\n",
    "def install_and_setup():\n",
    "    if not os.path.exists(\"fastdvdnet\"):\n",
    "        print(\"üõ†Ô∏è Cloning FastDVDnet...\")\n",
    "        os.system(\"git clone https://github.com/m-tassano/fastdvdnet.git\")\n",
    "        os.system(\"pip install tensorboardX\")\n",
    "    \n",
    "    if not os.path.exists(\"fastdvdnet/model/model.pth\"):\n",
    "        os.makedirs(\"fastdvdnet/model\", exist_ok=True)\n",
    "        os.system(\"wget -O fastdvdnet/model/model.pth https://github.com/m-tassano/fastdvdnet/raw/master/model.pth\")\n",
    "\n",
    "# --- DATASET ---\n",
    "class FrameSequenceDataset(Dataset):\n",
    "    def __init__(self, frame_paths):\n",
    "        self.frame_paths = frame_paths\n",
    "        self.total = len(frame_paths)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Sliding Window of 5 frames\n",
    "        indices = [max(0, min(self.total - 1, idx + offset)) for offset in range(-2, 3)]\n",
    "        \n",
    "        frames = []\n",
    "        for i in indices:\n",
    "            path = self.frame_paths[i]\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                img = np.zeros((360, 640, 3), dtype=np.uint8) # Fallback size\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            frames.append(img)\n",
    "            \n",
    "        stack = np.concatenate(frames, axis=2) # (H, W, 15)\n",
    "        tensor = torch.from_numpy(stack).permute(2, 0, 1) # (15, H, W)\n",
    "        return tensor\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "def run_mass_cleaning():\n",
    "    install_and_setup()\n",
    "    \n",
    "    # Import Model\n",
    "    sys.path.append(\"fastdvdnet\")\n",
    "    try:\n",
    "        from models import FastDVDnet\n",
    "    except ImportError:\n",
    "        from fastdvdnet.models import FastDVDnet\n",
    "\n",
    "    # Find all video folders (01, 02, ... 21)\n",
    "    video_folders = sorted(glob.glob(os.path.join(INPUT_ROOT, \"*\")))\n",
    "    # Filter to ensure they are directories\n",
    "    video_folders = [f for f in video_folders if os.path.isdir(f)]\n",
    "    \n",
    "    print(f\"üåç Found {len(video_folders)} videos to clean.\")\n",
    "\n",
    "    # Setup Model Once\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FastDVDnet(num_input_frames=5)\n",
    "    \n",
    "    state_dict = torch.load(\"fastdvdnet/model/model.pth\", map_location=device)\n",
    "    new_state = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"üî• Dual GPU Active\")\n",
    "        model = nn.DataParallel(model)\n",
    "        \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # --- LOOP OVER VIDEOS ---\n",
    "    for vid_path in video_folders:\n",
    "        vid_id = os.path.basename(vid_path)\n",
    "        print(f\"\\nüé¨ Processing Video: {vid_id}\")\n",
    "        \n",
    "        # 1. Get Frames\n",
    "        files = glob.glob(os.path.join(vid_path, \"*\"))\n",
    "        files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        files.sort(key=lambda x: natural_sort_key(os.path.basename(x)))\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"‚ö†Ô∏è Skipping {vid_id} (No images found)\")\n",
    "            continue\n",
    "            \n",
    "        # 2. Setup Output Folder\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, vid_id)\n",
    "        if os.path.exists(save_dir): shutil.rmtree(save_dir)\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "        # 3. Process\n",
    "        dataset = FrameSequenceDataset(files)\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(tqdm(loader, desc=f\"Cleaning {vid_id}\")):\n",
    "                data = data.to(device)\n",
    "                B, C, H, W = data.shape\n",
    "                \n",
    "                noise_sigma = torch.full((B, 1, H, W), NOISE_SIGMA).to(device)\n",
    "                \n",
    "                clean_batch = model(data, noise_sigma)\n",
    "                clean_batch = clean_batch.permute(0, 2, 3, 1).cpu().numpy()\n",
    "                \n",
    "                for i in range(B):\n",
    "                    img = np.clip(clean_batch[i] * 255, 0, 255).astype(np.uint8)\n",
    "                    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Standardized Name: frame_0000.jpg\n",
    "                    global_idx = batch_idx * BATCH_SIZE + i\n",
    "                    save_name = f\"frame_{global_idx:04d}.jpg\"\n",
    "                    \n",
    "                    cv2.imwrite(os.path.join(save_dir, save_name), img_bgr)\n",
    "                    \n",
    "    print(f\"\\n‚úÖ‚úÖ‚úÖ ALL VIDEOS CLEANED! Saved to: {OUTPUT_ROOT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_mass_cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise remover using FastDVDnet (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# 1. WHERE ARE YOUR NOISY FRAMES?\n",
    "# Adjust this to the root folder containing '01', '02', etc.\n",
    "INPUT_ROOT = \"/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos\" \n",
    "\n",
    "# 2. WHERE TO SAVE CLEAN FRAMES?\n",
    "OUTPUT_ROOT = \"/kaggle/working/denoised_dataset_train\"\n",
    "\n",
    "# 3. SETTINGS (The Winning Formula)\n",
    "NOISE_SIGMA = 40 / 255.0  \n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "# =================================================\n",
    "\n",
    "# --- UTILS ---\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
    "\n",
    "def install_and_setup():\n",
    "    if not os.path.exists(\"fastdvdnet\"):\n",
    "        print(\"üõ†Ô∏è Cloning FastDVDnet...\")\n",
    "        os.system(\"git clone https://github.com/m-tassano/fastdvdnet.git\")\n",
    "        os.system(\"pip install tensorboardX\")\n",
    "    \n",
    "    if not os.path.exists(\"fastdvdnet/model/model.pth\"):\n",
    "        os.makedirs(\"fastdvdnet/model\", exist_ok=True)\n",
    "        os.system(\"wget -O fastdvdnet/model/model.pth https://github.com/m-tassano/fastdvdnet/raw/master/model.pth\")\n",
    "\n",
    "# --- DATASET ---\n",
    "class FrameSequenceDataset(Dataset):\n",
    "    def __init__(self, frame_paths):\n",
    "        self.frame_paths = frame_paths\n",
    "        self.total = len(frame_paths)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Sliding Window of 5 frames\n",
    "        indices = [max(0, min(self.total - 1, idx + offset)) for offset in range(-2, 3)]\n",
    "        \n",
    "        frames = []\n",
    "        for i in indices:\n",
    "            path = self.frame_paths[i]\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                img = np.zeros((360, 640, 3), dtype=np.uint8) # Fallback size\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            frames.append(img)\n",
    "            \n",
    "        stack = np.concatenate(frames, axis=2) # (H, W, 15)\n",
    "        tensor = torch.from_numpy(stack).permute(2, 0, 1) # (15, H, W)\n",
    "        return tensor\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "def run_mass_cleaning():\n",
    "    install_and_setup()\n",
    "    \n",
    "    # Import Model\n",
    "    sys.path.append(\"fastdvdnet\")\n",
    "    try:\n",
    "        from models import FastDVDnet\n",
    "    except ImportError:\n",
    "        from fastdvdnet.models import FastDVDnet\n",
    "\n",
    "    # Find all video folders (01, 02, ... 21)\n",
    "    video_folders = sorted(glob.glob(os.path.join(INPUT_ROOT, \"*\")))\n",
    "    # Filter to ensure they are directories\n",
    "    video_folders = [f for f in video_folders if os.path.isdir(f)]\n",
    "    \n",
    "    print(f\"üåç Found {len(video_folders)} videos to clean.\")\n",
    "\n",
    "    # Setup Model Once\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FastDVDnet(num_input_frames=5)\n",
    "    \n",
    "    state_dict = torch.load(\"fastdvdnet/model/model.pth\", map_location=device)\n",
    "    new_state = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"üî• Dual GPU Active\")\n",
    "        model = nn.DataParallel(model)\n",
    "        \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # --- LOOP OVER VIDEOS ---\n",
    "    for vid_path in video_folders:\n",
    "        vid_id = os.path.basename(vid_path)\n",
    "        print(f\"\\nüé¨ Processing Video: {vid_id}\")\n",
    "        \n",
    "        # 1. Get Frames\n",
    "        files = glob.glob(os.path.join(vid_path, \"*\"))\n",
    "        files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        files.sort(key=lambda x: natural_sort_key(os.path.basename(x)))\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"‚ö†Ô∏è Skipping {vid_id} (No images found)\")\n",
    "            continue\n",
    "            \n",
    "        # 2. Setup Output Folder\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, vid_id)\n",
    "        if os.path.exists(save_dir): shutil.rmtree(save_dir)\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "        # 3. Process\n",
    "        dataset = FrameSequenceDataset(files)\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(tqdm(loader, desc=f\"Cleaning {vid_id}\")):\n",
    "                data = data.to(device)\n",
    "                B, C, H, W = data.shape\n",
    "                \n",
    "                noise_sigma = torch.full((B, 1, H, W), NOISE_SIGMA).to(device)\n",
    "                \n",
    "                clean_batch = model(data, noise_sigma)\n",
    "                clean_batch = clean_batch.permute(0, 2, 3, 1).cpu().numpy()\n",
    "                \n",
    "                for i in range(B):\n",
    "                    img = np.clip(clean_batch[i] * 255, 0, 255).astype(np.uint8)\n",
    "                    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Standardized Name: frame_0000.jpg\n",
    "                    global_idx = batch_idx * BATCH_SIZE + i\n",
    "                    save_name = f\"frame_{global_idx:04d}.jpg\"\n",
    "                    \n",
    "                    cv2.imwrite(os.path.join(save_dir, save_name), img_bgr)\n",
    "                    \n",
    "    print(f\"\\n‚úÖ‚úÖ‚úÖ ALL VIDEOS CLEANED! Saved to: {OUTPUT_ROOT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_mass_cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training scrpit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "TRAIN_DIR = '/kaggle/working/denoised_dataset_train'\n",
    "SAVE_PATH = 'multiscale_unet_conditional.pth'\n",
    "\n",
    "IMG_SIZE = 256\n",
    "CLIP_LEN = 4     # 4 frames input\n",
    "BATCH_SIZE = 16  # 8 per GPU\n",
    "EPOCHS = 50\n",
    "LR_G = 2e-4\n",
    "LR_D = 2e-5\n",
    "\n",
    "# [cite_start]Loss Weights [cite: 2419, 2782]\n",
    "LAMBDA_INT = 2.0\n",
    "LAMBDA_GD = 1.0\n",
    "LAMBDA_ADV = 0.05\n",
    "LAMBDA_FLOW = 2.0 \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =================================================\n",
    "\n",
    "# --- 1. ARCHITECTURE COMPONENTS (Generator) ---\n",
    "# [Unchanged from previous robust implementation]\n",
    "\n",
    "class AsymmetricConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(AsymmetricConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(kernel_size//2, 0))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(1, kernel_size), padding=(0, kernel_size//2))\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv2(self.relu(self.conv1(x)))))\n",
    "\n",
    "class ResidualSkipConnection(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualSkipConnection, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            AsymmetricConv(channels, channels),\n",
    "            AsymmetricConv(channels, channels)\n",
    "        )\n",
    "        self.shortcut = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.block(x) + self.shortcut(x))\n",
    "\n",
    "class ShortcutInceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ShortcutInceptionModule, self).__init__()\n",
    "        w_6 = out_channels // 6\n",
    "        w_3 = out_channels // 3\n",
    "        w_2 = out_channels - (w_6 + w_3)\n",
    "\n",
    "        self.branch1 = AsymmetricConv(in_channels, w_6)\n",
    "        self.branch2 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3))\n",
    "        self.branch3 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3), AsymmetricConv(w_3, w_2))\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        concat = torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1)\n",
    "        return self.relu(concat + self.shortcut(x))\n",
    "\n",
    "class MultiScaleUNet(nn.Module):\n",
    "    def __init__(self, in_channels=12, out_channels=3):\n",
    "        super(MultiScaleUNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.sim1 = ShortcutInceptionModule(in_channels, 96); self.pool1 = nn.MaxPool2d(2)\n",
    "        self.sim2 = ShortcutInceptionModule(96, 192);         self.pool2 = nn.MaxPool2d(2)\n",
    "        self.sim3 = ShortcutInceptionModule(192, 384);        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.sim4 = ShortcutInceptionModule(384, 768)\n",
    "\n",
    "        # Skip Connections\n",
    "        self.rsc1 = nn.Sequential(*[ResidualSkipConnection(96) for _ in range(4)])\n",
    "        self.rsc2 = nn.Sequential(*[ResidualSkipConnection(192) for _ in range(3)])\n",
    "        self.rsc3 = nn.Sequential(*[ResidualSkipConnection(384) for _ in range(2)])\n",
    "\n",
    "        # Decoder\n",
    "        self.sim5 = ShortcutInceptionModule(768, 384);   self.up1 = nn.ConvTranspose2d(384, 384, 2, 2)\n",
    "        self.sim6 = ShortcutInceptionModule(768, 192);   self.up2 = nn.ConvTranspose2d(192, 192, 2, 2)\n",
    "        self.sim7 = ShortcutInceptionModule(384, 96);    self.up3 = nn.ConvTranspose2d(96, 96, 2, 2)\n",
    "        self.sim8 = ShortcutInceptionModule(192, 96)\n",
    "        self.final = nn.Conv2d(96, out_channels, 3, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.sim1(x);        p1 = self.pool1(e1)\n",
    "        e2 = self.sim2(p1);       p2 = self.pool2(e2)\n",
    "        e3 = self.sim3(p2);       p3 = self.pool3(e3)\n",
    "        e4 = self.sim4(p3)\n",
    "\n",
    "        d1 = self.sim5(e4);       u1 = self.up1(d1)\n",
    "        cat1 = torch.cat([u1, self.rsc3(e3)], dim=1)\n",
    "\n",
    "        d2 = self.sim6(cat1);     u2 = self.up2(d2)\n",
    "        cat2 = torch.cat([u2, self.rsc2(e2)], dim=1)\n",
    "\n",
    "        d3 = self.sim7(cat2);     u3 = self.up3(d3)\n",
    "        cat3 = torch.cat([u3, self.rsc1(e1)], dim=1)\n",
    "\n",
    "        d4 = self.sim8(cat3)\n",
    "        return self.tanh(self.final(d4))\n",
    "\n",
    "# --- 2. CONDITIONAL PATCH DISCRIMINATOR (FIXED) ---\n",
    "class ConditionalPatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=6): # 3 (Current) + 3 (Past Condition)\n",
    "        super(ConditionalPatchDiscriminator, self).__init__()\n",
    "        \n",
    "        def disc_block(in_f, out_f, bn=True):\n",
    "            block = [nn.Conv2d(in_f, out_f, 4, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n",
    "            if bn: block.append(nn.BatchNorm2d(out_f))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *disc_block(in_channels, 64, bn=False), # 128x128\n",
    "            *disc_block(64, 128),                   # 64x64\n",
    "            *disc_block(128, 256),                  # 32x32\n",
    "            nn.Conv2d(256, 1, 4, padding=1)         # 32x32 (PatchGAN Map)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        # Concatenate condition (Last Frame) and target (Current Frame)\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)\n",
    "\n",
    "# --- 3. LOSSES ---\n",
    "def gradient_loss(gen_frames, gt_frames):\n",
    "    def gradient(x):\n",
    "        h_x = x.size()[-2]\n",
    "        w_x = x.size()[-1]\n",
    "        x_h = torch.abs(x[:, :, 1:, :] - x[:, :, :h_x-1, :])\n",
    "        x_w = torch.abs(x[:, :, :, 1:] - x[:, :, :, :w_x-1])\n",
    "        return x_h, x_w\n",
    "    gen_h, gen_w = gradient(gen_frames)\n",
    "    gt_h, gt_w = gradient(gt_frames)\n",
    "    return torch.mean(torch.abs(gen_h - gt_h)) + torch.mean(torch.abs(gen_w - gt_w))\n",
    "\n",
    "def flow_loss(gen_frames, gt_frames, prev_frames):\n",
    "    flow_gen = torch.abs(gen_frames - prev_frames)\n",
    "    flow_gt = torch.abs(gt_frames - prev_frames)\n",
    "    return torch.mean(torch.abs(flow_gen - flow_gt)) # L1 Loss for robustness\n",
    "\n",
    "# --- 4. DATASET ---\n",
    "class AvenueTrainDataset(Dataset):\n",
    "    def __init__(self, root_dir, clip_len=4, img_size=256):\n",
    "        self.clip_len = clip_len\n",
    "        self.samples = []\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        videos = sorted(os.listdir(root_dir))\n",
    "        for vid in videos:\n",
    "            path = os.path.join(root_dir, vid)\n",
    "            if not os.path.isdir(path): continue\n",
    "            frames = sorted(glob.glob(os.path.join(path, '*.jpg')))\n",
    "            if len(frames) < clip_len + 1: continue\n",
    "            \n",
    "            for i in range(len(frames) - clip_len):\n",
    "                self.samples.append(frames[i : i + clip_len + 1])\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        paths = self.samples[idx]\n",
    "        imgs = [self.transform(Image.open(p).convert('RGB')) for p in paths]\n",
    "        \n",
    "        input_seq = torch.cat(imgs[:-1], dim=0) # 12 channels\n",
    "        target_frame = imgs[-1]                 # 3 channels (t+1)\n",
    "        last_input_frame = imgs[-2]             # 3 channels (t) - For Conditioning\n",
    "        \n",
    "        return input_seq, target_frame, last_input_frame\n",
    "\n",
    "# --- 5. TRAINING LOOP (CONDITIONAL GAN) ---\n",
    "def train():\n",
    "    print(f\"Initializing Conditional Multi-scale U-Net Training on {DEVICE}...\")\n",
    "    \n",
    "    # Init Models\n",
    "    generator = MultiScaleUNet().to(DEVICE)\n",
    "    # Discriminator takes 6 channels: 3 (Condition/Last Frame) + 3 (Target/Generated)\n",
    "    discriminator = ConditionalPatchDiscriminator(in_channels=6).to(DEVICE)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        generator = nn.DataParallel(generator)\n",
    "        discriminator = nn.DataParallel(discriminator)\n",
    "        \n",
    "    opt_g = optim.Adam(generator.parameters(), lr=LR_G)\n",
    "    opt_d = optim.Adam(discriminator.parameters(), lr=LR_D)\n",
    "    \n",
    "    criterion_gan = nn.MSELoss() # LSGAN is more stable than BCE\n",
    "    criterion_pixel = nn.MSELoss()\n",
    "    \n",
    "    dataset = AvenueTrainDataset(TRAIN_DIR, CLIP_LEN, IMG_SIZE)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=12, pin_memory=True)\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(EPOCHS):\n",
    "            generator.train(); discriminator.train()\n",
    "            pbar = tqdm(loader, desc=f\"Ep {epoch+1}/{EPOCHS}\")\n",
    "            \n",
    "            for inputs, targets, last_frames in pbar:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "                last_frames = last_frames.to(DEVICE) # Condition for D\n",
    "                \n",
    "                # ==========================\n",
    "                #  Train Discriminator (D)\n",
    "                # ==========================\n",
    "                opt_d.zero_grad()\n",
    "                \n",
    "                # Real: D(LastFrame, RealTarget) -> 1\n",
    "                real_out = discriminator(last_frames, targets)\n",
    "                loss_real = criterion_gan(real_out, torch.ones_like(real_out))\n",
    "                \n",
    "                # Fake: D(LastFrame, FakeTarget) -> 0\n",
    "                fake_frame = generator(inputs)\n",
    "                fake_out = discriminator(last_frames, fake_frame.detach()) # Detach G\n",
    "                loss_fake = criterion_gan(fake_out, torch.zeros_like(fake_out))\n",
    "                \n",
    "                loss_d = 0.5 * (loss_real + loss_fake)\n",
    "                loss_d.backward()\n",
    "                opt_d.step()\n",
    "                \n",
    "                # ==========================\n",
    "                #  Train Generator (G)\n",
    "                # ==========================\n",
    "                opt_g.zero_grad()\n",
    "                \n",
    "                # 1. Adversarial Loss: D(LastFrame, FakeTarget) -> 1\n",
    "                fake_out_g = discriminator(last_frames, fake_frame)\n",
    "                l_adv = criterion_gan(fake_out_g, torch.ones_like(fake_out_g))\n",
    "                \n",
    "                # 2. Pixel Intensity Loss\n",
    "                l_int = criterion_pixel(fake_frame, targets)\n",
    "                \n",
    "                # 3. Gradient Loss\n",
    "                l_gd = gradient_loss(fake_frame, targets)\n",
    "                \n",
    "                # 4. Flow Loss (Temporal Consistency)\n",
    "                l_flow = flow_loss(fake_frame, targets, last_frames)\n",
    "                \n",
    "                # Total Loss\n",
    "                loss_g = (LAMBDA_INT * l_int) + \\\n",
    "                         (LAMBDA_GD * l_gd) + \\\n",
    "                         (LAMBDA_ADV * l_adv) + \\\n",
    "                         (LAMBDA_FLOW * l_flow)\n",
    "                         \n",
    "                loss_g.backward()\n",
    "                opt_g.step()\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'D_loss': f\"{loss_d.item():.4f}\",\n",
    "                    'G_Adv': f\"{l_adv.item():.4f}\",\n",
    "                    'G_Int': f\"{l_int.item():.4f}\",\n",
    "                    'G_Flow': f\"{l_flow.item():.4f}\"\n",
    "                })\n",
    "            \n",
    "            torch.save(generator.module.state_dict(), f\"unet_conditional_ep{epoch}.pth\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining Interrupted! Saving checkpoint...\")\n",
    "        state = generator.module.state_dict() if hasattr(generator, 'module') else generator.state_dict()\n",
    "        torch.save(state, 'INTERRUPTED_conditional.pth')\n",
    "        print(\"Saved safely.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Visualiser Script\n",
    "use the weight from above training script or replace by the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Update this to match your actual file name\n",
    "MODEL_PATH = 'Final_Models\\unet_conditional_ep8.pth' \n",
    "TEST_DIR = '/kaggle/working/denoised_dataset'\n",
    "TARGET_VIDEO = '17' \n",
    "\n",
    "IMG_SIZE = 256\n",
    "CLIP_LEN = 4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =================================================\n",
    "\n",
    "# --- 1. MODEL ARCHITECTURE (Must be defined to load weights) ---\n",
    "class AsymmetricConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(AsymmetricConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(kernel_size//2, 0))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(1, kernel_size), padding=(0, kernel_size//2))\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, x): return self.relu(self.bn(self.conv2(self.relu(self.conv1(x)))))\n",
    "\n",
    "class ResidualSkipConnection(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualSkipConnection, self).__init__()\n",
    "        self.block = nn.Sequential(AsymmetricConv(channels, channels), AsymmetricConv(channels, channels))\n",
    "        self.shortcut = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x): return self.relu(self.block(x) + self.shortcut(x))\n",
    "\n",
    "class ShortcutInceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ShortcutInceptionModule, self).__init__()\n",
    "        w_6 = out_channels // 6; w_3 = out_channels // 3; w_2 = out_channels - (w_6 + w_3)\n",
    "        self.branch1 = AsymmetricConv(in_channels, w_6)\n",
    "        self.branch2 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3))\n",
    "        self.branch3 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3), AsymmetricConv(w_3, w_2))\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.relu(torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1) + self.shortcut(x))\n",
    "\n",
    "class MultiScaleUNet(nn.Module):\n",
    "    def __init__(self, in_channels=12, out_channels=3):\n",
    "        super(MultiScaleUNet, self).__init__()\n",
    "        self.sim1 = ShortcutInceptionModule(in_channels, 96); self.pool1 = nn.MaxPool2d(2)\n",
    "        self.sim2 = ShortcutInceptionModule(96, 192);         self.pool2 = nn.MaxPool2d(2)\n",
    "        self.sim3 = ShortcutInceptionModule(192, 384);        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.sim4 = ShortcutInceptionModule(384, 768)\n",
    "        self.rsc1 = nn.Sequential(*[ResidualSkipConnection(96) for _ in range(4)])\n",
    "        self.rsc2 = nn.Sequential(*[ResidualSkipConnection(192) for _ in range(3)])\n",
    "        self.rsc3 = nn.Sequential(*[ResidualSkipConnection(384) for _ in range(2)])\n",
    "        self.sim5 = ShortcutInceptionModule(768, 384);   self.up1 = nn.ConvTranspose2d(384, 384, 2, 2)\n",
    "        self.sim6 = ShortcutInceptionModule(768, 192);   self.up2 = nn.ConvTranspose2d(192, 192, 2, 2)\n",
    "        self.sim7 = ShortcutInceptionModule(384, 96);    self.up3 = nn.ConvTranspose2d(96, 96, 2, 2)\n",
    "        self.sim8 = ShortcutInceptionModule(192, 96)\n",
    "        self.final = nn.Conv2d(96, out_channels, 3, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        e1 = self.sim1(x); p1 = self.pool1(e1)\n",
    "        e2 = self.sim2(p1); p2 = self.pool2(e2)\n",
    "        e3 = self.sim3(p2); p3 = self.pool3(e3)\n",
    "        e4 = self.sim4(p3)\n",
    "        d1 = self.sim5(e4); u1 = self.up1(d1)\n",
    "        d2 = self.sim6(torch.cat([u1, self.rsc3(e3)], dim=1)); u2 = self.up2(d2)\n",
    "        d3 = self.sim7(torch.cat([u2, self.rsc2(e2)], dim=1)); u3 = self.up3(d3)\n",
    "        d4 = self.sim8(torch.cat([u3, self.rsc1(e1)], dim=1))\n",
    "        return self.tanh(self.final(d4))\n",
    "\n",
    "# --- 2. SINGLE VIDEO DATASET ---\n",
    "class SingleVideoDataset(Dataset):\n",
    "    def __init__(self, vid_id, root_dir, clip_len=4, img_size=256):\n",
    "        self.clip_len = clip_len\n",
    "        self.samples = []\n",
    "        \n",
    "        # Locate Video\n",
    "        vid_path = os.path.join(root_dir, vid_id)\n",
    "        if not os.path.exists(vid_path):\n",
    "            # Try finding folder with different zero-padding (e.g. '2' vs '02')\n",
    "            candidates = [d for d in os.listdir(root_dir) if str(int(d)) == str(int(vid_id))]\n",
    "            if candidates: vid_path = os.path.join(root_dir, candidates[0])\n",
    "            else: raise ValueError(f\"Video {vid_id} not found in {root_dir}\")\n",
    "            \n",
    "        print(f\"Loading frames from {vid_path}...\")\n",
    "        self.frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        # Create sliding windows\n",
    "        if len(self.frames) >= clip_len + 1:\n",
    "            for i in range(len(self.frames) - clip_len):\n",
    "                self.samples.append(i)\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Input: [t, t+1, t+2, t+3]\n",
    "        in_paths = self.frames[idx : idx + self.clip_len]\n",
    "        # Target: [t+4]\n",
    "        tgt_path = self.frames[idx + self.clip_len]\n",
    "        \n",
    "        imgs = [self.transform(Image.open(p).convert('RGB')) for p in in_paths]\n",
    "        input_seq = torch.cat(imgs, dim=0) # (12, H, W)\n",
    "        target = self.transform(Image.open(tgt_path).convert('RGB'))\n",
    "        \n",
    "        return input_seq, target, idx + self.clip_len\n",
    "\n",
    "# --- 3. PLOT LOGIC ---\n",
    "def visualize():\n",
    "    print(f\"Generating Anomaly Graph for Video {TARGET_VIDEO} using {MODEL_PATH}...\")\n",
    "    \n",
    "    # Load Model\n",
    "    model = MultiScaleUNet().to(DEVICE)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: {MODEL_PATH} not found. Please train first or check the path.\")\n",
    "        return\n",
    "        \n",
    "    st = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    # Handle DataParallel dict keys if needed\n",
    "    if 'module.' in list(st.keys())[0]: st = {k.replace('module.', ''): v for k, v in st.items()}\n",
    "    model.load_state_dict(st)\n",
    "    model.eval()\n",
    "    \n",
    "    # Data\n",
    "    ds = SingleVideoDataset(TARGET_VIDEO, TEST_DIR, CLIP_LEN, IMG_SIZE)\n",
    "    loader = DataLoader(ds, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    frame_indices = []\n",
    "    errors = []\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, idxs in tqdm(loader):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            preds = model(inputs)\n",
    "            \n",
    "            # Calculate Reconstruction Error (MSE) per frame\n",
    "            # (Batch, C, H, W) -> Mean over (C, H, W) -> (Batch)\n",
    "            mse = ((preds - targets)**2).mean(dim=(1,2,3)).cpu().numpy()\n",
    "            \n",
    "            errors.extend(mse)\n",
    "            frame_indices.extend(idxs.numpy())\n",
    "            \n",
    "    # Process Scores\n",
    "    scores = np.array(errors)\n",
    "    \n",
    "    # Normalize [0, 1] for this video (Crucial for AP!)\n",
    "    mn, mx = scores.min(), scores.max()\n",
    "    if mx > mn:\n",
    "        scores_norm = (scores - mn) / (mx - mn)\n",
    "    else:\n",
    "        scores_norm = scores\n",
    "        \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(frame_indices, scores_norm, color='red', linewidth=2, label='Anomaly Score (Normalized MSE)')\n",
    "    \n",
    "    plt.title(f\"Anomaly Score Profile: Video {TARGET_VIDEO} (Epoch 2 Model)\", fontsize=16)\n",
    "    plt.xlabel(\"Frame Number\")\n",
    "    plt.ylabel(\"Anomaly Score (0=Normal, 1=Anomaly)\")\n",
    "    plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission script generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:03:11.589235Z",
     "iopub.status.busy": "2026-01-01T19:03:11.589057Z",
     "iopub.status.idle": "2026-01-01T19:03:19.723075Z",
     "shell.execute_reply": "2026-01-01T19:03:19.722118Z",
     "shell.execute_reply.started": "2026-01-01T19:03:11.589216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Generating Final Corrected Submission...\n",
      "‚ùå Model not found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "MODEL_PATH = '/kaggle/input/vlg-unetmodelep8/pytorch/default/1/unet_conditional_ep8.pth' \n",
    "CLEAN_DIR = '/kaggle/working/denoised_dataset' \n",
    "\n",
    "# Path to original noisy frames (for correct IDs like 939)\n",
    "# CONFIRM THIS PATH IS CORRECT on your system\n",
    "ORIGINAL_TEST_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\n",
    "\n",
    "SUBMISSION_FILE = 'submission.csv'\n",
    "\n",
    "IMG_SIZE = 256\n",
    "CLIP_LEN = 4\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =================================================\n",
    "\n",
    "# --- HELPER: NATURAL SORT ---\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
    "\n",
    "# --- 1. MODEL ARCHITECTURE ---\n",
    "class AsymmetricConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(AsymmetricConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(kernel_size//2, 0))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(1, kernel_size), padding=(0, kernel_size//2))\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, x): return self.relu(self.bn(self.conv2(self.relu(self.conv1(x)))))\n",
    "\n",
    "class ResidualSkipConnection(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualSkipConnection, self).__init__()\n",
    "        self.block = nn.Sequential(AsymmetricConv(channels, channels), AsymmetricConv(channels, channels))\n",
    "        self.shortcut = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x): return self.relu(self.block(x) + self.shortcut(x))\n",
    "\n",
    "class ShortcutInceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ShortcutInceptionModule, self).__init__()\n",
    "        w_6 = out_channels // 6; w_3 = out_channels // 3; w_2 = out_channels - (w_6 + w_3)\n",
    "        self.branch1 = AsymmetricConv(in_channels, w_6)\n",
    "        self.branch2 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3))\n",
    "        self.branch3 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3), AsymmetricConv(w_3, w_2))\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.relu(torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1) + self.shortcut(x))\n",
    "\n",
    "class MultiScaleUNet(nn.Module):\n",
    "    def __init__(self, in_channels=12, out_channels=3):\n",
    "        super(MultiScaleUNet, self).__init__()\n",
    "        self.sim1 = ShortcutInceptionModule(in_channels, 96); self.pool1 = nn.MaxPool2d(2)\n",
    "        self.sim2 = ShortcutInceptionModule(96, 192);         self.pool2 = nn.MaxPool2d(2)\n",
    "        self.sim3 = ShortcutInceptionModule(192, 384);        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.sim4 = ShortcutInceptionModule(384, 768)\n",
    "        self.rsc1 = nn.Sequential(*[ResidualSkipConnection(96) for _ in range(4)])\n",
    "        self.rsc2 = nn.Sequential(*[ResidualSkipConnection(192) for _ in range(3)])\n",
    "        self.rsc3 = nn.Sequential(*[ResidualSkipConnection(384) for _ in range(2)])\n",
    "        self.sim5 = ShortcutInceptionModule(768, 384);   self.up1 = nn.ConvTranspose2d(384, 384, 2, 2)\n",
    "        self.sim6 = ShortcutInceptionModule(768, 192);   self.up2 = nn.ConvTranspose2d(192, 192, 2, 2)\n",
    "        self.sim7 = ShortcutInceptionModule(384, 96);    self.up3 = nn.ConvTranspose2d(96, 96, 2, 2)\n",
    "        self.sim8 = ShortcutInceptionModule(192, 96)\n",
    "        self.final = nn.Conv2d(96, out_channels, 3, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        e1 = self.sim1(x); p1 = self.pool1(e1)\n",
    "        e2 = self.sim2(p1); p2 = self.pool2(e2)\n",
    "        e3 = self.sim3(p2); p3 = self.pool3(e3)\n",
    "        e4 = self.sim4(p3)\n",
    "        d1 = self.sim5(e4); u1 = self.up1(d1)\n",
    "        d2 = self.sim6(torch.cat([u1, self.rsc3(e3)], dim=1)); u2 = self.up2(d2)\n",
    "        d3 = self.sim7(torch.cat([u2, self.rsc2(e2)], dim=1)); u3 = self.up3(d3)\n",
    "        d4 = self.sim8(torch.cat([u3, self.rsc1(e1)], dim=1))\n",
    "        return self.tanh(self.final(d4))\n",
    "\n",
    "# --- 2. DATASET (Reads CLEANED frames) ---\n",
    "class CleanVideoDataset(Dataset):\n",
    "    def __init__(self, vid_id, root_dir, clip_len=4, img_size=256):\n",
    "        self.clip_len = clip_len\n",
    "        self.samples = []\n",
    "        vid_path = os.path.join(root_dir, vid_id)\n",
    "        # Assumes frame_0000.jpg format (from FastDVDnet output)\n",
    "        self.frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        if len(self.frames) >= clip_len + 1:\n",
    "            for i in range(len(self.frames) - clip_len):\n",
    "                self.samples.append(i)\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_paths = self.frames[idx : idx + self.clip_len]\n",
    "        tgt_path = self.frames[idx + self.clip_len]\n",
    "        imgs = [self.transform(Image.open(p).convert('RGB')) for p in in_paths]\n",
    "        input_seq = torch.cat(imgs, dim=0)\n",
    "        target = self.transform(Image.open(tgt_path).convert('RGB'))\n",
    "        return input_seq, target\n",
    "\n",
    "# --- 3. MAIN LOGIC ---\n",
    "def extract_frame_id(filename):\n",
    "    \"\"\"\n",
    "    Extracts 939 from 'frame_00939.jpg' or '939.jpg'\n",
    "    \"\"\"\n",
    "    # Regex to find the LAST sequence of digits in the filename\n",
    "    match = re.search(r'(\\d+)', os.path.splitext(filename)[0])\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def generate_final_submission():\n",
    "    print(\"üöÄ Generating Final Corrected Submission...\")\n",
    "    \n",
    "    # 1. Load Model\n",
    "    model = MultiScaleUNet()\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"‚ùå Model not found.\"); return\n",
    "    \n",
    "    st = torch.load(MODEL_PATH, map_location='cpu')\n",
    "    if 'module.' in list(st.keys())[0]: st = {k.replace('module.', ''): v for k, v in st.items()}\n",
    "    model.load_state_dict(st)\n",
    "    model.to(DEVICE)\n",
    "    if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
    "    model.eval()\n",
    "    \n",
    "    # 2. Iterate Videos\n",
    "    video_folders = sorted([f for f in os.listdir(CLEAN_DIR) if os.path.isdir(os.path.join(CLEAN_DIR, f))])\n",
    "    all_results = []\n",
    "    \n",
    "    for vid_id in video_folders:\n",
    "        print(f\"üé¨ Processing Video {vid_id}...\", end=\" \")\n",
    "        \n",
    "        # --- A. Get Correct Frame IDs from Original Folder ---\n",
    "        orig_vid_path = os.path.join(ORIGINAL_TEST_DIR, vid_id)\n",
    "        if not os.path.exists(orig_vid_path):\n",
    "             candidates = [d for d in os.listdir(ORIGINAL_TEST_DIR) if str(int(d)) == str(int(vid_id))]\n",
    "             if candidates: orig_vid_path = os.path.join(ORIGINAL_TEST_DIR, candidates[0])\n",
    "        \n",
    "        # Grab all frames (frame_00939.jpg)\n",
    "        orig_files = glob.glob(os.path.join(orig_vid_path, '*'))\n",
    "        orig_files = [f for f in orig_files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        orig_files.sort(key=lambda x: natural_sort_key(os.path.basename(x)))\n",
    "        \n",
    "        # Extract Real IDs (e.g., 939, 940...)\n",
    "        real_frame_ids = []\n",
    "        for f in orig_files:\n",
    "            fid = extract_frame_id(os.path.basename(f))\n",
    "            if fid is not None:\n",
    "                real_frame_ids.append(fid)\n",
    "                \n",
    "        if not real_frame_ids:\n",
    "            print(f\"‚ö†Ô∏è No frames found in original {vid_id}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # --- B. Inference ---\n",
    "        ds = CleanVideoDataset(vid_id, CLEAN_DIR, CLIP_LEN, IMG_SIZE)\n",
    "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "        \n",
    "        raw_scores = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in loader:\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                preds = model(inputs)\n",
    "                batch_mse = ((preds - targets)**2).mean(dim=(1,2,3)).cpu().numpy()\n",
    "                raw_scores.extend(batch_mse)\n",
    "        \n",
    "        # --- C. Normalization ---\n",
    "        scores_arr = np.array(raw_scores)\n",
    "        if len(scores_arr) > 0:\n",
    "            mn, mx = scores_arr.min(), scores_arr.max()\n",
    "            if mx > mn: scores_norm = (scores_arr - mn) / (mx - mn)\n",
    "            else: scores_norm = scores_arr\n",
    "        else:\n",
    "            scores_norm = []\n",
    "        \n",
    "        # --- D. Mapping ---\n",
    "        # 1. The first 4 frames (CLIP_LEN) have NO prediction -> fill 0.0\n",
    "        # 2. Prediction 0 aligns with Frame Index 4 (5th frame)\n",
    "        \n",
    "        count_mapped = 0\n",
    "        for i, real_id in enumerate(real_frame_ids):\n",
    "            if i < CLIP_LEN:\n",
    "                # Fill buffer frames\n",
    "                score = 0.0\n",
    "            else:\n",
    "                # Use prediction if available\n",
    "                pred_idx = i - CLIP_LEN\n",
    "                if pred_idx < len(scores_norm):\n",
    "                    score = scores_norm[pred_idx]\n",
    "                else:\n",
    "                    score = 0.0 # Should not happen if lengths align\n",
    "            \n",
    "            all_results.append({\n",
    "                'video_id': vid_id,\n",
    "                'frame_id': real_id,\n",
    "                'Predicted': float(score)\n",
    "            })\n",
    "            count_mapped += 1\n",
    "            \n",
    "        print(f\"Done. ({count_mapped} frames)\")\n",
    "\n",
    "    # 3. Save\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df['video_id'] = df['video_id'].astype(str)\n",
    "    df.sort_values(by=['video_id', 'frame_id'], inplace=True)\n",
    "    \n",
    "    df.to_csv(SUBMISSION_FILE, index=False)\n",
    "    print(f\"\\n‚úÖ‚úÖ‚úÖ FINAL SUBMISSION SAVED: {SUBMISSION_FILE}\")\n",
    "    print(\"Example (Video 01):\")\n",
    "    print(df[df['video_id'].isin(['01', '1'])].head(6))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_final_submission()\n",
    "\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Path to the CSV you just generated (with video_id, frame_id, score)\n",
    "INPUT_CSV = 'submission.csv' \n",
    "OUTPUT_CSV = 'submission_final.csv'\n",
    "# =================================================\n",
    "\n",
    "def format_submission():\n",
    "    print(f\"üìÇ Loading {INPUT_CSV}...\")\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    \n",
    "    # 1. Ensure clean integer types\n",
    "    # This automatically turns \"01\" into 1, handling the leading zero removal\n",
    "    df['video_id'] = df['video_id'].astype(int)\n",
    "    df['frame_id'] = df['frame_id'].astype(int)\n",
    "    \n",
    "    # 2. Create the merged 'ID' column (Format: 1_939)\n",
    "    print(\"üîÑ Merging columns to 'videoID_frameID' format...\")\n",
    "    df['ID'] = df['video_id'].astype(str) + \"_\" + df['frame_id'].astype(str)\n",
    "    \n",
    "    # 3. Select ONLY the required columns\n",
    "    final_df = df[['ID', 'Predicted']]\n",
    "    \n",
    "    # 4. Fill any remaining NaNs with 0.0 (Just in case)\n",
    "    final_df['Predicted'] = final_df['Predicted'].fillna(0.0)\n",
    "    \n",
    "    # 5. Save\n",
    "    final_df.to_csv(OUTPUT_CSV, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ SUCCESS! Final file saved to: {OUTPUT_CSV}\")\n",
    "    print(\"Example Rows:\")\n",
    "    print(final_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    format_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid Normalising (good for visualisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "INPUT_CSV = '/kaggle/working/submission_final.csv'\n",
    "OUTPUT_CSV = 'submission_processed.csv'\n",
    "\n",
    "# The \"Center\" of your sigmoid (The threshold between Normal vs Anomaly)\n",
    "# You observed normals are < 0.1, so 0.1 is the tipping point.\n",
    "CENTER = 0.07\n",
    "\n",
    "# How hard to push values to 0 and 1.\n",
    "# Low (e.g. 10) = Gentle S-curve\n",
    "# High (e.g. 50) = Hard Step Function (Almost binary)\n",
    "STEEPNESS = 15\n",
    "# =================================================\n",
    "\n",
    "def sigmoid_transform(x, center, k):\n",
    "    \"\"\"\n",
    "    Custom Sigmoid:\n",
    "    x: Input score array\n",
    "    center: The value that maps to 0.5 (tipping point)\n",
    "    k: Steepness coefficient\n",
    "    \"\"\"\n",
    "    # 1. Shift x so the center is at 0\n",
    "    shifted = x - center\n",
    "    \n",
    "    # 2. Scale by steepness\n",
    "    scaled = k * shifted\n",
    "    \n",
    "    # 3. Apply standard sigmoid: 1 / (1 + e^-x)\n",
    "    return 1 / (1 + np.exp(-scaled))\n",
    "\n",
    "def process_and_visualize():\n",
    "    print(f\"üìÇ Loading {INPUT_CSV}...\")\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    \n",
    "    original_scores = df['Predicted'].values\n",
    "    \n",
    "    # --- APPLY TRANSFORM ---\n",
    "    print(f\"‚ö° Applying Sigmoid (Center={CENTER}, Steepness={STEEPNESS})...\")\n",
    "    new_scores = sigmoid_transform(original_scores, CENTER, STEEPNESS)\n",
    "    \n",
    "    df['Predicted'] = new_scores\n",
    "    \n",
    "    # --- VISUALIZATION (THE \"U\" GRAPH) ---\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Plot 1: Original Distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(original_scores, bins=50, color='blue', alpha=0.7)\n",
    "    plt.axvline(CENTER, color='red', linestyle='--', label=f'Center ({CENTER})')\n",
    "    plt.title(\"Original Scores (Before)\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Processed Distribution (Should look like a U)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(new_scores, bins=50, color='green', alpha=0.7)\n",
    "    plt.title(f\"Processed Scores (After)\\nSteepness: {STEEPNESS}\")\n",
    "    plt.xlabel(\"Score (0=Normal, 1=Anomaly)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"score_distribution_u_graph.png\")\n",
    "    print(\"üì∏ Saved visualization to 'score_distribution_u_graph.png'\")\n",
    "    plt.show()\n",
    "    \n",
    "    # --- SAVE ---\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"‚úÖ Saved processed scores to {OUTPUT_CSV}\")\n",
    "    print(df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_and_visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly window detection (smoothning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Use the file that gave you 0.66\n",
    "INPUT_CSV = '/kaggle/working/submission_processed.csv' \n",
    "OUTPUT_CSV = 'submission_boosted_unet.csv'\n",
    "\n",
    "# DILATION WINDOW\n",
    "# Avenue is 25fps. Events last ~1-2 seconds.\n",
    "# A window of 25 means we look +/- 12 frames around a peak.\n",
    "WINDOW_SIZE = 20\n",
    "# =================================================\n",
    "\n",
    "def boost_score():\n",
    "    print(f\"Reading {INPUT_CSV}...\")\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    \n",
    "    df['VideoID'] = df['ID'].apply(lambda x: x.split('_')[0])\n",
    "    \n",
    "    final_preds = []\n",
    "    \n",
    "    print(f\"Applying Peak Widening (Window={WINDOW_SIZE})...\")\n",
    "    \n",
    "    for vid, group in df.groupby('VideoID', sort=False):\n",
    "        raw = group['Predicted'].copy()\n",
    "        \n",
    "        # 1. ROLLING MAX (Dilation)\n",
    "        # This spreads the '1.0' scores to their neighbors.\n",
    "        # \"If frame T is bad, then frame T+1 is probably bad too.\"\n",
    "        dilated = raw.rolling(window=WINDOW_SIZE, center=True, min_periods=1).max()\n",
    "        \n",
    "        # 2. LIGHT SMOOTHING (Average)\n",
    "        # Rolling max creates \"blocky\" steps. We smooth the edges slightly.\n",
    "        smoothed = dilated.rolling(window=10, center=True, min_periods=1).mean()\n",
    "        \n",
    "        \n",
    "        \n",
    "        final_preds.extend(smoothed.values)\n",
    "        \n",
    "    df['Predicted'] = final_preds\n",
    "    \n",
    "    # Final Normalize (Safety)\n",
    "    df['Predicted'] = np.clip(df['Predicted'], 0.0, 1.0)\n",
    "    \n",
    "    # --- VISUALIZATION ---\n",
    "    # Let's see how much \"fatter\" the detection became\n",
    "    v05 = df[df['ID'].str.startswith('05_')]\n",
    "    if not v05.empty:\n",
    "        x = v05['ID'].apply(lambda x: int(x.split('_')[1]))\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # We need to reload original to compare\n",
    "        orig = pd.read_csv(INPUT_CSV)\n",
    "        orig_v05 = orig[orig['ID'].str.startswith('05_')]\n",
    "        \n",
    "        plt.plot(x, orig_v05['Predicted'], color='gray', alpha=0.5, label='Original (0.66)')\n",
    "        plt.plot(x, v05['Predicted'], color='green', linewidth=2, label='Widened (Boosted)')\n",
    "        \n",
    "        plt.title(\"Peak Widening Effect (Video 05)\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Save\n",
    "    out = df[['ID', 'Predicted']]\n",
    "    out.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"Saved boosted scores to {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    boost_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is used in final essemble code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final visualiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "INPUT_CSV = '/kaggle/working/submission_boosted_unet.csv'\n",
    "THRESHOLD = 0.5 \n",
    "# =================================================\n",
    "\n",
    "def visualize_inline():\n",
    "    print(f\"üìÇ Loading {INPUT_CSV}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: File not found. Did you run the previous step?\")\n",
    "        return\n",
    "\n",
    "    # 1. Parse 'ID' (e.g., '1_939') back into Video and Frame columns\n",
    "    print(\"üîÑ Parsing data...\")\n",
    "    split_data = df['ID'].str.split('_', expand=True)\n",
    "    df['video_id'] = split_data[0].astype(int)\n",
    "    df['frame_id'] = split_data[1].astype(int)\n",
    "    \n",
    "    # 2. Get unique videos\n",
    "    videos = sorted(df['video_id'].unique())\n",
    "    print(f\"üåç Visualizing {len(videos)} videos...\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 3. Loop and Plot Inline\n",
    "    for vid in videos:\n",
    "        # Filter data for this video\n",
    "        vid_df = df[df['video_id'] == vid].sort_values(by='frame_id')\n",
    "        \n",
    "        frames = vid_df['frame_id'].values\n",
    "        scores = vid_df['Predicted'].values\n",
    "        \n",
    "        # Create Plot\n",
    "        plt.figure(figsize=(14, 4)) # Wide and short for better scrolling\n",
    "        \n",
    "        # Plot the Score Line\n",
    "        plt.plot(frames, scores, color='#007acc', linewidth=2, label='Anomaly Score')\n",
    "        \n",
    "        # Fill area under curve for anomalies (Red Zones)\n",
    "        plt.fill_between(frames, scores, THRESHOLD, where=(scores >= THRESHOLD), \n",
    "                         interpolate=True, color='red', alpha=0.3, label='Anomaly Detected')\n",
    "        \n",
    "        # Add Threshold Line\n",
    "        plt.axhline(y=THRESHOLD, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Styling\n",
    "        plt.title(f\"Video {vid}: Anomaly Profile\", fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Frame Number\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(loc='upper right')\n",
    "        \n",
    "        # SHOW PLOT\n",
    "        plt.show()\n",
    "        print(\"\\n\") # Spacing\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_inline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
