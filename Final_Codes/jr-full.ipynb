{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Joint Representation Model Train - Test Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipped frames correction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm # Progress bar\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Path to the CORRUPTED testing videos\n",
    "TEST_DATA_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\n",
    "\n",
    "# Path where we will save the CLEANED videos\n",
    "CLEAN_DATA_DIR = '/kaggle/working/cleaned_testing_videos'\n",
    "\n",
    "MODEL_PATH = 'Final_models\/rotnet_model.pth'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =================================================\n",
    "\n",
    "def clean_dataset():\n",
    "    print(f\"Processing on: {DEVICE}\")\n",
    "    \n",
    "    # 1. Load the Trained RotNet\n",
    "    model = models.resnet18(pretrained=False) # No need to download weights again\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2) # Matches our binary training\n",
    "    \n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Standard transform for the model input\n",
    "    # Note: We do NOT augment here, just resize/norm\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 2. Find all images\n",
    "    # We walk through the directory to keep structure\n",
    "    image_paths = sorted(glob.glob(os.path.join(TEST_DATA_DIR, '**', '*.jpg'), recursive=True))\n",
    "    print(f\"Found {len(image_paths)} frames to process.\")\n",
    "    \n",
    "    # 3. Processing Loop\n",
    "    flip_count = 0\n",
    "    \n",
    "    for img_path in tqdm(image_paths, desc=\"Cleaning\"):\n",
    "        # A. Setup paths\n",
    "        # Get relative path (e.g., \"01/frame_0001.jpg\") to maintain structure\n",
    "        rel_path = os.path.relpath(img_path, TEST_DATA_DIR)\n",
    "        save_path = os.path.join(CLEAN_DATA_DIR, rel_path)\n",
    "        \n",
    "        # Create folder if not exists\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        # B. Predict Rotation\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        input_tensor = preprocess(image).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            label = predicted.item()\n",
    "            \n",
    "        # C. Fix and Save\n",
    "        # Label 0 = Upright (Keep as is)\n",
    "        # Label 1 = Flipped (Needs 180 rotation to fix)\n",
    "        \n",
    "        if label == 1:\n",
    "            # It was detected as Upside Down, so we rotate it -180 (or 180) to fix\n",
    "            fixed_image = image.transpose(Image.FLIP_TOP_BOTTOM) \n",
    "            flip_count += 1\n",
    "        else:\n",
    "            fixed_image = image\n",
    "            \n",
    "        # Save the fixed image\n",
    "        fixed_image.save(save_path)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Cleaning Complete!\")\n",
    "    print(f\"Total Images: {len(image_paths)}\")\n",
    "    print(f\"Images Flipped/Fixed: {flip_count}\")\n",
    "    print(f\"Cleaned dataset saved to: {CLEAN_DATA_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise remover using FastDVDnet (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# 1. WHERE ARE YOUR NOISY FRAMES?\n",
    "# Adjust this to the root folder containing '01', '02', etc.\n",
    "INPUT_ROOT = \"/kaggle/working/cleaned_testing_videos\" \n",
    "\n",
    "# 2. WHERE TO SAVE CLEAN FRAMES?\n",
    "OUTPUT_ROOT = \"/kaggle/working/denoised_dataset_test\"\n",
    "\n",
    "# 3. SETTINGS (The Winning Formula)\n",
    "NOISE_SIGMA = 40 / 255.0  \n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "# =================================================\n",
    "\n",
    "# --- UTILS ---\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
    "\n",
    "def install_and_setup():\n",
    "    if not os.path.exists(\"fastdvdnet\"):\n",
    "        print(\"ðŸ› ï¸ Cloning FastDVDnet...\")\n",
    "        os.system(\"git clone https://github.com/m-tassano/fastdvdnet.git\")\n",
    "        os.system(\"pip install tensorboardX\")\n",
    "    \n",
    "    if not os.path.exists(\"fastdvdnet/model/model.pth\"):\n",
    "        os.makedirs(\"fastdvdnet/model\", exist_ok=True)\n",
    "        os.system(\"wget -O fastdvdnet/model/model.pth https://github.com/m-tassano/fastdvdnet/raw/master/model.pth\")\n",
    "\n",
    "# --- DATASET ---\n",
    "class FrameSequenceDataset(Dataset):\n",
    "    def __init__(self, frame_paths):\n",
    "        self.frame_paths = frame_paths\n",
    "        self.total = len(frame_paths)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Sliding Window of 5 frames\n",
    "        indices = [max(0, min(self.total - 1, idx + offset)) for offset in range(-2, 3)]\n",
    "        \n",
    "        frames = []\n",
    "        for i in indices:\n",
    "            path = self.frame_paths[i]\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                img = np.zeros((360, 640, 3), dtype=np.uint8) # Fallback size\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            frames.append(img)\n",
    "            \n",
    "        stack = np.concatenate(frames, axis=2) # (H, W, 15)\n",
    "        tensor = torch.from_numpy(stack).permute(2, 0, 1) # (15, H, W)\n",
    "        return tensor\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "def run_mass_cleaning():\n",
    "    install_and_setup()\n",
    "    \n",
    "    # Import Model\n",
    "    sys.path.append(\"fastdvdnet\")\n",
    "    try:\n",
    "        from models import FastDVDnet\n",
    "    except ImportError:\n",
    "        from fastdvdnet.models import FastDVDnet\n",
    "\n",
    "    # Find all video folders (01, 02, ... 21)\n",
    "    video_folders = sorted(glob.glob(os.path.join(INPUT_ROOT, \"*\")))\n",
    "    # Filter to ensure they are directories\n",
    "    video_folders = [f for f in video_folders if os.path.isdir(f)]\n",
    "    \n",
    "    print(f\"ðŸŒ Found {len(video_folders)} videos to clean.\")\n",
    "\n",
    "    # Setup Model Once\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FastDVDnet(num_input_frames=5)\n",
    "    \n",
    "    state_dict = torch.load(\"fastdvdnet/model/model.pth\", map_location=device)\n",
    "    new_state = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"ðŸ”¥ Dual GPU Active\")\n",
    "        model = nn.DataParallel(model)\n",
    "        \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # --- LOOP OVER VIDEOS ---\n",
    "    for vid_path in video_folders:\n",
    "        vid_id = os.path.basename(vid_path)\n",
    "        print(f\"\\nðŸŽ¬ Processing Video: {vid_id}\")\n",
    "        \n",
    "        # 1. Get Frames\n",
    "        files = glob.glob(os.path.join(vid_path, \"*\"))\n",
    "        files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        files.sort(key=lambda x: natural_sort_key(os.path.basename(x)))\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"âš ï¸ Skipping {vid_id} (No images found)\")\n",
    "            continue\n",
    "            \n",
    "        # 2. Setup Output Folder\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, vid_id)\n",
    "        if os.path.exists(save_dir): shutil.rmtree(save_dir)\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "        # 3. Process\n",
    "        dataset = FrameSequenceDataset(files)\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(tqdm(loader, desc=f\"Cleaning {vid_id}\")):\n",
    "                data = data.to(device)\n",
    "                B, C, H, W = data.shape\n",
    "                \n",
    "                noise_sigma = torch.full((B, 1, H, W), NOISE_SIGMA).to(device)\n",
    "                \n",
    "                clean_batch = model(data, noise_sigma)\n",
    "                clean_batch = clean_batch.permute(0, 2, 3, 1).cpu().numpy()\n",
    "                \n",
    "                for i in range(B):\n",
    "                    img = np.clip(clean_batch[i] * 255, 0, 255).astype(np.uint8)\n",
    "                    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Standardized Name: frame_0000.jpg\n",
    "                    global_idx = batch_idx * BATCH_SIZE + i\n",
    "                    save_name = f\"frame_{global_idx:04d}.jpg\"\n",
    "                    \n",
    "                    cv2.imwrite(os.path.join(save_dir, save_name), img_bgr)\n",
    "                    \n",
    "    print(f\"\\nâœ…âœ…âœ… ALL VIDEOS CLEANED! Saved to: {OUTPUT_ROOT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_mass_cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise remover using FastDVDnet (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# 1. WHERE ARE YOUR NOISY FRAMES?\n",
    "# Adjust this to the root folder containing '01', '02', etc.\n",
    "INPUT_ROOT = \"/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos\" \n",
    "\n",
    "# 2. WHERE TO SAVE CLEAN FRAMES?\n",
    "OUTPUT_ROOT = \"/kaggle/working/denoised_dataset_train\"\n",
    "\n",
    "# 3. SETTINGS (The Winning Formula)\n",
    "NOISE_SIGMA = 40 / 255.0  \n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "# =================================================\n",
    "\n",
    "# --- UTILS ---\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
    "\n",
    "def install_and_setup():\n",
    "    if not os.path.exists(\"fastdvdnet\"):\n",
    "        print(\"ðŸ› ï¸ Cloning FastDVDnet...\")\n",
    "        os.system(\"git clone https://github.com/m-tassano/fastdvdnet.git\")\n",
    "        os.system(\"pip install tensorboardX\")\n",
    "    \n",
    "    if not os.path.exists(\"fastdvdnet/model/model.pth\"):\n",
    "        os.makedirs(\"fastdvdnet/model\", exist_ok=True)\n",
    "        os.system(\"wget -O fastdvdnet/model/model.pth https://github.com/m-tassano/fastdvdnet/raw/master/model.pth\")\n",
    "\n",
    "# --- DATASET ---\n",
    "class FrameSequenceDataset(Dataset):\n",
    "    def __init__(self, frame_paths):\n",
    "        self.frame_paths = frame_paths\n",
    "        self.total = len(frame_paths)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Sliding Window of 5 frames\n",
    "        indices = [max(0, min(self.total - 1, idx + offset)) for offset in range(-2, 3)]\n",
    "        \n",
    "        frames = []\n",
    "        for i in indices:\n",
    "            path = self.frame_paths[i]\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                img = np.zeros((360, 640, 3), dtype=np.uint8) # Fallback size\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            frames.append(img)\n",
    "            \n",
    "        stack = np.concatenate(frames, axis=2) # (H, W, 15)\n",
    "        tensor = torch.from_numpy(stack).permute(2, 0, 1) # (15, H, W)\n",
    "        return tensor\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "def run_mass_cleaning():\n",
    "    install_and_setup()\n",
    "    \n",
    "    # Import Model\n",
    "    sys.path.append(\"fastdvdnet\")\n",
    "    try:\n",
    "        from models import FastDVDnet\n",
    "    except ImportError:\n",
    "        from fastdvdnet.models import FastDVDnet\n",
    "\n",
    "    # Find all video folders (01, 02, ... 21)\n",
    "    video_folders = sorted(glob.glob(os.path.join(INPUT_ROOT, \"*\")))\n",
    "    # Filter to ensure they are directories\n",
    "    video_folders = [f for f in video_folders if os.path.isdir(f)]\n",
    "    \n",
    "    print(f\"ðŸŒ Found {len(video_folders)} videos to clean.\")\n",
    "\n",
    "    # Setup Model Once\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FastDVDnet(num_input_frames=5)\n",
    "    \n",
    "    state_dict = torch.load(\"fastdvdnet/model/model.pth\", map_location=device)\n",
    "    new_state = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"ðŸ”¥ Dual GPU Active\")\n",
    "        model = nn.DataParallel(model)\n",
    "        \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # --- LOOP OVER VIDEOS ---\n",
    "    for vid_path in video_folders:\n",
    "        vid_id = os.path.basename(vid_path)\n",
    "        print(f\"\\nðŸŽ¬ Processing Video: {vid_id}\")\n",
    "        \n",
    "        # 1. Get Frames\n",
    "        files = glob.glob(os.path.join(vid_path, \"*\"))\n",
    "        files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        files.sort(key=lambda x: natural_sort_key(os.path.basename(x)))\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"âš ï¸ Skipping {vid_id} (No images found)\")\n",
    "            continue\n",
    "            \n",
    "        # 2. Setup Output Folder\n",
    "        save_dir = os.path.join(OUTPUT_ROOT, vid_id)\n",
    "        if os.path.exists(save_dir): shutil.rmtree(save_dir)\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "        # 3. Process\n",
    "        dataset = FrameSequenceDataset(files)\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(tqdm(loader, desc=f\"Cleaning {vid_id}\")):\n",
    "                data = data.to(device)\n",
    "                B, C, H, W = data.shape\n",
    "                \n",
    "                noise_sigma = torch.full((B, 1, H, W), NOISE_SIGMA).to(device)\n",
    "                \n",
    "                clean_batch = model(data, noise_sigma)\n",
    "                clean_batch = clean_batch.permute(0, 2, 3, 1).cpu().numpy()\n",
    "                \n",
    "                for i in range(B):\n",
    "                    img = np.clip(clean_batch[i] * 255, 0, 255).astype(np.uint8)\n",
    "                    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Standardized Name: frame_0000.jpg\n",
    "                    global_idx = batch_idx * BATCH_SIZE + i\n",
    "                    save_name = f\"frame_{global_idx:04d}.jpg\"\n",
    "                    \n",
    "                    cv2.imwrite(os.path.join(save_dir, save_name), img_bgr)\n",
    "                    \n",
    "    print(f\"\\nâœ…âœ…âœ… ALL VIDEOS CLEANED! Saved to: {OUTPUT_ROOT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_mass_cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical Flow Calculation (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CONFIG\n",
    "# Use your Neural Cleaned videos for best flow calculation\n",
    "SOURCE_DIR = '/kaggle/working/denoised_dataset_train'\n",
    "DEST_DIR = '/kaggle/working/training_optical_flow'\n",
    "\n",
    "def extract_optical_flow():\n",
    "    if not os.path.exists(DEST_DIR): os.makedirs(DEST_DIR)\n",
    "    \n",
    "    print(\"Generating Optical Flow Maps...\")\n",
    "    \n",
    "    for vid in tqdm(sorted(os.listdir(SOURCE_DIR))):\n",
    "        vid_path = os.path.join(SOURCE_DIR, vid)\n",
    "        save_path = os.path.join(DEST_DIR, vid)\n",
    "        if not os.path.isdir(vid_path): continue\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n",
    "        prev_frame = cv2.imread(frames[0])\n",
    "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Save first flow as black (no motion) to keep frame count same\n",
    "        h, w = prev_gray.shape\n",
    "        blank_flow = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        cv2.imwrite(os.path.join(save_path, os.path.basename(frames[0])), blank_flow)\n",
    "        \n",
    "        for i in range(1, len(frames)):\n",
    "            curr_frame = cv2.imread(frames[i])\n",
    "            curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Calculate Dense Optical Flow (Farneback)\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, \n",
    "                                                0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            \n",
    "            # Visualize Flow as RGB Image\n",
    "            # Magnitude and Angle\n",
    "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            \n",
    "            # HSV encoding\n",
    "            hsv = np.zeros_like(prev_frame)\n",
    "            hsv[..., 1] = 255\n",
    "            # Hue = Angle, Value = Magnitude (Speed)\n",
    "            hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "            hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            \n",
    "            # Convert to RGB for saving\n",
    "            rgb_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "            \n",
    "            cv2.imwrite(os.path.join(save_path, os.path.basename(frames[i])), rgb_flow)\n",
    "            \n",
    "            prev_gray = curr_gray\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_optical_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical Flow Calculation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CONFIG\n",
    "# Use your Neural Cleaned videos for best flow calculation\n",
    "SOURCE_DIR = '/kaggle/working/denoised_dataset_test'\n",
    "DEST_DIR = '/kaggle/working/testing_optical_flow'\n",
    "\n",
    "def extract_optical_flow():\n",
    "    if not os.path.exists(DEST_DIR): os.makedirs(DEST_DIR)\n",
    "    \n",
    "    print(\"Generating Optical Flow Maps...\")\n",
    "    \n",
    "    for vid in tqdm(sorted(os.listdir(SOURCE_DIR))):\n",
    "        vid_path = os.path.join(SOURCE_DIR, vid)\n",
    "        save_path = os.path.join(DEST_DIR, vid)\n",
    "        if not os.path.isdir(vid_path): continue\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n",
    "        prev_frame = cv2.imread(frames[0])\n",
    "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Save first flow as black (no motion) to keep frame count same\n",
    "        h, w = prev_gray.shape\n",
    "        blank_flow = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        cv2.imwrite(os.path.join(save_path, os.path.basename(frames[0])), blank_flow)\n",
    "        \n",
    "        for i in range(1, len(frames)):\n",
    "            curr_frame = cv2.imread(frames[i])\n",
    "            curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Calculate Dense Optical Flow (Farneback)\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, \n",
    "                                                0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            \n",
    "            # Visualize Flow as RGB Image\n",
    "            # Magnitude and Angle\n",
    "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            \n",
    "            # HSV encoding\n",
    "            hsv = np.zeros_like(prev_frame)\n",
    "            hsv[..., 1] = 255\n",
    "            # Hue = Angle, Value = Magnitude (Speed)\n",
    "            hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "            hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            \n",
    "            # Convert to RGB for saving\n",
    "            rgb_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "            \n",
    "            cv2.imwrite(os.path.join(save_path, os.path.basename(frames[i])), rgb_flow)\n",
    "            \n",
    "            prev_gray = curr_gray\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_optical_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training scrpit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Expected train time 7-8 hrs untill saturation (Stop early to save model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# INPUTS\n",
    "RGB_DIR = '/kaggle/working/denoised_dataset_train'\n",
    "FLOW_DIR = '/kaggle/working/training_optical_flow'\n",
    "\n",
    "# OUTPUTS\n",
    "MODEL_SAVE_PATH = 'joint_resnet_generator.pth'\n",
    "DISC_SAVE_PATH = 'joint_resnet_discriminator.pth'\n",
    "\n",
    "# HYPERPARAMETERS (Optimized for Speed)\n",
    "BATCH_SIZE = 48      # Increased from 4 -> 48 (Should fit in 16GB)\n",
    "EPOCHS = 30\n",
    "LR = 2e-4\n",
    "LR_STEP = 10         # Slightly relaxed decay since we do fewer steps per epoch now\n",
    "LR_GAMMA = 0.1\n",
    "T_STEPS = 4\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# LOSS WEIGHTS\n",
    "LAMBDA_INT = 2.0\n",
    "LAMBDA_GD = 1.0\n",
    "LAMBDA_ADV = 0.05\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =================================================\n",
    "\n",
    "# --- 1. MODEL ARCHITECTURE ---\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels=512):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1); self.bn1 = nn.BatchNorm2d(channels); self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1); self.bn2 = nn.BatchNorm2d(channels)\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn2(self.conv2(self.relu(self.bn1(self.conv1(x))))) + x)\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels, 64, 7, 1, 3), nn.BatchNorm2d(64), nn.ReLU(inplace=True))\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True))\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True))\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True))\n",
    "        self.bottleneck = nn.Sequential(*[ResidualBlock(512) for _ in range(9)])\n",
    "    def forward(self, x):\n",
    "        return self.bottleneck(self.layer4(self.layer3(self.layer2(self.layer1(x)))))\n",
    "\n",
    "class JointRepresentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JointRepresentationModel, self).__init__()\n",
    "        self.spatial_encoder = ResNetEncoder(in_channels=12)\n",
    "        self.temporal_encoder = ResNetEncoder(in_channels=3)\n",
    "        self.dec1 = nn.Sequential(nn.ConvTranspose2d(1024, 256, 3, 2, 1, output_padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True))\n",
    "        self.dec2 = nn.Sequential(nn.ConvTranspose2d(256, 128, 3, 2, 1, output_padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True))\n",
    "        self.dec3 = nn.Sequential(nn.ConvTranspose2d(128, 64, 3, 2, 1, output_padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True))\n",
    "        self.final = nn.Conv2d(64, 3, 7, 1, 3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, rgb, flow):\n",
    "        fused = torch.cat([self.spatial_encoder(rgb), self.temporal_encoder(flow)], dim=1)\n",
    "        return self.sigmoid(self.final(self.dec3(self.dec2(self.dec1(fused)))))\n",
    "\n",
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PatchGANDiscriminator, self).__init__()\n",
    "        def d_block(in_c, out_c, bn=True):\n",
    "            layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "            if bn: layers.append(nn.BatchNorm2d(out_c))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        self.model = nn.Sequential(*d_block(3, 64, bn=False), *d_block(64, 128), *d_block(128, 256), nn.Conv2d(256, 1, 4, 1, 1))\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "# --- 2. LOSS FUNCTIONS ---\n",
    "def gradient_loss(gen, gt):\n",
    "    def grad(x): return torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]), torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :])\n",
    "    gen_dx, gen_dy = grad(gen); gt_dx, gt_dy = grad(gt)\n",
    "    return torch.mean(torch.abs(gen_dx - gt_dx)) + torch.mean(torch.abs(gen_dy - gt_dy))\n",
    "\n",
    "def intensity_loss(gen, gt): return nn.MSELoss()(gen, gt)\n",
    "\n",
    "# --- 3. DATASET ---\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self, rgb_dir, flow_dir, transform=None, t_steps=4):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        videos = sorted(os.listdir(rgb_dir))\n",
    "        for vid in videos:\n",
    "            rgb_p = os.path.join(rgb_dir, vid); flow_p = os.path.join(flow_dir, vid)\n",
    "            if not os.path.isdir(rgb_p): continue\n",
    "            frames = sorted(glob.glob(os.path.join(rgb_p, '*.jpg')))\n",
    "            if len(frames) < t_steps + 1: continue\n",
    "            for i in range(len(frames) - t_steps):\n",
    "                self.samples.append((frames[i:i+t_steps], os.path.join(flow_p, os.path.basename(frames[i+t_steps-1])), frames[i+t_steps]))\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_ps, flow_p, tgt_p = self.samples[idx]\n",
    "        rgb = [Image.open(p).convert('RGB') for p in rgb_ps]\n",
    "        flow = Image.open(flow_p).convert('RGB')\n",
    "        tgt = Image.open(tgt_p).convert('RGB')\n",
    "        if self.transform:\n",
    "            rgb = [self.transform(i) for i in rgb]; flow = self.transform(flow); tgt = self.transform(tgt)\n",
    "        return torch.cat(rgb, dim=0), flow, tgt\n",
    "\n",
    "# --- 4. TRAINING LOOP ---\n",
    "def train():\n",
    "    print(f\"Initializing FAST Joint ResNet (Batch={BATCH_SIZE}) on {DEVICE}...\")\n",
    "    \n",
    "    tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n",
    "    dataset = JointDataset(RGB_DIR, FLOW_DIR, transform=tf, t_steps=T_STEPS)\n",
    "    \n",
    "    # Num workers 8 to prevent CPU bottleneck at Batch 32\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True, prefetch_factor=2)\n",
    "    \n",
    "    gen = JointRepresentationModel().to(DEVICE)\n",
    "    disc = PatchGANDiscriminator().to(DEVICE)\n",
    "    if torch.cuda.device_count() > 1: gen = nn.DataParallel(gen); disc = nn.DataParallel(disc)\n",
    "        \n",
    "    opt_g = optim.Adam(gen.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "    opt_d = optim.Adam(disc.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "    \n",
    "    sched_g = optim.lr_scheduler.StepLR(opt_g, step_size=LR_STEP, gamma=LR_GAMMA)\n",
    "    sched_d = optim.lr_scheduler.StepLR(opt_d, step_size=LR_STEP, gamma=LR_GAMMA)\n",
    "    \n",
    "    mse = nn.MSELoss()\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(EPOCHS):\n",
    "            gen.train(); disc.train()\n",
    "            loop = tqdm(loader, desc=f\"Ep {epoch+1}/{EPOCHS}\")\n",
    "            \n",
    "            run_g = 0.0; run_int = 0.0; run_gd = 0.0; run_adv = 0.0; run_d = 0.0\n",
    "            \n",
    "            for rgb, flow, target in loop:\n",
    "                rgb, flow, target = rgb.to(DEVICE), flow.to(DEVICE), target.to(DEVICE)\n",
    "                \n",
    "                # --- G Step ---\n",
    "                opt_g.zero_grad()\n",
    "                fake = gen(rgb, flow)\n",
    "                \n",
    "                pred_fake = disc(fake)\n",
    "                l_adv = mse(pred_fake, torch.ones_like(pred_fake))\n",
    "                l_int = intensity_loss(fake, target)\n",
    "                l_gd = gradient_loss(fake, target)\n",
    "                \n",
    "                l_g = (LAMBDA_INT * l_int) + (LAMBDA_GD * l_gd) + (LAMBDA_ADV * l_adv)\n",
    "                l_g.backward()\n",
    "                opt_g.step()\n",
    "                \n",
    "                # --- D Step ---\n",
    "                opt_d.zero_grad()\n",
    "                pred_real = disc(target)\n",
    "                l_d_real = mse(pred_real, torch.ones_like(pred_real))\n",
    "                pred_fake_d = disc(fake.detach())\n",
    "                l_d_fake = mse(pred_fake_d, torch.zeros_like(pred_fake_d))\n",
    "                l_d = 0.5 * (l_d_real + l_d_fake)\n",
    "                l_d.backward()\n",
    "                opt_d.step()\n",
    "                \n",
    "                run_g += l_g.item(); run_int += l_int.item(); run_gd += l_gd.item(); run_adv += l_adv.item(); run_d += l_d.item()\n",
    "                loop.set_postfix(G=l_g.item(), D=l_d.item())\n",
    "            \n",
    "            count = len(loader)\n",
    "            print(f\"\\n[Ep {epoch+1}] G: {run _g/count:.4f} (Int:{run_int/count:.4f} Gd:{run_gd/count:.4f} Adv:{run_adv/count:.4f}) | D: {run_d/count:.4f} | LR: {sched_g.get_last_lr()[0]:.2e}\")\n",
    "            \n",
    "            sched_g.step(); sched_d.step()\n",
    "            \n",
    "            if (epoch+1) % 5 == 0:\n",
    "                torch.save(gen.module.state_dict() if hasattr(gen,'module') else gen.state_dict(), f\"joint_resnet_ep{epoch+1}.pth\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n[!] Interrupted. Saving...\")\n",
    "        \n",
    "    torch.save(gen.module.state_dict() if hasattr(gen,'module') else gen.state_dict(), MODEL_SAVE_PATH)\n",
    "    torch.save(disc.module.state_dict() if hasattr(disc,'module') else disc.state_dict(), DISC_SAVE_PATH)\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Visualiser Script\n",
    "use the weight from above training script or replace by the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# 1. Paths to CLEANED TEST data\n",
    "RGB_TEST_DIR = '/kaggle/working/denoised_dataset_test' \n",
    "FLOW_TEST_DIR = '/kaggle/working/testing_optical_flow'\n",
    "\n",
    "# 2. Path to your saved JOINT model\n",
    "MODEL_PATH = 'Final_Models\/joint_resnet_generator.pth'\n",
    "\n",
    "TARGET_VIDEO = '10' # Change to any video ID like '02', '03'...\n",
    "IMG_SIZE = 256\n",
    "T_STEPS = 4 \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# =================================================\n",
    "\n",
    "# --- 1. ARCHITECTURE RE-DEFINITION ---\n",
    "# (Must match your training script exactly)\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels=512):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1); self.bn1 = nn.BatchNorm2d(channels); self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1); self.bn2 = nn.BatchNorm2d(channels)\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn2(self.conv2(self.relu(self.bn1(self.conv1(x))))) + x)\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels, 64, 7, 1, 3), nn.BatchNorm2d(64), nn.ReLU(inplace=True))\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True))\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True))\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True))\n",
    "        self.bottleneck = nn.Sequential(*[ResidualBlock(512) for _ in range(9)])\n",
    "    def forward(self, x):\n",
    "        return self.bottleneck(self.layer4(self.layer3(self.layer2(self.layer1(x)))))\n",
    "\n",
    "class JointRepresentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JointRepresentationModel, self).__init__()\n",
    "        self.spatial_encoder = ResNetEncoder(in_channels=12)\n",
    "        self.temporal_encoder = ResNetEncoder(in_channels=3)\n",
    "        self.dec1 = nn.Sequential(nn.ConvTranspose2d(1024, 256, 3, 2, 1, output_padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True))\n",
    "        self.dec2 = nn.Sequential(nn.ConvTranspose2d(256, 128, 3, 2, 1, output_padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True))\n",
    "        self.dec3 = nn.Sequential(nn.ConvTranspose2d(128, 64, 3, 2, 1, output_padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True))\n",
    "        self.final = nn.Conv2d(64, 3, 7, 1, 3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, rgb, flow):\n",
    "        f_spatial = self.spatial_encoder(rgb)\n",
    "        f_temporal = self.temporal_encoder(flow)\n",
    "        fused = torch.cat([f_spatial, f_temporal], dim=1)\n",
    "        return self.sigmoid(self.final(self.dec3(self.dec2(self.dec1(fused)))))\n",
    "\n",
    "# --- 2. INFERENCE & PLOTTING ---\n",
    "def visualize_joint_video():\n",
    "    print(f\"ðŸŽ¬ Processing Joint Representation Visualization for Video {TARGET_VIDEO}...\")\n",
    "    \n",
    "    # Setup Model\n",
    "    model = JointRepresentationModel().to(DEVICE)\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        state = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "        # Clean DataParallel keys if present\n",
    "        new_state = {k.replace('module.', ''): v for k, v in state.items()}\n",
    "        model.load_state_dict(new_state)\n",
    "    else:\n",
    "        print(f\"âŒ Model not found at {MODEL_PATH}\"); return\n",
    "    model.eval()\n",
    "\n",
    "    tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n",
    "\n",
    "    # Locate Data\n",
    "    rgb_path = os.path.join(RGB_TEST_DIR, TARGET_VIDEO)\n",
    "    flow_path = os.path.join(FLOW_TEST_DIR, TARGET_VIDEO)\n",
    "    \n",
    "    rgb_files = sorted(glob.glob(os.path.join(rgb_path, '*.jpg')))\n",
    "    \n",
    "    psnr_list = []\n",
    "    mse_list = []\n",
    "    frame_indices = []\n",
    "\n",
    "    print(\"ðŸƒ Running dual-stream inference...\")\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(rgb_files) - T_STEPS)):\n",
    "            # 1. Get RGB Sequence (Past 4 frames)\n",
    "            rgb_seq = [tf(Image.open(rgb_files[j]).convert('RGB')) for j in range(i, i + T_STEPS)]\n",
    "            rgb_tensor = torch.cat(rgb_seq, dim=0).unsqueeze(0).to(DEVICE) # (1, 12, 256, 256)\n",
    "            \n",
    "            # 2. Get Flow Frame (Corresponding to the prediction target frame)\n",
    "            # Assuming flow file has same name as the RGB frame it represents\n",
    "            target_frame_name = os.path.basename(rgb_files[i + T_STEPS])\n",
    "            flow_file = os.path.join(flow_path, target_frame_name)\n",
    "            \n",
    "            if not os.path.exists(flow_file): continue\n",
    "            \n",
    "            flow_tensor = tf(Image.open(flow_file).convert('RGB')).unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            # 3. Ground Truth Target (RGB)\n",
    "            target_tensor = tf(Image.open(rgb_files[i + T_STEPS]).convert('RGB')).unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            # 4. Predict\n",
    "            fake_rgb = model(rgb_tensor, flow_tensor)\n",
    "            \n",
    "            # 5. Calculate Error (MSE)\n",
    "            error = torch.mean((fake_rgb - target_tensor)**2).item()\n",
    "            mse_list.append(error)\n",
    "            frame_indices.append(i + T_STEPS)\n",
    "\n",
    "    # --- NORMALIZE & PLOT ---\n",
    "    mse_arr = np.array(mse_list)\n",
    "    # Map MSE to 0-1 Anomaly Score\n",
    "    if mse_arr.max() > mse_arr.min():\n",
    "        scores = (mse_arr - mse_arr.min()) / (mse_arr.max() - mse_arr.min())\n",
    "    else:\n",
    "        scores = mse_arr\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(frame_indices, scores, color='#2c3e50', linewidth=2, label='Joint Anomaly Score')\n",
    "    plt.fill_between(frame_indices, scores, color='#34495e', alpha=0.2)\n",
    "    \n",
    "    plt.title(f\"Joint Representation (RGB+Flow) Anomaly Profile: Video {TARGET_VIDEO}\", fontsize=15, fontweight='bold')\n",
    "    plt.xlabel(\"Frame Number\", fontsize=12)\n",
    "    plt.ylabel(\"Anomaly Score (Normalized MSE)\", fontsize=12)\n",
    "    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Potential Threshold')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_joint_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission script generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:03:11.589235Z",
     "iopub.status.busy": "2026-01-01T19:03:11.589057Z",
     "iopub.status.idle": "2026-01-01T19:03:19.723075Z",
     "shell.execute_reply": "2026-01-01T19:03:19.722118Z",
     "shell.execute_reply.started": "2026-01-01T19:03:11.589216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "RGB_TEST_DIR = '/kaggle/working/denoised_dataset_test' \n",
    "FLOW_TEST_DIR = '/kaggle/working/testing_optical_flow'\n",
    "MODEL_PATH = 'Final_Models\/joint_resnet_generator.pth'\n",
    "IMG_SIZE = 256\n",
    "T_STEPS = 4 \n",
    "\n",
    "# Setup for Dual GPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "GPU_IDS = [0, 1] if torch.cuda.device_count() > 1 else [0]\n",
    "# =================================================\n",
    "\n",
    "# --- 1. ARCHITECTURE RE-DEFINITION ---\n",
    "# (Keeping your ResidualBlock and ResNetEncoder as defined in your snippet)\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels=512):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1); self.bn1 = nn.BatchNorm2d(channels); self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1); self.bn2 = nn.BatchNorm2d(channels)\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn2(self.conv2(self.relu(self.bn1(self.conv1(x))))) + x)\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels, 64, 7, 1, 3), nn.BatchNorm2d(64), nn.ReLU(inplace=True))\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True))\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True))\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(256, 512, 3, 2, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True))\n",
    "        self.bottleneck = nn.Sequential(*[ResidualBlock(512) for _ in range(9)])\n",
    "    def forward(self, x):\n",
    "        return self.bottleneck(self.layer4(self.layer3(self.layer2(self.layer1(x)))))\n",
    "\n",
    "class JointRepresentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JointRepresentationModel, self).__init__()\n",
    "        self.spatial_encoder = ResNetEncoder(in_channels=12)\n",
    "        self.temporal_encoder = ResNetEncoder(in_channels=3)\n",
    "        self.dec1 = nn.Sequential(nn.ConvTranspose2d(1024, 256, 3, 2, 1, output_padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True))\n",
    "        self.dec2 = nn.Sequential(nn.ConvTranspose2d(256, 128, 3, 2, 1, output_padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True))\n",
    "        self.dec3 = nn.Sequential(nn.ConvTranspose2d(128, 64, 3, 2, 1, output_padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True))\n",
    "        self.final = nn.Conv2d(64, 3, 7, 1, 3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, rgb, flow):\n",
    "        f_spatial = self.spatial_encoder(rgb)\n",
    "        f_temporal = self.temporal_encoder(flow)\n",
    "        fused = torch.cat([f_spatial, f_temporal], dim=1)\n",
    "        return self.sigmoid(self.final(self.dec3(self.dec2(self.dec1(fused)))))\n",
    "\n",
    "# --- 2. INFERENCE ENGINE ---\n",
    "def run_full_inference():\n",
    "    # Model Initialization\n",
    "    model = JointRepresentationModel()\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        state = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "        new_state = {k.replace('module.', ''): v for k, v in state.items()}\n",
    "        model.load_state_dict(new_state)\n",
    "    \n",
    "    # Wrap for Dual GPU\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"ðŸš€ Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model, device_ids=GPU_IDS)\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n",
    "    video_folders = sorted([d for d in os.listdir(RGB_TEST_DIR) if os.path.isdir(os.path.join(RGB_TEST_DIR, d))])\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    print(f\"ðŸ“‚ Found {len(video_folders)} videos. Starting inference...\")\n",
    "\n",
    "    for video_id in video_folders:\n",
    "        rgb_path = os.path.join(RGB_TEST_DIR, video_id)\n",
    "        flow_path = os.path.join(FLOW_TEST_DIR, video_id)\n",
    "        rgb_files = sorted(glob.glob(os.path.join(rgb_path, '*.jpg')))\n",
    "        \n",
    "        video_mse_list = []\n",
    "        video_frame_ids = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(rgb_files) - T_STEPS):\n",
    "                # Data Preparation\n",
    "                rgb_seq = [tf(Image.open(rgb_files[j]).convert('RGB')) for j in range(i, i + T_STEPS)]\n",
    "                rgb_tensor = torch.cat(rgb_seq, dim=0).unsqueeze(0).to(DEVICE)\n",
    "                \n",
    "                target_frame_name = os.path.basename(rgb_files[i + T_STEPS])\n",
    "                flow_file = os.path.join(flow_path, target_frame_name)\n",
    "                \n",
    "                if not os.path.exists(flow_file): continue\n",
    "                \n",
    "                flow_tensor = tf(Image.open(flow_file).convert('RGB')).unsqueeze(0).to(DEVICE)\n",
    "                target_tensor = tf(Image.open(rgb_files[i + T_STEPS]).convert('RGB')).unsqueeze(0).to(DEVICE)\n",
    "                \n",
    "                # Model Prediction\n",
    "                fake_rgb = model(rgb_tensor, flow_tensor)\n",
    "                \n",
    "                # Error Calculation\n",
    "                mse = torch.mean((fake_rgb - target_tensor)**2).item()\n",
    "                video_mse_list.append(mse)\n",
    "                \n",
    "                # Format ID as VideoID_FrameNumber\n",
    "                frame_idx = target_frame_name.split('.')[0]\n",
    "                video_frame_ids.append(f\"{video_id}_{frame_idx}\")\n",
    "\n",
    "        # --- VIDEO-WISE NORMALIZATION ---\n",
    "        mse_arr = np.array(video_mse_list)\n",
    "        if len(mse_arr) > 0:\n",
    "            v_min, v_max = mse_arr.min(), mse_arr.max()\n",
    "            # Normalize scores between 0 and 1 for this specific video\n",
    "            norm_scores = (mse_arr - v_min) / (v_max - v_min + 1e-6) if v_max > v_min else mse_arr\n",
    "            \n",
    "            for fid, score in zip(video_frame_ids, norm_scores):\n",
    "                all_results.append({'ID': fid, 'Predicted': score})\n",
    "\n",
    "        print(f\"âœ… Video {video_id} processed.\")\n",
    "\n",
    "    # --- 3. SAVE CSV ---\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv('submission.csv', index=False)\n",
    "    print(f\"ðŸ’¾ CSV generated with {len(df)} rows. Filename: submission.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filling missing indexes and fixing ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # Added for safer number extraction\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "\n",
    "RGB_TEST_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\n",
    "INPUT_CSV = 'submission.csv'\n",
    "OUTPUT_CSV = 'final_submission.csv'\n",
    "T_STEPS = 4 \n",
    "# =================================================\n",
    "\n",
    "def extract_frame_num(filename):\n",
    "    # Extracts numbers from 'frame_01094.jpg' -> 1094\n",
    "    # Finds the last sequence of digits in the filename\n",
    "    matches = re.findall(r'(\\d+)', filename)\n",
    "    if matches:\n",
    "        return int(matches[-1]) # Takes the last number found (01094)\n",
    "    return None\n",
    "\n",
    "def reconstruct_final():\n",
    "    if not os.path.exists(INPUT_CSV):\n",
    "        print(f\"âŒ Error: {INPUT_CSV} not found.\")\n",
    "        return\n",
    "\n",
    "    raw_df = pd.read_csv(INPUT_CSV)\n",
    "    all_predictions = raw_df['Predicted'].tolist()\n",
    "    \n",
    "    print(f\"ðŸ“¥ Loaded {len(all_predictions)} predictions.\")\n",
    "    \n",
    "    # Sort folders numerically (1, 2, ... 10)\n",
    "    video_ids = sorted(\n",
    "        [d for d in os.listdir(RGB_TEST_DIR) if os.path.isdir(os.path.join(RGB_TEST_DIR, d))],\n",
    "        key=lambda x: int(x) if x.isdigit() else x\n",
    "    )\n",
    "\n",
    "    final_rows = []\n",
    "    preds_idx = 0 \n",
    "\n",
    "    for vid in video_ids:\n",
    "        vid_path = os.path.join(RGB_TEST_DIR, vid)\n",
    "        \n",
    "        # Get all jpg files\n",
    "        files = glob.glob(os.path.join(vid_path, '*.jpg'))\n",
    "        \n",
    "        # Create a list of (frame_number, full_path) tuples\n",
    "        frame_data = []\n",
    "        for f in files:\n",
    "            f_num = extract_frame_num(os.path.basename(f))\n",
    "            if f_num is not None:\n",
    "                frame_data.append(f_num)\n",
    "        \n",
    "        # Sort by frame number (Crucial!)\n",
    "        frame_nums = sorted(frame_data)\n",
    "        \n",
    "        total_frames = len(frame_nums)\n",
    "        if total_frames == 0:\n",
    "            print(f\"âš ï¸ Video {vid}: Found 0 valid images.\")\n",
    "            continue\n",
    "\n",
    "        # Expected predictions = Total Frames - Warmup Steps\n",
    "        expected_preds = total_frames - T_STEPS\n",
    "        \n",
    "        if expected_preds <= 0:\n",
    "            print(f\"âš ï¸ Video {vid}: Not enough frames ({total_frames}). Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Check if we have enough predictions left\n",
    "        if preds_idx + expected_preds > len(all_predictions):\n",
    "            print(f\"âŒ Error: Running out of predictions at Video {vid}!\")\n",
    "            print(f\"   Needed {expected_preds}, but only {len(all_predictions) - preds_idx} remain.\")\n",
    "            break\n",
    "            \n",
    "        # Extract predictions for this video\n",
    "        vid_scores = all_predictions[preds_idx : preds_idx + expected_preds]\n",
    "        preds_idx += expected_preds \n",
    "        \n",
    "        # --- NORMALIZE (Video-Wise) ---\n",
    "        v_scores = np.array(vid_scores)\n",
    "        v_min, v_max = v_scores.min(), v_scores.max()\n",
    "        \n",
    "        if v_max > v_min:\n",
    "            norm_scores = (v_scores - v_min) / (v_max - v_min)\n",
    "        else:\n",
    "            norm_scores = np.zeros_like(v_scores)\n",
    "\n",
    "        # --- FILL CSV ---\n",
    "        # 1. Warm-up frames (e.g. first 4 frames) -> Set to 0.0\n",
    "        for i in range(T_STEPS):\n",
    "            actual_num = frame_nums[i]\n",
    "            final_rows.append({\n",
    "                'ID': f\"{int(vid)}_{actual_num}\", # e.g. 1_1094\n",
    "                'Predicted': 0.0\n",
    "            })\n",
    "            \n",
    "        # 2. Predicted frames\n",
    "        for i, score in enumerate(norm_scores):\n",
    "            actual_num = frame_nums[i + T_STEPS]\n",
    "            final_rows.append({\n",
    "                'ID': f\"{int(vid)}_{actual_num}\", \n",
    "                'Predicted': score\n",
    "            })\n",
    "            \n",
    "        print(f\"âœ… Video {vid}: {total_frames} frames ({frame_nums[0]} -> {frame_nums[-1]})\")\n",
    "\n",
    "    # Final Save\n",
    "    if final_rows:\n",
    "        df_final = pd.DataFrame(final_rows)\n",
    "        df_final.to_csv(OUTPUT_CSV, index=False)\n",
    "        print(f\"\\nðŸŽ‰ SUCCESS! Saved {len(df_final)} rows to {OUTPUT_CSV}\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Failed to generate rows.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reconstruct_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid Normalising (good for visualisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "INPUT_CSV = '/kaggle/working/final_submission.csv'\n",
    "OUTPUT_CSV = 'submission_processed.csv'\n",
    "\n",
    "# The \"Center\" of your sigmoid (The threshold between Normal vs Anomaly)\n",
    "# You observed normals are < 0.1, so 0.1 is the tipping point.\n",
    "CENTER = 0.07\n",
    "\n",
    "# How hard to push values to 0 and 1.\n",
    "# Low (e.g. 10) = Gentle S-curve\n",
    "# High (e.g. 50) = Hard Step Function (Almost binary)\n",
    "STEEPNESS = 15\n",
    "# =================================================\n",
    "\n",
    "def sigmoid_transform(x, center, k):\n",
    "    \"\"\"\n",
    "    Custom Sigmoid:\n",
    "    x: Input score array\n",
    "    center: The value that maps to 0.5 (tipping point)\n",
    "    k: Steepness coefficient\n",
    "    \"\"\"\n",
    "    # 1. Shift x so the center is at 0\n",
    "    shifted = x - center\n",
    "    \n",
    "    # 2. Scale by steepness\n",
    "    scaled = k * shifted\n",
    "    \n",
    "    # 3. Apply standard sigmoid: 1 / (1 + e^-x)\n",
    "    return 1 / (1 + np.exp(-scaled))\n",
    "\n",
    "def process_and_visualize():\n",
    "    print(f\"ðŸ“‚ Loading {INPUT_CSV}...\")\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    \n",
    "    original_scores = df['Predicted'].values\n",
    "    \n",
    "    # --- APPLY TRANSFORM ---\n",
    "    print(f\"âš¡ Applying Sigmoid (Center={CENTER}, Steepness={STEEPNESS})...\")\n",
    "    new_scores = sigmoid_transform(original_scores, CENTER, STEEPNESS)\n",
    "    \n",
    "    df['Predicted'] = new_scores\n",
    "    \n",
    "    # --- VISUALIZATION (THE \"U\" GRAPH) ---\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Plot 1: Original Distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(original_scores, bins=50, color='blue', alpha=0.7)\n",
    "    plt.axvline(CENTER, color='red', linestyle='--', label=f'Center ({CENTER})')\n",
    "    plt.title(\"Original Scores (Before)\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Processed Distribution (Should look like a U)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(new_scores, bins=50, color='green', alpha=0.7)\n",
    "    plt.title(f\"Processed Scores (After)\\nSteepness: {STEEPNESS}\")\n",
    "    plt.xlabel(\"Score (0=Normal, 1=Anomaly)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"score_distribution_u_graph.png\")\n",
    "    print(\"ðŸ“¸ Saved visualization to 'score_distribution_u_graph.png'\")\n",
    "    plt.show()\n",
    "    \n",
    "    # --- SAVE ---\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"âœ… Saved processed scores to {OUTPUT_CSV}\")\n",
    "    print(df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_and_visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly window detection (smoothning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Use the file that gave you 0.66\n",
    "INPUT_CSV = '/kaggle/working/submission_processed.csv' \n",
    "OUTPUT_CSV = 'submission_boosted_unet.csv'\n",
    "\n",
    "# DILATION WINDOW\n",
    "# Avenue is 25fps. Events last ~1-2 seconds.\n",
    "# A window of 25 means we look +/- 12 frames around a peak.\n",
    "WINDOW_SIZE = 20\n",
    "# =================================================\n",
    "\n",
    "def boost_score():\n",
    "    print(f\"Reading {INPUT_CSV}...\")\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    \n",
    "    df['VideoID'] = df['ID'].apply(lambda x: x.split('_')[0])\n",
    "    \n",
    "    final_preds = []\n",
    "    \n",
    "    print(f\"Applying Peak Widening (Window={WINDOW_SIZE})...\")\n",
    "    \n",
    "    for vid, group in df.groupby('VideoID', sort=False):\n",
    "        raw = group['Predicted'].copy()\n",
    "        \n",
    "        # 1. ROLLING MAX (Dilation)\n",
    "        # This spreads the '1.0' scores to their neighbors.\n",
    "        # \"If frame T is bad, then frame T+1 is probably bad too.\"\n",
    "        dilated = raw.rolling(window=WINDOW_SIZE, center=True, min_periods=1).max()\n",
    "        \n",
    "        # 2. LIGHT SMOOTHING (Average)\n",
    "        # Rolling max creates \"blocky\" steps. We smooth the edges slightly.\n",
    "        smoothed = dilated.rolling(window=10, center=True, min_periods=1).mean()\n",
    "        \n",
    "        \n",
    "        \n",
    "        final_preds.extend(smoothed.values)\n",
    "        \n",
    "    df['Predicted'] = final_preds\n",
    "    \n",
    "    # Final Normalize (Safety)\n",
    "    df['Predicted'] = np.clip(df['Predicted'], 0.0, 1.0)\n",
    "    \n",
    "    # --- VISUALIZATION ---\n",
    "    # Let's see how much \"fatter\" the detection became\n",
    "    v05 = df[df['ID'].str.startswith('05_')]\n",
    "    if not v05.empty:\n",
    "        x = v05['ID'].apply(lambda x: int(x.split('_')[1]))\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # We need to reload original to compare\n",
    "        orig = pd.read_csv(INPUT_CSV)\n",
    "        orig_v05 = orig[orig['ID'].str.startswith('05_')]\n",
    "        \n",
    "        plt.plot(x, orig_v05['Predicted'], color='gray', alpha=0.5, label='Original (0.66)')\n",
    "        plt.plot(x, v05['Predicted'], color='green', linewidth=2, label='Widened (Boosted)')\n",
    "        \n",
    "        plt.title(\"Peak Widening Effect (Video 05)\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Save\n",
    "    out = df[['ID', 'Predicted']]\n",
    "    out.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"Saved boosted scores to {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    boost_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is used in final essemble code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final visualiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "INPUT_CSV = '/kaggle/working/submission_boosted_unet.csv'\n",
    "THRESHOLD = 0.5 \n",
    "# =================================================\n",
    "\n",
    "def visualize_inline():\n",
    "    print(f\"ðŸ“‚ Loading {INPUT_CSV}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ Error: File not found. Did you run the previous step?\")\n",
    "        return\n",
    "\n",
    "    # 1. Parse 'ID' (e.g., '1_939') back into Video and Frame columns\n",
    "    print(\"ðŸ”„ Parsing data...\")\n",
    "    split_data = df['ID'].str.split('_', expand=True)\n",
    "    df['video_id'] = split_data[0].astype(int)\n",
    "    df['frame_id'] = split_data[1].astype(int)\n",
    "    \n",
    "    # 2. Get unique videos\n",
    "    videos = sorted(df['video_id'].unique())\n",
    "    print(f\"ðŸŒ Visualizing {len(videos)} videos...\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 3. Loop and Plot Inline\n",
    "    for vid in videos:\n",
    "        # Filter data for this video\n",
    "        vid_df = df[df['video_id'] == vid].sort_values(by='frame_id')\n",
    "        \n",
    "        frames = vid_df['frame_id'].values\n",
    "        scores = vid_df['Predicted'].values\n",
    "        \n",
    "        # Create Plot\n",
    "        plt.figure(figsize=(14, 4)) # Wide and short for better scrolling\n",
    "        \n",
    "        # Plot the Score Line\n",
    "        plt.plot(frames, scores, color='#007acc', linewidth=2, label='Anomaly Score')\n",
    "        \n",
    "        # Fill area under curve for anomalies (Red Zones)\n",
    "        plt.fill_between(frames, scores, THRESHOLD, where=(scores >= THRESHOLD), \n",
    "                         interpolate=True, color='red', alpha=0.3, label='Anomaly Detected')\n",
    "        \n",
    "        # Add Threshold Line\n",
    "        plt.axhline(y=THRESHOLD, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Styling\n",
    "        plt.title(f\"Video {vid}: Anomaly Profile\", fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Frame Number\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(loc='upper right')\n",
    "        \n",
    "        # SHOW PLOT\n",
    "        plt.show()\n",
    "        print(\"\\n\") # Spacing\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_inline()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15067517,
     "sourceId": 126766,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
