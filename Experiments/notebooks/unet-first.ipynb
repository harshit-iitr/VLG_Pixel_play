{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":126766,"databundleVersionId":15067517,"sourceType":"competition"},{"sourceId":697643,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":529173,"modelId":543190}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom tqdm import tqdm # Progress bar\n\n# ================= CONFIGURATION =================\n# Path to the CORRUPTED testing videos\nTEST_DATA_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\n\n# Path where we will save the CLEANED videos\nCLEAN_DATA_DIR = '/kaggle/working/cleaned_testing_videos'\n\nMODEL_PATH = '/kaggle/input/flipercorrectorvlg/pytorch/default/1/rotnet_model(1).pth'\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\ndef clean_dataset():\n    print(f\"Processing on: {DEVICE}\")\n    \n    # 1. Load the Trained RotNet\n    model = models.resnet18(pretrained=False) # No need to download weights again\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, 2) # Matches our binary training\n    \n    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n    model = model.to(DEVICE)\n    model.eval()\n    \n    # Standard transform for the model input\n    # Note: We do NOT augment here, just resize/norm\n    preprocess = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    # 2. Find all images\n    # We walk through the directory to keep structure\n    image_paths = sorted(glob.glob(os.path.join(TEST_DATA_DIR, '**', '*.jpg'), recursive=True))\n    print(f\"Found {len(image_paths)} frames to process.\")\n    \n    # 3. Processing Loop\n    flip_count = 0\n    \n    for img_path in tqdm(image_paths, desc=\"Cleaning\"):\n        # A. Setup paths\n        # Get relative path (e.g., \"01/frame_0001.jpg\") to maintain structure\n        rel_path = os.path.relpath(img_path, TEST_DATA_DIR)\n        save_path = os.path.join(CLEAN_DATA_DIR, rel_path)\n        \n        # Create folder if not exists\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        \n        # B. Predict Rotation\n        image = Image.open(img_path).convert('RGB')\n        input_tensor = preprocess(image).unsqueeze(0).to(DEVICE)\n        \n        with torch.no_grad():\n            outputs = model(input_tensor)\n            _, predicted = torch.max(outputs, 1)\n            label = predicted.item()\n            \n        # C. Fix and Save\n        # Label 0 = Upright (Keep as is)\n        # Label 1 = Flipped (Needs 180 rotation to fix)\n        \n        if label == 1:\n            # It was detected as Upside Down, so we rotate it -180 (or 180) to fix\n            fixed_image = image.transpose(Image.FLIP_TOP_BOTTOM) \n            flip_count += 1\n        else:\n            fixed_image = image\n            \n        # Save the fixed image\n        fixed_image.save(save_path)\n\n    print(\"-\" * 30)\n    print(\"Cleaning Complete!\")\n    print(f\"Total Images: {len(image_paths)}\")\n    print(f\"Images Flipped/Fixed: {flip_count}\")\n    print(f\"Cleaned dataset saved to: {CLEAN_DATA_DIR}\")\n\nif __name__ == \"__main__\":\n    clean_dataset()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# ================= CONFIGURATION =================\nTRAIN_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos'\nSAVE_PATH = 'multiscale_unet_conditional.pth'\n\nIMG_SIZE = 256\nCLIP_LEN = 4     # 4 frames input\nBATCH_SIZE = 16  # 8 per GPU\nEPOCHS = 50\nLR_G = 2e-4\nLR_D = 2e-5\n\n# [cite_start]Loss Weights [cite: 2419, 2782]\nLAMBDA_INT = 2.0\nLAMBDA_GD = 1.0\nLAMBDA_ADV = 0.05\nLAMBDA_FLOW = 2.0 \n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\n# --- 1. ARCHITECTURE COMPONENTS (Generator) ---\n# [Unchanged from previous robust implementation]\n\nclass AsymmetricConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super(AsymmetricConv, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(kernel_size//2, 0))\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(1, kernel_size), padding=(0, kernel_size//2))\n        self.bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        return self.relu(self.bn(self.conv2(self.relu(self.conv1(x)))))\n\nclass ResidualSkipConnection(nn.Module):\n    def __init__(self, channels):\n        super(ResidualSkipConnection, self).__init__()\n        self.block = nn.Sequential(\n            AsymmetricConv(channels, channels),\n            AsymmetricConv(channels, channels)\n        )\n        self.shortcut = nn.Conv2d(channels, channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        return self.relu(self.block(x) + self.shortcut(x))\n\nclass ShortcutInceptionModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ShortcutInceptionModule, self).__init__()\n        w_6 = out_channels // 6\n        w_3 = out_channels // 3\n        w_2 = out_channels - (w_6 + w_3)\n\n        self.branch1 = AsymmetricConv(in_channels, w_6)\n        self.branch2 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3))\n        self.branch3 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3), AsymmetricConv(w_3, w_2))\n        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        concat = torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1)\n        return self.relu(concat + self.shortcut(x))\n\nclass MultiScaleUNet(nn.Module):\n    def __init__(self, in_channels=12, out_channels=3):\n        super(MultiScaleUNet, self).__init__()\n        # Encoder\n        self.sim1 = ShortcutInceptionModule(in_channels, 96); self.pool1 = nn.MaxPool2d(2)\n        self.sim2 = ShortcutInceptionModule(96, 192);         self.pool2 = nn.MaxPool2d(2)\n        self.sim3 = ShortcutInceptionModule(192, 384);        self.pool3 = nn.MaxPool2d(2)\n        self.sim4 = ShortcutInceptionModule(384, 768)\n\n        # Skip Connections\n        self.rsc1 = nn.Sequential(*[ResidualSkipConnection(96) for _ in range(4)])\n        self.rsc2 = nn.Sequential(*[ResidualSkipConnection(192) for _ in range(3)])\n        self.rsc3 = nn.Sequential(*[ResidualSkipConnection(384) for _ in range(2)])\n\n        # Decoder\n        self.sim5 = ShortcutInceptionModule(768, 384);   self.up1 = nn.ConvTranspose2d(384, 384, 2, 2)\n        self.sim6 = ShortcutInceptionModule(768, 192);   self.up2 = nn.ConvTranspose2d(192, 192, 2, 2)\n        self.sim7 = ShortcutInceptionModule(384, 96);    self.up3 = nn.ConvTranspose2d(96, 96, 2, 2)\n        self.sim8 = ShortcutInceptionModule(192, 96)\n        self.final = nn.Conv2d(96, out_channels, 3, padding=1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        e1 = self.sim1(x);        p1 = self.pool1(e1)\n        e2 = self.sim2(p1);       p2 = self.pool2(e2)\n        e3 = self.sim3(p2);       p3 = self.pool3(e3)\n        e4 = self.sim4(p3)\n\n        d1 = self.sim5(e4);       u1 = self.up1(d1)\n        cat1 = torch.cat([u1, self.rsc3(e3)], dim=1)\n\n        d2 = self.sim6(cat1);     u2 = self.up2(d2)\n        cat2 = torch.cat([u2, self.rsc2(e2)], dim=1)\n\n        d3 = self.sim7(cat2);     u3 = self.up3(d3)\n        cat3 = torch.cat([u3, self.rsc1(e1)], dim=1)\n\n        d4 = self.sim8(cat3)\n        return self.tanh(self.final(d4))\n\n# --- 2. CONDITIONAL PATCH DISCRIMINATOR (FIXED) ---\nclass ConditionalPatchDiscriminator(nn.Module):\n    def __init__(self, in_channels=6): # 3 (Current) + 3 (Past Condition)\n        super(ConditionalPatchDiscriminator, self).__init__()\n        \n        def disc_block(in_f, out_f, bn=True):\n            block = [nn.Conv2d(in_f, out_f, 4, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n            if bn: block.append(nn.BatchNorm2d(out_f))\n            return block\n\n        self.model = nn.Sequential(\n            *disc_block(in_channels, 64, bn=False), # 128x128\n            *disc_block(64, 128),                   # 64x64\n            *disc_block(128, 256),                  # 32x32\n            nn.Conv2d(256, 1, 4, padding=1)         # 32x32 (PatchGAN Map)\n        )\n\n    def forward(self, img_A, img_B):\n        # Concatenate condition (Last Frame) and target (Current Frame)\n        img_input = torch.cat((img_A, img_B), 1)\n        return self.model(img_input)\n\n# --- 3. LOSSES ---\ndef gradient_loss(gen_frames, gt_frames):\n    def gradient(x):\n        h_x = x.size()[-2]\n        w_x = x.size()[-1]\n        x_h = torch.abs(x[:, :, 1:, :] - x[:, :, :h_x-1, :])\n        x_w = torch.abs(x[:, :, :, 1:] - x[:, :, :, :w_x-1])\n        return x_h, x_w\n    gen_h, gen_w = gradient(gen_frames)\n    gt_h, gt_w = gradient(gt_frames)\n    return torch.mean(torch.abs(gen_h - gt_h)) + torch.mean(torch.abs(gen_w - gt_w))\n\ndef flow_loss(gen_frames, gt_frames, prev_frames):\n    flow_gen = torch.abs(gen_frames - prev_frames)\n    flow_gt = torch.abs(gt_frames - prev_frames)\n    return torch.mean(torch.abs(flow_gen - flow_gt)) # L1 Loss for robustness\n\n# --- 4. DATASET ---\nclass AvenueTrainDataset(Dataset):\n    def __init__(self, root_dir, clip_len=4, img_size=256):\n        self.clip_len = clip_len\n        self.samples = []\n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        \n        videos = sorted(os.listdir(root_dir))\n        for vid in videos:\n            path = os.path.join(root_dir, vid)\n            if not os.path.isdir(path): continue\n            frames = sorted(glob.glob(os.path.join(path, '*.jpg')))\n            if len(frames) < clip_len + 1: continue\n            \n            for i in range(len(frames) - clip_len):\n                self.samples.append(frames[i : i + clip_len + 1])\n\n    def __len__(self): return len(self.samples)\n\n    def __getitem__(self, idx):\n        paths = self.samples[idx]\n        imgs = [self.transform(Image.open(p).convert('RGB')) for p in paths]\n        \n        input_seq = torch.cat(imgs[:-1], dim=0) # 12 channels\n        target_frame = imgs[-1]                 # 3 channels (t+1)\n        last_input_frame = imgs[-2]             # 3 channels (t) - For Conditioning\n        \n        return input_seq, target_frame, last_input_frame\n\n# --- 5. TRAINING LOOP (CONDITIONAL GAN) ---\ndef train():\n    print(f\"Initializing Conditional Multi-scale U-Net Training on {DEVICE}...\")\n    \n    # Init Models\n    generator = MultiScaleUNet().to(DEVICE)\n    # Discriminator takes 6 channels: 3 (Condition/Last Frame) + 3 (Target/Generated)\n    discriminator = ConditionalPatchDiscriminator(in_channels=6).to(DEVICE)\n    \n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n        generator = nn.DataParallel(generator)\n        discriminator = nn.DataParallel(discriminator)\n        \n    opt_g = optim.Adam(generator.parameters(), lr=LR_G)\n    opt_d = optim.Adam(discriminator.parameters(), lr=LR_D)\n    \n    criterion_gan = nn.MSELoss() # LSGAN is more stable than BCE\n    criterion_pixel = nn.MSELoss()\n    \n    dataset = AvenueTrainDataset(TRAIN_DIR, CLIP_LEN, IMG_SIZE)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=12, pin_memory=True)\n    \n    try:\n        for epoch in range(EPOCHS):\n            generator.train(); discriminator.train()\n            pbar = tqdm(loader, desc=f\"Ep {epoch+1}/{EPOCHS}\")\n            \n            for inputs, targets, last_frames in pbar:\n                inputs = inputs.to(DEVICE)\n                targets = targets.to(DEVICE)\n                last_frames = last_frames.to(DEVICE) # Condition for D\n                \n                # ==========================\n                #  Train Discriminator (D)\n                # ==========================\n                opt_d.zero_grad()\n                \n                # Real: D(LastFrame, RealTarget) -> 1\n                real_out = discriminator(last_frames, targets)\n                loss_real = criterion_gan(real_out, torch.ones_like(real_out))\n                \n                # Fake: D(LastFrame, FakeTarget) -> 0\n                fake_frame = generator(inputs)\n                fake_out = discriminator(last_frames, fake_frame.detach()) # Detach G\n                loss_fake = criterion_gan(fake_out, torch.zeros_like(fake_out))\n                \n                loss_d = 0.5 * (loss_real + loss_fake)\n                loss_d.backward()\n                opt_d.step()\n                \n                # ==========================\n                #  Train Generator (G)\n                # ==========================\n                opt_g.zero_grad()\n                \n                # 1. Adversarial Loss: D(LastFrame, FakeTarget) -> 1\n                fake_out_g = discriminator(last_frames, fake_frame)\n                l_adv = criterion_gan(fake_out_g, torch.ones_like(fake_out_g))\n                \n                # 2. Pixel Intensity Loss\n                l_int = criterion_pixel(fake_frame, targets)\n                \n                # 3. Gradient Loss\n                l_gd = gradient_loss(fake_frame, targets)\n                \n                # 4. Flow Loss (Temporal Consistency)\n                l_flow = flow_loss(fake_frame, targets, last_frames)\n                \n                # Total Loss\n                loss_g = (LAMBDA_INT * l_int) + \\\n                         (LAMBDA_GD * l_gd) + \\\n                         (LAMBDA_ADV * l_adv) + \\\n                         (LAMBDA_FLOW * l_flow)\n                         \n                loss_g.backward()\n                opt_g.step()\n                \n                pbar.set_postfix({\n                    'D_loss': f\"{loss_d.item():.4f}\",\n                    'G_Adv': f\"{l_adv.item():.4f}\",\n                    'G_Int': f\"{l_int.item():.4f}\",\n                    'G_Flow': f\"{l_flow.item():.4f}\"\n                })\n            \n            torch.save(generator.module.state_dict(), f\"unet_conditional_ep{epoch}.pth\")\n            \n    except KeyboardInterrupt:\n        print(\"\\nTraining Interrupted! Saving checkpoint...\")\n        state = generator.module.state_dict() if hasattr(generator, 'module') else generator.state_dict()\n        torch.save(state, 'INTERRUPTED_conditional.pth')\n        print(\"Saved safely.\")\n\nif __name__ == \"__main__\":\n    train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T08:10:11.372293Z","iopub.execute_input":"2025-12-30T08:10:11.372998Z","iopub.status.idle":"2025-12-30T12:06:23.809740Z","shell.execute_reply.started":"2025-12-30T08:10:11.372966Z","shell.execute_reply":"2025-12-30T12:06:23.808978Z"}},"outputs":[{"name":"stdout","text":"Initializing Conditional Multi-scale U-Net Training on cuda...\nUsing 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\nEp 1/50: 100%|██████████| 572/572 [26:17<00:00,  2.76s/it, D_loss=0.2546, G_Adv=0.2551, G_Int=0.0010, G_Flow=0.0192]\nEp 2/50: 100%|██████████| 572/572 [26:12<00:00,  2.75s/it, D_loss=0.2530, G_Adv=0.2515, G_Int=0.0010, G_Flow=0.0147]\nEp 3/50: 100%|██████████| 572/572 [26:13<00:00,  2.75s/it, D_loss=0.2509, G_Adv=0.2567, G_Int=0.0009, G_Flow=0.0188]\nEp 4/50: 100%|██████████| 572/572 [26:12<00:00,  2.75s/it, D_loss=0.2519, G_Adv=0.2485, G_Int=0.0027, G_Flow=0.0289]\nEp 5/50: 100%|██████████| 572/572 [26:10<00:00,  2.75s/it, D_loss=0.2512, G_Adv=0.2526, G_Int=0.0015, G_Flow=0.0199]\nEp 6/50: 100%|██████████| 572/572 [26:10<00:00,  2.75s/it, D_loss=0.2506, G_Adv=0.2502, G_Int=0.0013, G_Flow=0.0177]\nEp 7/50: 100%|██████████| 572/572 [26:09<00:00,  2.74s/it, D_loss=0.2505, G_Adv=0.2439, G_Int=0.0007, G_Flow=0.0178]\nEp 8/50: 100%|██████████| 572/572 [26:12<00:00,  2.75s/it, D_loss=0.2493, G_Adv=0.2528, G_Int=0.0022, G_Flow=0.0266]\nEp 9/50: 100%|██████████| 572/572 [26:10<00:00,  2.75s/it, D_loss=0.2503, G_Adv=0.2545, G_Int=0.0009, G_Flow=0.0156]\nEp 10/50:   1%|          | 4/572 [00:17<41:15,  4.36s/it, D_loss=0.2504, G_Adv=0.2515, G_Int=0.0010, G_Flow=0.0156]  \n","output_type":"stream"},{"name":"stdout","text":"\nTraining Interrupted! Saving checkpoint...\nSaved safely.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}