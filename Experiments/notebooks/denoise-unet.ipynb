{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":126766,"databundleVersionId":15067517,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":14313854,"sourceType":"datasetVersion","datasetId":9137653},{"sourceId":14343334,"sourceType":"datasetVersion","datasetId":9158240},{"sourceId":699370,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":530588,"modelId":544492},{"sourceId":700847,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":531773,"modelId":545572},{"sourceId":703638,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":534035,"modelId":547703}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics","metadata":{"_uuid":"2ac061a7-c7fe-400d-839c-18297f572cc1","_cell_guid":"2a3ed0c3-c59b-4ee2-8639-aa836cbc7bfd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\nimport glob\nfrom tqdm import tqdm\n\n# ================= CONFIGURATION =================\n# Point this to the folder containing '01', '02', etc.\n# Based on your screenshot, it ends in 'testing_videos'\ndataset_root = r\"/kaggle/working/cleaned_testing_videos\"\n\n\n# Where to save the output MP4 files\noutput_folder = \"rendered_videos\"\n\n# Frame rate (Frames Per Second). \n# 25 or 30 is standard. Lower it (e.g., 10) if the video feels too fast.\nfps = 25 \n# =================================================\n\ndef create_video_from_frames(video_folder_path, output_path, fps):\n    # 1. Find all images (jpg, png, jpeg)\n    images = []\n    for ext in ['*.jpg', '*.jpeg', '*.png']:\n        images.extend(glob.glob(os.path.join(video_folder_path, ext)))\n    \n    if not images:\n        print(f\"No images found in {video_folder_path}\")\n        return\n\n    # 2. Sort them numerically! \n    # Standard sort usually fails on \"frame_1\" vs \"frame_10\", \n    # but your data has padding (0093), so standard sort usually works.\n    # We use a lambda just to be safe.\n    try:\n        images.sort(key=lambda x: int(os.path.basename(x).split('_')[1].split('.')[0]))\n    except:\n        images.sort() # Fallback if naming convention is different\n\n    # 3. Read the first frame to get dimensions\n    frame = cv2.imread(images[0])\n    height, width, layers = frame.shape\n    size = (width, height)\n\n    # 4. Initialize VideoWriter\n    # 'mp4v' is a standard codec for .mp4\n    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, size)\n\n    # 5. Write frames\n    video_name = os.path.basename(output_path)\n    for image_path in tqdm(images, desc=f\"Rendering {video_name}\", unit=\"frame\"):\n        img = cv2.imread(image_path)\n        out.write(img)\n\n    out.release()\n    print(f\"Saved: {output_path}\")\n\ndef main():\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    # Get list of video directories (01, 02, etc.)\n    # We assume directories inside 'testing_videos' are the video clips\n    video_dirs = [d for d in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, d))]\n    video_dirs.sort()\n\n    print(f\"Found {len(video_dirs)} video folders. Starting conversion...\")\n\n    for video_dir in video_dirs:\n        full_path = os.path.join(dataset_root, video_dir)\n        save_path = os.path.join(output_folder, f\"video_{video_dir}.mp4\")\n        \n        create_video_from_frames(full_path, save_path, fps)\n\n    print(\"\\nDone! Check the 'rendered_videos' folder.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"2967474a-800b-47e3-8559-4ec5006eb687","_cell_guid":"f108c975-4e18-42b4-a74c-afa0e9c95c49","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-12-30T18:00:16.410215Z","iopub.execute_input":"2025-12-30T18:00:16.411117Z","iopub.status.idle":"2025-12-30T18:01:01.596720Z","shell.execute_reply.started":"2025-12-30T18:00:16.411085Z","shell.execute_reply":"2025-12-30T18:01:01.596103Z"}},"outputs":[{"name":"stdout","text":"Found 21 video folders. Starting conversion...\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_01.mp4: 100%|██████████| 499/499 [00:01<00:00, 255.10frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_01.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_02.mp4: 100%|██████████| 1211/1211 [00:04<00:00, 261.50frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_02.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_03.mp4: 100%|██████████| 737/737 [00:02<00:00, 255.51frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_03.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_04.mp4: 100%|██████████| 947/947 [00:03<00:00, 258.81frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_04.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_05.mp4: 100%|██████████| 1007/1007 [00:03<00:00, 265.85frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_05.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_06.mp4: 100%|██████████| 627/627 [00:02<00:00, 268.46frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_06.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_07.mp4: 100%|██████████| 588/588 [00:02<00:00, 262.29frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_07.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_08.mp4: 100%|██████████| 36/36 [00:00<00:00, 239.82frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_08.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_09.mp4: 100%|██████████| 359/359 [00:01<00:00, 268.43frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_09.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_10.mp4: 100%|██████████| 722/722 [00:02<00:00, 257.58frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_10.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_11.mp4: 100%|██████████| 472/472 [00:01<00:00, 266.89frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_11.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_12.mp4: 100%|██████████| 735/735 [00:02<00:00, 259.84frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_12.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_13.mp4: 100%|██████████| 528/528 [00:01<00:00, 265.20frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_13.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_14.mp4: 100%|██████████| 496/496 [00:01<00:00, 270.71frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_14.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_15.mp4: 100%|██████████| 732/732 [00:02<00:00, 264.27frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_15.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_16.mp4: 100%|██████████| 740/740 [00:02<00:00, 269.77frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_16.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_17.mp4: 100%|██████████| 417/417 [00:01<00:00, 262.86frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_17.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_18.mp4: 100%|██████████| 275/275 [00:01<00:00, 253.72frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_18.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_19.mp4: 100%|██████████| 229/229 [00:00<00:00, 249.90frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_19.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_20.mp4: 100%|██████████| 273/273 [00:01<00:00, 259.94frame/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_20.mp4\n","output_type":"stream"},{"name":"stderr","text":"Rendering video_21.mp4: 100%|██████████| 76/76 [00:00<00:00, 255.13frame/s]","output_type":"stream"},{"name":"stdout","text":"Saved: rendered_videos/video_21.mp4\n\nDone! Check the 'rendered_videos' folder.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport cv2\nimport os\nimport glob\nimport numpy as np\nfrom tqdm import tqdm\nimport warnings\nimport logging\nimport sys\n\n# ================= WARNING SUPPRESSION =================\n# 1. Python Warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 2. YOLO/Torch Hub Logging (The noisy part)\nlogging.getLogger(\"utils.general\").setLevel(logging.ERROR)\nlogging.getLogger(\"models.yolo\").setLevel(logging.ERROR)\nlogging.getLogger(\"torch.hub\").setLevel(logging.ERROR)\n\n# 3. Environment Flags\nos.environ[\"YOLO_VERBOSE\"] = \"False\"\n# =======================================================\n\n# ================= CONFIGURATION =================\n# Input: Your Cleaned Training Videos\nINPUT_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos'\n\n# Output: The Masked Frames for AutoEncoder Training\nOUTPUT_DIR = '/kaggle/working/training_videos_masked_context'\n\n# COCO Classes to KEEP (Context)\n# 0: Person\n# 1-8: Vehicles (Bicycle, Car, Motorcycle, Airplane, Bus, Train, Truck, Boat)\n# 24-28: Accessories (Backpack, Umbrella, Handbag, Tie, Suitcase)\n# 32: Sports Ball\nKEEP_CLASSES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 24, 25, 26, 27, 28, 32]\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\ndef apply_silent_context_masking():\n    print(f\"Loading YOLOv5s (Silent Mode) on {DEVICE}...\")\n    \n    # Redirect stdout to suppress the \"Fusing layers...\" print from YOLO\n    sys.stdout = open(os.devnull, 'w')\n    try:\n        try:\n            model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, verbose=False)\n        except:\n            model = torch.hub.load('/root/.cache/torch/hub/ultralytics_yolov5_master', 'custom', path='yolov5s.pt', source='local', verbose=False)\n    finally:\n        # Restore stdout\n        sys.stdout = sys.__stdout__\n        \n    model.to(DEVICE).eval()\n    model.conf = 0.25\n\n    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)\n\n    videos = sorted(os.listdir(INPUT_DIR))\n    print(f\"Processing {len(videos)} training videos...\")\n\n    for vid in tqdm(videos):\n        vid_path = os.path.join(INPUT_DIR, vid)\n        if not os.path.isdir(vid_path): continue\n        \n        save_path = os.path.join(OUTPUT_DIR, vid)\n        os.makedirs(save_path, exist_ok=True)\n        \n        frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n        \n        for f_path in frames:\n            img = cv2.imread(f_path)\n            if img is None: continue\n            \n            # Inference\n            results = model(img[..., ::-1], size=640)\n            preds = results.xyxy[0].cpu().numpy()\n            \n            # Create Black Mask\n            mask = np.zeros_like(img, dtype=np.uint8)\n            \n            h, w, _ = img.shape\n            padding = 15 \n            \n            for *xyxy, conf, cls in preds:\n                if int(cls) in KEEP_CLASSES:\n                    x1, y1, x2, y2 = map(int, xyxy)\n                    \n                    x1 = max(0, x1 - padding)\n                    y1 = max(0, y1 - padding)\n                    x2 = min(w, x2 + padding)\n                    y2 = min(h, y2 + padding)\n                    \n                    mask[y1:y2, x1:x2] = img[y1:y2, x1:x2]\n            \n            cv2.imwrite(os.path.join(save_path, os.path.basename(f_path)), mask)\n\n    print(f\"Done! Masked training data saved to: {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    apply_silent_context_masking()","metadata":{"_uuid":"2fb5b2a8-f1ea-4f5a-8f81-d0c5d81043d7","_cell_guid":"4d8761b5-00b1-4449-b9f8-f392d9f388f7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport gc\nimport torch.backends.cudnn as cudnn\n\n# ================= CONFIGURATION =================\nTRAIN_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos'\nMODEL_SAVE_PATH = 'st_ae_fast_256.pth'\n\nIMG_SIZE = 256\nBATCH_SIZE = 16      # Increased further (smaller model + RAM cache)\nEPOCHS = 20\nCLIP_LEN = 16\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ncudnn.benchmark = True\n# =================================================\n\n# --- 1. RAM-CACHED DATASET ---\nclass CachedVideoDataset(Dataset):\n    def __init__(self, root_dir, clip_len=16, img_size=256):\n        self.clip_len = clip_len\n        self.samples = []\n        \n        # Define transform once\n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor()\n        ])\n        \n        print(\">>> Pre-loading dataset into RAM (This takes 1-2 mins but makes training 10x faster)...\")\n        \n        video_folders = sorted([f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))])\n        \n        # Cache for loaded tensors to avoid duplicate reads\n        # Dict structure: { 'video_path/frame_01.jpg': tensor_data, ... }\n        self.cache = {}\n        \n        for vid in tqdm(video_folders, desc=\"Caching Videos\"):\n            vid_path = os.path.join(root_dir, vid)\n            frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n            \n            if len(frames) < 2 * clip_len: continue\n            \n            # Load all frames for this video into RAM immediately\n            vid_tensors = []\n            for f_path in frames:\n                # Load, Resize, ToTensor immediately. Store float32 tensor in RAM.\n                img = Image.open(f_path).convert('RGB')\n                vid_tensors.append(self.transform(img))\n            \n            # Create indices\n            # Stride 2 to save RAM but keep variety\n            for i in range(0, len(vid_tensors) - (2 * clip_len) + 1, 2):\n                # We store INDICES (integers), not copies of data\n                # (video_index, start_frame_index)\n                self.samples.append((vid_tensors, i))\n                \n        print(f\"Cached {len(self.samples)} clips in RAM.\")\n\n    def __len__(self): return len(self.samples)\n\n    def __getitem__(self, idx):\n        # Retrieve from RAM\n        vid_tensors, start_idx = self.samples[idx]\n        \n        # Slicing a list is fast\n        in_frames = vid_tensors[start_idx : start_idx + self.clip_len]\n        tgt_frames = vid_tensors[start_idx + self.clip_len : start_idx + (2 * self.clip_len)]\n        \n        # Stack: (T, C, H, W) -> Permute to (C, T, H, W)\n        return (torch.stack(in_frames, dim=0).permute(1, 0, 2, 3), \n                torch.stack(tgt_frames, dim=0).permute(1, 0, 2, 3))\n\n# --- 2. OPTIMIZED ARCHITECTURE (Lighter Bottleneck) ---\nclass STAutoEncoder_Optimized(nn.Module):\n    def __init__(self): \n        super(STAutoEncoder_Optimized, self).__init__()\n        \n        # Encoder: 32 -> 48 -> 64 -> 128 -> 256 (Capped at 256 to save speed)\n        self.conv1 = nn.Conv3d(3, 32, 3, padding=1);   self.bn1 = nn.BatchNorm3d(32);  self.pool1 = nn.MaxPool3d((1, 2, 2))\n        self.conv2 = nn.Conv3d(32, 48, 3, padding=1);  self.bn2 = nn.BatchNorm3d(48);  self.pool2 = nn.MaxPool3d((2, 2, 2))\n        self.conv3 = nn.Conv3d(48, 64, 3, padding=1);  self.bn3 = nn.BatchNorm3d(64);  self.pool3 = nn.MaxPool3d((2, 2, 2))\n        self.conv4 = nn.Conv3d(64, 128, 3, padding=1); self.bn4 = nn.BatchNorm3d(128); self.pool4 = nn.MaxPool3d((2, 2, 2))\n        self.conv5 = nn.Conv3d(128, 256, 3, padding=1);self.bn5 = nn.BatchNorm3d(256) \n        self.relu = nn.LeakyReLU(0.1, inplace=True) # Inplace saves memory\n        \n        # Branch A: Reconstruction (256 -> 3)\n        self.rec_d1 = nn.ConvTranspose3d(256, 128, 3, (2,2,2), 1, (1,1,1)); self.rbn1 = nn.BatchNorm3d(128)\n        self.rec_d2 = nn.ConvTranspose3d(128, 64, 3, (2,2,2), 1, (1,1,1));  self.rbn2 = nn.BatchNorm3d(64)\n        self.rec_d3 = nn.ConvTranspose3d(64, 48, 3, (2,2,2), 1, (1,1,1));   self.rbn3 = nn.BatchNorm3d(48)\n        self.rec_d4 = nn.ConvTranspose3d(48, 32, 3, (1,2,2), 1, (0,1,1));   self.rbn4 = nn.BatchNorm3d(32)\n        self.rec_out = nn.Conv3d(32, 3, 3, padding=1)\n        \n        # Branch B: Prediction (256 -> 3)\n        self.pre_d1 = nn.ConvTranspose3d(256, 128, 3, (2,2,2), 1, (1,1,1)); self.pbn1 = nn.BatchNorm3d(128)\n        self.pre_d2 = nn.ConvTranspose3d(128, 64, 3, (2,2,2), 1, (1,1,1));  self.pbn2 = nn.BatchNorm3d(64)\n        self.pre_d3 = nn.ConvTranspose3d(64, 48, 3, (2,2,2), 1, (1,1,1));   self.pbn3 = nn.BatchNorm3d(48)\n        self.pre_d4 = nn.ConvTranspose3d(48, 32, 3, (1,2,2), 1, (0,1,1));   self.pbn4 = nn.BatchNorm3d(32)\n        self.pre_out = nn.Conv3d(32, 3, 3, padding=1)\n        \n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Encoder\n        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(self.relu(self.bn2(self.conv2(x))))\n        x = self.pool3(self.relu(self.bn3(self.conv3(x))))\n        x = self.pool4(self.relu(self.bn4(self.conv4(x))))\n        l = self.relu(self.bn5(self.conv5(x)))\n        \n        # Recon\n        r = self.relu(self.rbn1(self.rec_d1(l)))\n        r = self.relu(self.rbn2(self.rec_d2(r)))\n        r = self.relu(self.rbn3(self.rec_d3(r)))\n        r = self.relu(self.rbn4(self.rec_d4(r)))\n        r = self.sigmoid(self.rec_out(r))\n        \n        # Pred\n        p = self.relu(self.pbn1(self.pre_d1(l)))\n        p = self.relu(self.pbn2(self.pre_d2(p)))\n        p = self.relu(self.pbn3(self.pre_d3(p)))\n        p = self.relu(self.pbn4(self.pre_d4(p)))\n        p = self.sigmoid(self.pre_out(p))\n        return r, p\n\n# --- 3. TRAINING LOOP ---\ndef train_ram_charged():\n    torch.cuda.empty_cache(); gc.collect()\n    print(f\"Training RAM-CHARGED Optimized STAE on {DEVICE}...\")\n    \n    # Init Dataset (Will cache everything now)\n    dataset = CachedVideoDataset(TRAIN_DIR, clip_len=CLIP_LEN, img_size=IMG_SIZE)\n    \n    # Persistent workers = False because data is already in RAM (Main Process)\n    # Actually, num_workers=0 is fastest for RAM-resident data to avoid pickling overhead\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    \n    model = STAutoEncoder_Optimized()\n    if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n    model = model.to(DEVICE)\n    \n    optimizer = optim.Adam(model.parameters(), lr=2e-4) # Slightly higher LR for speed\n    scaler = torch.cuda.amp.GradScaler()\n    criterion = nn.MSELoss()\n    \n    for epoch in range(EPOCHS):\n        model.train()\n        loop = tqdm(loader, desc=f\"Ep {epoch+1}/{EPOCHS}\")\n        \n        for inp, tgt in loop:\n            inp, tgt = inp.to(DEVICE), tgt.to(DEVICE)\n            optimizer.zero_grad(set_to_none=True) # Slightly faster than zero_grad()\n            \n            with torch.cuda.amp.autocast():\n                rec, pred = model(inp)\n                loss = criterion(rec, inp) + criterion(pred, tgt)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            loop.set_postfix(loss=loss.item())\n            \n    torch.save(model.module.state_dict() if hasattr(model, 'module') else model.state_dict(), MODEL_SAVE_PATH)\n    print(f\"DONE. Model saved to {MODEL_SAVE_PATH}\")\n\nif __name__ == \"__main__\":\n    train_ram_charged()","metadata":{"_uuid":"0dba3568-e16b-4778-b61e-93bcd9990d52","_cell_guid":"62d5365a-7624-457b-a344-858f391fcd2a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom tqdm import tqdm # Progress bar\n\n# ================= CONFIGURATION =================\n# Path to the CORRUPTED testing videos\nTEST_DATA_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\n\n# Path where we will save the CLEANED videos\nCLEAN_DATA_DIR = '/kaggle/working/cleaned_testing_videos'\n\nMODEL_PATH = '/kaggle/input/vlg-rot/pytorch/default/1/rotnet_model(1).pth'\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\ndef clean_dataset():\n    print(f\"Processing on: {DEVICE}\")\n    \n    # 1. Load the Trained RotNet\n    model = models.resnet18(pretrained=False) # No need to download weights again\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, 2) # Matches our binary training\n    \n    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n    model = model.to(DEVICE)\n    model.eval()\n    \n    # Standard transform for the model input\n    # Note: We do NOT augment here, just resize/norm\n    preprocess = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    # 2. Find all images\n    # We walk through the directory to keep structure\n    image_paths = sorted(glob.glob(os.path.join(TEST_DATA_DIR, '**', '*.jpg'), recursive=True))\n    print(f\"Found {len(image_paths)} frames to process.\")\n    \n    # 3. Processing Loop\n    flip_count = 0\n    \n    for img_path in tqdm(image_paths, desc=\"Cleaning\"):\n        # A. Setup paths\n        # Get relative path (e.g., \"01/frame_0001.jpg\") to maintain structure\n        rel_path = os.path.relpath(img_path, TEST_DATA_DIR)\n        save_path = os.path.join(CLEAN_DATA_DIR, rel_path)\n        \n        # Create folder if not exists\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        \n        # B. Predict Rotation\n        image = Image.open(img_path).convert('RGB')\n        input_tensor = preprocess(image).unsqueeze(0).to(DEVICE)\n        \n        with torch.no_grad():\n            outputs = model(input_tensor)\n            _, predicted = torch.max(outputs, 1)\n            label = predicted.item()\n            \n        # C. Fix and Save\n        # Label 0 = Upright (Keep as is)\n        # Label 1 = Flipped (Needs 180 rotation to fix)\n        \n        if label == 1:\n            # It was detected as Upside Down, so we rotate it -180 (or 180) to fix\n            fixed_image = image.transpose(Image.FLIP_TOP_BOTTOM) \n            flip_count += 1\n        else:\n            fixed_image = image\n            \n        # Save the fixed image\n        fixed_image.save(save_path)\n\n    print(\"-\" * 30)\n    print(\"Cleaning Complete!\")\n    print(f\"Total Images: {len(image_paths)}\")\n    print(f\"Images Flipped/Fixed: {flip_count}\")\n    print(f\"Cleaned dataset saved to: {CLEAN_DATA_DIR}\")\n\nif __name__ == \"__main__\":\n    clean_dataset()","metadata":{"_uuid":"201571d2-8f6d-4e70-84bb-aad8ec703ee9","_cell_guid":"3891990f-0986-4fcf-9330-04f47ab270b3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-12-30T17:55:19.539510Z","iopub.execute_input":"2025-12-30T17:55:19.540121Z","iopub.status.idle":"2025-12-30T17:58:44.045245Z","shell.execute_reply.started":"2025-12-30T17:55:19.540090Z","shell.execute_reply":"2025-12-30T17:58:44.044509Z"}},"outputs":[{"name":"stdout","text":"Processing on: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Found 11706 frames to process.\n","output_type":"stream"},{"name":"stderr","text":"Cleaning: 100%|██████████| 11706/11706 [02:57<00:00, 66.09it/s]","output_type":"stream"},{"name":"stdout","text":"------------------------------\nCleaning Complete!\nTotal Images: 11706\nImages Flipped/Fixed: 1195\nCleaned dataset saved to: /kaggle/working/cleaned_testing_videos\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport cv2\nimport os\nimport glob\nimport numpy as np\nfrom tqdm import tqdm\nimport warnings\nimport logging\nimport sys\n\n# ================= WARNING SUPPRESSION =================\n# 1. Python Warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 2. YOLO/Torch Hub Logging (The noisy part)\nlogging.getLogger(\"utils.general\").setLevel(logging.ERROR)\nlogging.getLogger(\"models.yolo\").setLevel(logging.ERROR)\nlogging.getLogger(\"torch.hub\").setLevel(logging.ERROR)\n\n# 3. Environment Flags\nos.environ[\"YOLO_VERBOSE\"] = \"False\"\n# =======================================================\n\n# ================= CONFIGURATION =================\n# Input: Your Cleaned Training Videos\nINPUT_DIR = '/kaggle/working/cleaned_testing_videos'\n\n# Output: The Masked Frames for AutoEncoder Training\nOUTPUT_DIR = '/kaggle/working/testing_videos_masked_context'\n\n# COCO Classes to KEEP (Context)\n# 0: Person\n# 1-8: Vehicles (Bicycle, Car, Motorcycle, Airplane, Bus, Train, Truck, Boat)\n# 24-28: Accessories (Backpack, Umbrella, Handbag, Tie, Suitcase)\n# 32: Sports Ball\nKEEP_CLASSES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 24, 25, 26, 27, 28, 32]\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\ndef apply_silent_context_masking():\n    print(f\"Loading YOLOv5s (Silent Mode) on {DEVICE}...\")\n    \n    # Redirect stdout to suppress the \"Fusing layers...\" print from YOLO\n    sys.stdout = open(os.devnull, 'w')\n    try:\n        try:\n            model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, verbose=False)\n        except:\n            model = torch.hub.load('/root/.cache/torch/hub/ultralytics_yolov5_master', 'custom', path='yolov5s.pt', source='local', verbose=False)\n    finally:\n        # Restore stdout\n        sys.stdout = sys.__stdout__\n        \n    model.to(DEVICE).eval()\n    model.conf = 0.25\n\n    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)\n\n    videos = sorted(os.listdir(INPUT_DIR))\n    print(f\"Processing {len(videos)} training videos...\")\n\n    for vid in tqdm(videos):\n        vid_path = os.path.join(INPUT_DIR, vid)\n        if not os.path.isdir(vid_path): continue\n        \n        save_path = os.path.join(OUTPUT_DIR, vid)\n        os.makedirs(save_path, exist_ok=True)\n        \n        frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n        \n        for f_path in frames:\n            img = cv2.imread(f_path)\n            if img is None: continue\n            \n            # Inference\n            results = model(img[..., ::-1], size=640)\n            preds = results.xyxy[0].cpu().numpy()\n            \n            # Create Black Mask\n            mask = np.zeros_like(img, dtype=np.uint8)\n            \n            h, w, _ = img.shape\n            padding = 15 \n            \n            for *xyxy, conf, cls in preds:\n                if int(cls) in KEEP_CLASSES:\n                    x1, y1, x2, y2 = map(int, xyxy)\n                    \n                    x1 = max(0, x1 - padding)\n                    y1 = max(0, y1 - padding)\n                    x2 = min(w, x2 + padding)\n                    y2 = min(h, y2 + padding)\n                    \n                    mask[y1:y2, x1:x2] = img[y1:y2, x1:x2]\n            \n            cv2.imwrite(os.path.join(save_path, os.path.basename(f_path)), mask)\n\n    print(f\"Done! Masked training data saved to: {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    apply_silent_context_masking()","metadata":{"_uuid":"10cedd80-52c5-4476-b815-5675eae64a49","_cell_guid":"02fabb69-726d-4cc8-b35c-00ccfe4d5a31","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torchvision import transforms\nfrom tqdm import tqdm\nfrom scipy.ndimage import gaussian_filter1d\n\n# ================= CONFIGURATION =================\nTEST_DIR = '/kaggle/working/testing_videos_masked_context'\nMODEL_PATH = 'st_autoencoder_rgb_dual.pth'\nTARGET_VIDEO_ID = '20'  # Change this to test different videos ('01', '02', etc.)\n\nCLIP_LEN = 16\nIMG_SIZE = 128\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\n# --- 1. MODEL ARCHITECTURE (Must match training) ---\nclass STAutoEncoder_RGB(nn.Module):\n    def __init__(self): \n        super(STAutoEncoder_RGB, self).__init__()\n        self.conv1 = nn.Conv3d(3, 32, 3, padding=1); self.bn1 = nn.BatchNorm3d(32); self.pool1 = nn.MaxPool3d(2, 2)\n        self.conv2 = nn.Conv3d(32, 48, 3, padding=1); self.bn2 = nn.BatchNorm3d(48); self.pool2 = nn.MaxPool3d(2, 2)\n        self.conv3 = nn.Conv3d(48, 64, 3, padding=1); self.bn3 = nn.BatchNorm3d(64); self.pool3 = nn.MaxPool3d(2, 2)\n        self.conv4 = nn.Conv3d(64, 64, 3, padding=1); self.bn4 = nn.BatchNorm3d(64)\n        self.relu = nn.LeakyReLU(0.1)\n        \n        self.rec_deconv1 = nn.ConvTranspose3d(64, 48, 3, 2, 1, 1); self.rec_bn1 = nn.BatchNorm3d(48)\n        self.rec_deconv2 = nn.ConvTranspose3d(48, 32, 3, 2, 1, 1); self.rec_bn2 = nn.BatchNorm3d(32)\n        self.rec_deconv3 = nn.ConvTranspose3d(32, 32, 3, 2, 1, 1); self.rec_bn3 = nn.BatchNorm3d(32)\n        self.rec_final = nn.Conv3d(32, 3, 3, padding=1)\n        \n        self.pred_deconv1 = nn.ConvTranspose3d(64, 48, 3, 2, 1, 1); self.pred_bn1 = nn.BatchNorm3d(48)\n        self.pred_deconv2 = nn.ConvTranspose3d(48, 32, 3, 2, 1, 1); self.pred_bn2 = nn.BatchNorm3d(32)\n        self.pred_deconv3 = nn.ConvTranspose3d(32, 32, 3, 2, 1, 1); self.pred_bn3 = nn.BatchNorm3d(32)\n        self.pred_final = nn.Conv3d(32, 3, 3, padding=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        h = self.relu(self.bn4(self.conv4(self.pool3(self.relu(self.bn3(self.conv3(self.pool2(self.relu(self.bn2(self.conv2(self.pool1(self.relu(self.bn1(self.conv1(x)))))))))))))))\n        r = self.sigmoid(self.rec_final(self.relu(self.rec_bn3(self.rec_deconv3(self.relu(self.rec_bn2(self.rec_deconv2(self.relu(self.rec_bn1(self.rec_deconv1(h)))))))))))\n        p = self.sigmoid(self.pred_final(self.relu(self.pred_bn3(self.pred_deconv3(self.relu(self.pred_bn2(self.pred_deconv2(self.relu(self.pred_bn1(self.pred_deconv1(h)))))))))))\n        return r, p\n\n# --- 2. VISUALIZATION LOGIC ---\ndef visualize_dual_branch():\n    print(f\"Visualizing Dual-Branch Model on Video {TARGET_VIDEO_ID}...\")\n    \n    # Locate Video\n    vid_path = os.path.join(TEST_DIR, TARGET_VIDEO_ID)\n    if not os.path.exists(vid_path):\n        candidates = [d for d in os.listdir(TEST_DIR) if str(int(d)) == str(int(TARGET_VIDEO_ID))]\n        if not candidates: print(\"Video not found.\"); return\n        vid_path = os.path.join(TEST_DIR, candidates[0])\n    \n    frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n    print(f\"Found {len(frames)} frames.\")\n    \n    # Load Model\n    model = STAutoEncoder_RGB()\n    if not os.path.exists(MODEL_PATH): print(\"Model file not found!\"); return\n    \n    # Handle DataParallel state dict if saved that way\n    state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n    model.load_state_dict(new_state_dict)\n    \n    model.to(DEVICE).eval()\n    \n    tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n    loss_fn = nn.MSELoss(reduction='none')\n    \n    rec_scores = []\n    pred_scores = []\n    frame_indices = []\n    \n    print(\"Running Inference...\")\n    with torch.no_grad():\n        # Stride 1 for smooth plotting\n        for i in tqdm(range(len(frames) - (2 * CLIP_LEN))):\n            # Input: Frames [i ... i+16]\n            inp_paths = frames[i : i + CLIP_LEN]\n            # Target: Frames [i+16 ... i+32] (For Prediction check)\n            tgt_paths = frames[i + CLIP_LEN : i + (2 * CLIP_LEN)]\n            \n            # Load\n            inp_vol = torch.stack([tf(Image.open(p).convert('RGB')) for p in inp_paths]).permute(1,0,2,3).unsqueeze(0).to(DEVICE)\n            tgt_vol = torch.stack([tf(Image.open(p).convert('RGB')) for p in tgt_paths]).permute(1,0,2,3).unsqueeze(0).to(DEVICE)\n            \n            # Forward\n            rec, pred = model(inp_vol)\n            \n            # Calculate Errors\n            # Reconstruction Error: Compare 'rec' with 'inp'\n            l_rec = loss_fn(rec, inp_vol).mean().item()\n            \n            # Prediction Error: Compare 'pred' with 'tgt'\n            l_pred = loss_fn(pred, tgt_vol).mean().item()\n            \n            rec_scores.append(l_rec)\n            pred_scores.append(l_pred)\n            frame_indices.append(i + CLIP_LEN) # Plot at the \"future\" point\n\n    # Normalize scores for plotting\n    def normalize(arr):\n        arr = np.array(arr)\n        return (arr - arr.min()) / (arr.max() - arr.min())\n\n    norm_rec = normalize(rec_scores)\n    norm_pred = normalize(pred_scores)\n    \n    # Combine (Total Anomaly Score)\n    # Prediction error usually weighs more for motion anomalies\n    total_score = (0.3 * norm_rec) + (0.7 * norm_pred)\n    smoothed_score = gaussian_filter1d(total_score, sigma=3)\n    \n    # Plot\n    plt.figure(figsize=(15, 8))\n    \n    plt.subplot(2, 1, 1)\n    plt.plot(frame_indices, norm_rec, label='Reconstruction Error (Static)', color='blue', alpha=0.6)\n    plt.plot(frame_indices, norm_pred, label='Prediction Error (Future)', color='red', alpha=0.6)\n    plt.title(f\"Dual-Branch Error Analysis: Video {TARGET_VIDEO_ID}\")\n    plt.ylabel(\"Normalized Error\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(2, 1, 2)\n    plt.plot(frame_indices, total_score, color='gray', alpha=0.3, label='Raw Combined Score')\n    plt.plot(frame_indices, smoothed_score, color='green', linewidth=2, label='Smoothed Anomaly Score')\n    plt.xlabel(\"Frame Number\")\n    plt.ylabel(\"Anomaly Score\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Anomaly Threshold Line (Arbitrary 0.5 for viz)\n    plt.axhline(y=0.5, color='black', linestyle='--')\n    \n    save_file = f\"dual_branch_plot_{TARGET_VIDEO_ID}.png\"\n    plt.savefig(save_file)\n    print(f\"Plot saved to {save_file}\")\n    plt.show()\n\nif __name__ == \"__main__\":\n    visualize_dual_branch()","metadata":{"_uuid":"98fecebe-52e6-43d5-9b56-f5ba3a4f7241","_cell_guid":"81b31d32-03d0-49ba-a57a-06397dfef3eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch.multiprocessing\n\n# ================= CONFIGURATION =================\nTEST_DIR = '/kaggle/working/testing_videos_masked_context'\nMODEL_PATH = 'st_autoencoder_rgb_dual.pth'\nOUTPUT_CSV = 'submission_dual_fast.csv'\n\nCLIP_LEN = 16    # Input 16 frames\nPRED_LEN = 16    # Predict 16 frames\nIMG_SIZE = 128\nBATCH_SIZE = 64  # Increased for Dual GPU Speed\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Weights\nW_REC = 0.3\nW_PRED = 0.7\n# =================================================\n\n# --- 1. MODEL ARCHITECTURE ---\nclass STAutoEncoder_RGB(nn.Module):\n    def __init__(self): \n        super(STAutoEncoder_RGB, self).__init__()\n        self.conv1 = nn.Conv3d(3, 32, 3, padding=1); self.bn1 = nn.BatchNorm3d(32); self.pool1 = nn.MaxPool3d(2, 2)\n        self.conv2 = nn.Conv3d(32, 48, 3, padding=1); self.bn2 = nn.BatchNorm3d(48); self.pool2 = nn.MaxPool3d(2, 2)\n        self.conv3 = nn.Conv3d(48, 64, 3, padding=1); self.bn3 = nn.BatchNorm3d(64); self.pool3 = nn.MaxPool3d(2, 2)\n        self.conv4 = nn.Conv3d(64, 64, 3, padding=1); self.bn4 = nn.BatchNorm3d(64)\n        self.relu = nn.LeakyReLU(0.1)\n        \n        self.rec_deconv1 = nn.ConvTranspose3d(64, 48, 3, 2, 1, 1); self.rec_bn1 = nn.BatchNorm3d(48)\n        self.rec_deconv2 = nn.ConvTranspose3d(48, 32, 3, 2, 1, 1); self.rec_bn2 = nn.BatchNorm3d(32)\n        self.rec_deconv3 = nn.ConvTranspose3d(32, 32, 3, 2, 1, 1); self.rec_bn3 = nn.BatchNorm3d(32)\n        self.rec_final = nn.Conv3d(32, 3, 3, padding=1)\n        \n        self.pred_deconv1 = nn.ConvTranspose3d(64, 48, 3, 2, 1, 1); self.pred_bn1 = nn.BatchNorm3d(48)\n        self.pred_deconv2 = nn.ConvTranspose3d(48, 32, 3, 2, 1, 1); self.pred_bn2 = nn.BatchNorm3d(32)\n        self.pred_deconv3 = nn.ConvTranspose3d(32, 32, 3, 2, 1, 1); self.pred_bn3 = nn.BatchNorm3d(32)\n        self.pred_final = nn.Conv3d(32, 3, 3, padding=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        h = self.relu(self.bn4(self.conv4(self.pool3(self.relu(self.bn3(self.conv3(self.pool2(self.relu(self.bn2(self.conv2(self.pool1(self.relu(self.bn1(self.conv1(x)))))))))))))))\n        r = self.sigmoid(self.rec_final(self.relu(self.rec_bn3(self.rec_deconv3(self.relu(self.rec_bn2(self.rec_deconv2(self.relu(self.rec_bn1(self.rec_deconv1(h)))))))))))\n        p = self.sigmoid(self.pred_final(self.relu(self.pred_bn3(self.pred_deconv3(self.relu(self.pred_bn2(self.pred_deconv2(self.relu(self.pred_bn1(self.pred_deconv1(h)))))))))))\n        return r, p\n\n# --- 2. FAST DATASET (Pre-indexing) ---\nclass FastTestDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.samples = []\n        self.transform = transform\n        \n        print(\"Indexing all test videos...\")\n        videos = sorted(os.listdir(root_dir))\n        \n        for vid in videos:\n            vid_path = os.path.join(root_dir, vid)\n            if not os.path.isdir(vid_path): continue\n            \n            frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n            if len(frames) < CLIP_LEN + PRED_LEN: continue\n            \n            # Create a sample for every valid sliding window\n            for i in range(len(frames) - CLIP_LEN - PRED_LEN):\n                # We store indices/paths only to save RAM\n                input_paths = frames[i : i + CLIP_LEN]\n                target_paths = frames[i + CLIP_LEN : i + CLIP_LEN + PRED_LEN]\n                \n                # ID corresponds to the start of the prediction\n                pred_frame = frames[i + CLIP_LEN]\n                fid = f\"{int(vid) if vid.isdigit() else vid}_{int(os.path.basename(pred_frame).split('_')[-1].split('.')[0])}\"\n                \n                self.samples.append((input_paths, target_paths, fid))\n        \n        print(f\"Indexed {len(self.samples)} samples for inference.\")\n\n    def __len__(self): return len(self.samples)\n\n    def __getitem__(self, idx):\n        in_paths, tgt_paths, fid = self.samples[idx]\n        \n        def load_vol(paths):\n            vol = [Image.open(p).convert('RGB') for p in paths]\n            if self.transform: vol = [self.transform(img) for img in vol]\n            return torch.stack(vol, dim=0).permute(1, 0, 2, 3)\n            \n        return load_vol(in_paths), load_vol(tgt_paths), fid\n\n# --- 3. GENERATION LOOP ---\ndef generate_fast():\n    print(f\">>> Initializing FAST Dual-GPU Inference (Batch={BATCH_SIZE})...\")\n    \n    # Load Model\n    model = STAutoEncoder_RGB()\n    if not os.path.exists(MODEL_PATH):\n        print(f\"Error: {MODEL_PATH} not found.\")\n        return\n        \n    state = torch.load(MODEL_PATH, map_location=DEVICE)\n    clean_state = {k.replace('module.', ''): v for k, v in state.items()}\n    model.load_state_dict(clean_state)\n    \n    # Enable Dual GPU\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n        model = nn.DataParallel(model)\n    model.to(DEVICE).eval()\n    \n    # Dataset & Loader\n    tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n    dataset = FastTestDataset(TEST_DIR, transform=tf)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n    \n    mse_loss = nn.MSELoss(reduction='none')\n    results = []\n    \n    print(\"Running Inference Loop...\")\n    with torch.no_grad():\n        for inp, tgt, fids in tqdm(loader):\n            inp, tgt = inp.to(DEVICE), tgt.to(DEVICE)\n            \n            rec, pred = model(inp)\n            \n            # Vectorized Loss Calculation (per batch item)\n            # MSE returns (Batch, C, T, H, W). Mean over dimensions (1,2,3,4) gives per-item loss\n            loss_rec = ((rec - inp) ** 2).mean(dim=(1,2,3,4))\n            loss_pred = ((pred - tgt) ** 2).mean(dim=(1,2,3,4))\n            \n            # Combine\n            scores = (W_REC * loss_rec * 100) + (W_PRED * loss_pred * 100)\n            \n            # Store\n            scores_np = scores.cpu().numpy()\n            for fid, score in zip(fids, scores_np):\n                results.append({'Id': fid, 'Predicted': float(score)})\n    \n    # --- POST PROCESSING ---\n    print(\"Normalizing...\")\n    df = pd.DataFrame(results)\n    \n    # 1. Fill Missing Frames\n    all_frames = []\n    for v in sorted(os.listdir(TEST_DIR)):\n        vp = os.path.join(TEST_DIR, v)\n        if not os.path.isdir(vp): continue\n        fs = sorted(glob.glob(os.path.join(vp, '*.jpg')))\n        for f in fs:\n            fid = f\"{int(v) if v.isdigit() else v}_{int(os.path.basename(f).split('_')[-1].split('.')[0])}\"\n            all_frames.append(fid)\n            \n    df_full = pd.DataFrame({'Id': all_frames})\n    df_final = pd.merge(df_full, df, on='Id', how='left').fillna(0.0)\n    \n    # 2. Global Normalization (No Smoothing)\n    mx = df_final['Predicted'].max()\n    mn = df_final['Predicted'].min()\n    if mx > mn:\n        df_final['Predicted'] = (df_final['Predicted'] - mn) / (mx - mn)\n        \n    df_final.to_csv(OUTPUT_CSV, index=False)\n    print(f\"Done! Saved to {OUTPUT_CSV}\")\n\nif __name__ == \"__main__\":\n    generate_fast()","metadata":{"_uuid":"60a4254d-379b-41ff-b46a-4a7a860eaecf","_cell_guid":"d4f82c73-af7e-4537-89a0-f5dd148fe7eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom scipy.ndimage import gaussian_filter1d\nfrom tqdm import tqdm\nimport gc\n\n# ================= CONFIGURATION =================\nTEST_DIR = '/kaggle/working/cleaned_testing_videos'\nMODEL_PATH = 'st_ae_fast_256.pth' \n\nTARGET_VIDEO_ID = '09'   \nIMG_SIZE = 256\nCLIP_LEN = 16\nBATCH_SIZE = 8           # Reduced to 8 to prevent OOM\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Submission Weights\nW_REC = 0.3\nW_PRED = 0.7\n# =================================================\n\n# --- 1. MODEL ARCHITECTURE (Must match training) ---\nclass STAutoEncoder_Optimized(nn.Module):\n    def __init__(self): \n        super(STAutoEncoder_Optimized, self).__init__()\n        # Encoder\n        self.conv1 = nn.Conv3d(3, 32, 3, padding=1);   self.bn1 = nn.BatchNorm3d(32);  self.pool1 = nn.MaxPool3d((1, 2, 2))\n        self.conv2 = nn.Conv3d(32, 48, 3, padding=1);  self.bn2 = nn.BatchNorm3d(48);  self.pool2 = nn.MaxPool3d((2, 2, 2))\n        self.conv3 = nn.Conv3d(48, 64, 3, padding=1);  self.bn3 = nn.BatchNorm3d(64);  self.pool3 = nn.MaxPool3d((2, 2, 2))\n        self.conv4 = nn.Conv3d(64, 128, 3, padding=1); self.bn4 = nn.BatchNorm3d(128); self.pool4 = nn.MaxPool3d((2, 2, 2))\n        self.conv5 = nn.Conv3d(128, 256, 3, padding=1);self.bn5 = nn.BatchNorm3d(256) \n        self.relu = nn.LeakyReLU(0.1, inplace=True)\n        \n        # Recon\n        self.rec_d1 = nn.ConvTranspose3d(256, 128, 3, (2,2,2), 1, (1,1,1)); self.rbn1 = nn.BatchNorm3d(128)\n        self.rec_d2 = nn.ConvTranspose3d(128, 64, 3, (2,2,2), 1, (1,1,1));  self.rbn2 = nn.BatchNorm3d(64)\n        self.rec_d3 = nn.ConvTranspose3d(64, 48, 3, (2,2,2), 1, (1,1,1));   self.rbn3 = nn.BatchNorm3d(48)\n        self.rec_d4 = nn.ConvTranspose3d(48, 32, 3, (1,2,2), 1, (0,1,1));   self.rbn4 = nn.BatchNorm3d(32)\n        self.rec_out = nn.Conv3d(32, 3, 3, padding=1)\n        \n        # Pred\n        self.pre_d1 = nn.ConvTranspose3d(256, 128, 3, (2,2,2), 1, (1,1,1)); self.pbn1 = nn.BatchNorm3d(128)\n        self.pre_d2 = nn.ConvTranspose3d(128, 64, 3, (2,2,2), 1, (1,1,1));  self.pbn2 = nn.BatchNorm3d(64)\n        self.pre_d3 = nn.ConvTranspose3d(64, 48, 3, (2,2,2), 1, (1,1,1));   self.pbn3 = nn.BatchNorm3d(48)\n        self.pre_d4 = nn.ConvTranspose3d(48, 32, 3, (1,2,2), 1, (0,1,1));   self.pbn4 = nn.BatchNorm3d(32)\n        self.pre_out = nn.Conv3d(32, 3, 3, padding=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(self.relu(self.bn2(self.conv2(x))))\n        x = self.pool3(self.relu(self.bn3(self.conv3(x))))\n        x = self.pool4(self.relu(self.bn4(self.conv4(x))))\n        l = self.relu(self.bn5(self.conv5(x)))\n        \n        r = self.sigmoid(self.rec_out(self.relu(self.rbn4(self.rec_d4(self.relu(self.rbn3(self.rec_d3(self.relu(self.rbn2(self.rec_d2(self.relu(self.rbn1(self.rec_d1(l))))))))))))))\n        p = self.sigmoid(self.pre_out(self.relu(self.pbn4(self.pre_d4(self.relu(self.pbn3(self.pre_d3(self.relu(self.pbn2(self.pre_d2(self.relu(self.pbn1(self.pre_d1(l))))))))))))))\n        return r, p\n\n# --- 2. STANDARD DATASET (On-Demand Loading) ---\nclass SafeVideoDataset(Dataset):\n    def __init__(self, vid_id, root_dir, clip_len=16, img_size=256):\n        self.clip_len = clip_len\n        \n        # Locate Video\n        vid_path = os.path.join(root_dir, vid_id)\n        if not os.path.exists(vid_path):\n            candidates = [d for d in os.listdir(root_dir) if str(int(d)) == str(int(vid_id))]\n            if candidates: vid_path = os.path.join(root_dir, candidates[0])\n            else: raise ValueError(\"Video not found\")\n            \n        self.frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n        print(f\"Found {len(self.frames)} frames in {vid_path}\")\n        \n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor()\n        ])\n        \n        # Valid start indices\n        self.valid_indices = []\n        if len(self.frames) >= 2 * clip_len:\n            self.valid_indices = list(range(len(self.frames) - (2 * clip_len)))\n\n    def __len__(self): return len(self.valid_indices)\n\n    def __getitem__(self, idx):\n        start = self.valid_indices[idx]\n        \n        # Load Input [t ... t+16]\n        in_vol = []\n        for i in range(start, start + self.clip_len):\n            img = Image.open(self.frames[i]).convert('RGB')\n            in_vol.append(self.transform(img))\n            \n        # Load Target [t+16 ... t+32]\n        tgt_vol = []\n        for i in range(start + self.clip_len, start + (2 * self.clip_len)):\n            img = Image.open(self.frames[i]).convert('RGB')\n            tgt_vol.append(self.transform(img))\n            \n        return (torch.stack(in_vol, dim=0).permute(1, 0, 2, 3), \n                torch.stack(tgt_vol, dim=0).permute(1, 0, 2, 3),\n                start + self.clip_len) # Frame Index for plotting\n\n# --- 3. VISUALIZATION LOGIC ---\ndef visualize_safe():\n    torch.cuda.empty_cache(); gc.collect()\n    print(f\"Running Safe Visualization for Video {TARGET_VIDEO_ID}...\")\n    \n    # 1. Load Model\n    model = STAutoEncoder_Optimized()\n    if not os.path.exists(MODEL_PATH): print(f\"Error: {MODEL_PATH} missing.\"); return\n    \n    # Clean state dict (remove 'module.' if present)\n    st = torch.load(MODEL_PATH, map_location=DEVICE)\n    if 'module.' in list(st.keys())[0]: st = {k.replace('module.', ''): v for k, v in st.items()}\n    model.load_state_dict(st)\n    \n    model.to(DEVICE).eval()\n    \n    # 2. Setup Data\n    ds = SafeVideoDataset(TARGET_VIDEO_ID, TEST_DIR, CLIP_LEN, IMG_SIZE)\n    # num_workers=2 is safe, 0 is safest\n    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    raw_scores = []\n    frame_indices = []\n    \n    print(\"Running Inference (Safe Mode)...\")\n    with torch.no_grad():\n        for inp, tgt, idxs in tqdm(loader):\n            inp, tgt = inp.to(DEVICE), tgt.to(DEVICE)\n            \n            rec, pred = model(inp)\n            \n            # Per-sample loss\n            loss_rec = ((rec - inp)**2).mean(dim=(1,2,3,4)).cpu().numpy()\n            loss_pred = ((pred - tgt)**2).mean(dim=(1,2,3,4)).cpu().numpy()\n            \n            # Combine\n            batch_scores = (W_REC * loss_rec) + (W_PRED * loss_pred)\n            \n            raw_scores.extend(batch_scores)\n            frame_indices.extend(idxs.numpy())\n\n    # --- 4. SUBMISSION LOGIC ---\n    print(\"Calculating Final Scores...\")\n    scores = np.array(raw_scores)\n    \n    # Normalize (Min-Max)\n    if scores.max() > scores.min():\n        scores = (scores - scores.min()) / (scores.max() - scores.min())\n        \n    \n    # --- 5. PLOT ---\n    plt.figure(figsize=(15, 6))\n    plt.plot(frame_indices, scores, color='gray', alpha=0.3, label='Raw Score')\n    plt.plot(frame_indices, scores, color='red', linewidth=2, label='Smoothed Score (Final)')\n    \n    plt.title(f\"Anomaly Detection: Video {TARGET_VIDEO_ID}\", fontsize=14)\n    plt.xlabel(\"Frame Number\")\n    plt.ylabel(\"Anomaly Probability\")\n    plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.5)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\nif __name__ == \"__main__\":\n    visualize_safe()","metadata":{"_uuid":"b7671180-f342-493d-97a1-4b4c469d8233","_cell_guid":"5fd62bf8-7f4d-4251-8a3c-9061d8b8a264","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom scipy.ndimage import gaussian_filter1d\nimport gc\n\n# ================= CONFIGURATION =================\nTEST_DIR = '/kaggle/working/cleaned_testing_videos'\nMODEL_PATH = 'st_ae_fast_256.pth'\nOUTPUT_CSV = 'submission.csv'\n\nIMG_SIZE = 256\nCLIP_LEN = 16\nBATCH_SIZE = 32          # Safe batch size for Dual GPU (16 per card)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Submission Weights\nW_REC = 0.3\nW_PRED = 0.7\n# =================================================\n\n# --- 1. MODEL ARCHITECTURE ---\nclass STAutoEncoder_Optimized(nn.Module):\n    def __init__(self): \n        super(STAutoEncoder_Optimized, self).__init__()\n        # Encoder\n        self.conv1 = nn.Conv3d(3, 32, 3, padding=1);   self.bn1 = nn.BatchNorm3d(32);  self.pool1 = nn.MaxPool3d((1, 2, 2))\n        self.conv2 = nn.Conv3d(32, 48, 3, padding=1);  self.bn2 = nn.BatchNorm3d(48);  self.pool2 = nn.MaxPool3d((2, 2, 2))\n        self.conv3 = nn.Conv3d(48, 64, 3, padding=1);  self.bn3 = nn.BatchNorm3d(64);  self.pool3 = nn.MaxPool3d((2, 2, 2))\n        self.conv4 = nn.Conv3d(64, 128, 3, padding=1); self.bn4 = nn.BatchNorm3d(128); self.pool4 = nn.MaxPool3d((2, 2, 2))\n        self.conv5 = nn.Conv3d(128, 256, 3, padding=1);self.bn5 = nn.BatchNorm3d(256) \n        self.relu = nn.LeakyReLU(0.1, inplace=True)\n        \n        # Recon\n        self.rec_d1 = nn.ConvTranspose3d(256, 128, 3, (2,2,2), 1, (1,1,1)); self.rbn1 = nn.BatchNorm3d(128)\n        self.rec_d2 = nn.ConvTranspose3d(128, 64, 3, (2,2,2), 1, (1,1,1));  self.rbn2 = nn.BatchNorm3d(64)\n        self.rec_d3 = nn.ConvTranspose3d(64, 48, 3, (2,2,2), 1, (1,1,1));   self.rbn3 = nn.BatchNorm3d(48)\n        self.rec_d4 = nn.ConvTranspose3d(48, 32, 3, (1,2,2), 1, (0,1,1));   self.rbn4 = nn.BatchNorm3d(32)\n        self.rec_out = nn.Conv3d(32, 3, 3, padding=1)\n        \n        # Pred\n        self.pre_d1 = nn.ConvTranspose3d(256, 128, 3, (2,2,2), 1, (1,1,1)); self.pbn1 = nn.BatchNorm3d(128)\n        self.pre_d2 = nn.ConvTranspose3d(128, 64, 3, (2,2,2), 1, (1,1,1));  self.pbn2 = nn.BatchNorm3d(64)\n        self.pre_d3 = nn.ConvTranspose3d(64, 48, 3, (2,2,2), 1, (1,1,1));   self.pbn3 = nn.BatchNorm3d(48)\n        self.pre_d4 = nn.ConvTranspose3d(48, 32, 3, (1,2,2), 1, (0,1,1));   self.pbn4 = nn.BatchNorm3d(32)\n        self.pre_out = nn.Conv3d(32, 3, 3, padding=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(self.relu(self.bn2(self.conv2(x))))\n        x = self.pool3(self.relu(self.bn3(self.conv3(x))))\n        x = self.pool4(self.relu(self.bn4(self.conv4(x))))\n        l = self.relu(self.bn5(self.conv5(x)))\n        \n        r = self.sigmoid(self.rec_out(self.relu(self.rbn4(self.rec_d4(self.relu(self.rbn3(self.rec_d3(self.relu(self.rbn2(self.rec_d2(self.relu(self.rbn1(self.rec_d1(l))))))))))))))\n        p = self.sigmoid(self.pre_out(self.relu(self.pbn4(self.pre_d4(self.relu(self.pbn3(self.pre_d3(self.relu(self.pbn2(self.pre_d2(self.relu(self.pbn1(self.pre_d1(l))))))))))))))\n        return r, p\n\n# --- 2. PRE-INDEXED DATASET (Efficient) ---\nclass InferenceDataset(Dataset):\n    def __init__(self, root_dir, clip_len=16, img_size=256):\n        self.clip_len = clip_len\n        self.samples = [] # Stores (path_list_input, path_list_target, frame_id)\n        \n        print(\"Indexing Test Videos...\")\n        videos = sorted(os.listdir(root_dir))\n        \n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor()\n        ])\n        \n        for vid in videos:\n            vid_path = os.path.join(root_dir, vid)\n            if not os.path.isdir(vid_path): continue\n            \n            frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n            if len(frames) < 2 * clip_len: continue\n            \n            # Create sliding window samples\n            for i in range(len(frames) - (2 * clip_len)):\n                in_paths = frames[i : i + clip_len]\n                tgt_paths = frames[i + clip_len : i + (2 * clip_len)]\n                \n                # ID for submission (frame at the end of the input clip)\n                # Usually we predict for frame t+16, so let's use that ID\n                target_frame = frames[i + clip_len]\n                fid = f\"{int(vid) if vid.isdigit() else vid}_{int(os.path.basename(target_frame).split('_')[-1].split('.')[0])}\"\n                \n                self.samples.append((in_paths, tgt_paths, fid))\n\n    def __len__(self): return len(self.samples)\n\n    def __getitem__(self, idx):\n        in_p, tgt_p, fid = self.samples[idx]\n        \n        vol_in = [self.transform(Image.open(p).convert('RGB')) for p in in_p]\n        vol_tgt = [self.transform(Image.open(p).convert('RGB')) for p in tgt_p]\n        \n        return (torch.stack(vol_in, dim=0).permute(1, 0, 2, 3), \n                torch.stack(vol_tgt, dim=0).permute(1, 0, 2, 3),\n                fid)\n\n# --- 3. GENERATION LOOP ---\ndef generate_submission():\n    torch.cuda.empty_cache(); gc.collect()\n    print(\">>> Initializing Dual-GPU Submission Generation...\")\n    \n    # Load Model\n    model = STAutoEncoder_Optimized()\n    if not os.path.exists(MODEL_PATH): print(\"Model not found!\"); return\n    \n    # Handle DataParallel loading\n    st = torch.load(MODEL_PATH, map_location=DEVICE)\n    if 'module.' in list(st.keys())[0]: st = {k.replace('module.', ''): v for k, v in st.items()}\n    model.load_state_dict(st)\n    \n    # Dual GPU Activation\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n        model = nn.DataParallel(model)\n    model.to(DEVICE).eval()\n    \n    # Dataset\n    dataset = InferenceDataset(TEST_DIR, CLIP_LEN, IMG_SIZE)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n    \n    results = []\n    \n    print(\"Running Inference...\")\n    with torch.no_grad():\n        for inp, tgt, fids in tqdm(loader):\n            inp, tgt = inp.to(DEVICE), tgt.to(DEVICE)\n            \n            rec, pred = model(inp)\n            \n            # Vectorized Loss Calculation\n            loss_rec = ((rec - inp)**2).mean(dim=(1,2,3,4))\n            loss_pred = ((pred - tgt)**2).mean(dim=(1,2,3,4))\n            \n            # Weighted Sum\n            scores = (W_REC * loss_rec) + (W_PRED * loss_pred)\n            \n            # Move to CPU\n            scores_np = scores.cpu().numpy()\n            \n            for fid, s in zip(fids, scores_np):\n                results.append({'Id': fid, 'Predicted': float(s)})\n\n    # --- POST PROCESSING ---\n    print(\"Processing Scores...\")\n    df = pd.DataFrame(results)\n    \n    # 1. Fill Missing Frames\n    all_frames = []\n    for v in sorted(os.listdir(TEST_DIR)):\n        vp = os.path.join(TEST_DIR, v)\n        if not os.path.isdir(vp): continue\n        fs = sorted(glob.glob(os.path.join(vp, '*.jpg')))\n        for f in fs:\n            fid = f\"{int(v) if v.isdigit() else v}_{int(os.path.basename(f).split('_')[-1].split('.')[0])}\"\n            all_frames.append(fid)\n            \n    df_full = pd.DataFrame({'Id': all_frames})\n    df_final = pd.merge(df_full, df, on='Id', how='left').fillna(0.0)\n    \n    # 2. Extract Video IDs for grouping\n    df_final['VideoID'] = df_final['Id'].apply(lambda x: x.split('_')[0])\n    \n    # 3. Per-Video Normalization & Smoothing\n    final_preds = []\n    \n    for vid, group in df_final.groupby('VideoID', sort=False):\n        raw = group['Predicted'].values\n        \n        # Min-Max Normalize this video\n        if raw.max() > raw.min():\n            raw = (raw - raw.min()) / (raw.max() - raw.min())\n            \n        # Gaussian Smooth\n        smooth = gaussian_filter1d(raw, sigma=2.0)\n        final_preds.extend(smooth)\n    \n    # 4. Final Global Normalization (Just in case smoothing pushed boundaries)\n    final_arr = np.array(final_preds)\n    if final_arr.max() > final_arr.min():\n        final_arr = (final_arr - final_arr.min()) / (final_arr.max() - final_arr.min())\n        \n    df_final['Predicted'] = final_arr\n    \n    # Save\n    sub_df = df_final[['Id', 'Predicted']]\n    sub_df.to_csv(OUTPUT_CSV, index=False)\n    print(f\"Success! Saved to {OUTPUT_CSV}\")\n    print(sub_df.head())\n\nif __name__ == \"__main__\":\n    generate_submission()","metadata":{"_uuid":"1873b8d6-3b6b-4260-b1ea-dd498184a3c2","_cell_guid":"dea3eabc-4780-4da5-8223-5e688c5dab3a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom scipy.ndimage import gaussian_filter1d\nimport gc\n\n# ================= CONFIGURATION =================\nTEST_DIR = '/kaggle/working/cleaned_testing_videos'\nMODEL_PATH = '/kaggle/input/vlg-256/pytorch/default/1/st_ae_fast_256.pth'\nOUTPUT_CSV = 'submission_corrected.csv'\n\nIMG_SIZE = 256\nCLIP_LEN = 16\nBATCH_SIZE = 32\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Submission Weights\nW_REC = 0.3\nW_PRED = 0.7\n# =================================================\n\n# --- 1. MODEL ARCHITECTURE ---\nclass STAutoEncoder_Optimized(nn.Module):\n    def __init__(self): \n        super(STAutoEncoder_Optimized, self).__init__()\n        # Encoder\n        self.conv1 = nn.Conv3d(3, 32, 3, padding=1);   self.bn1 = nn.BatchNorm3d(32);  self.pool1 = nn.MaxPool3d((1, 2, 2))\n        self.conv2 = nn.Conv3d(32, 48, 3, padding=1);  self.bn2 = nn.BatchNorm3d(48);  self.pool2 = nn.MaxPool3d((2, 2, 2))\n        self.conv3 = nn.Conv3d(48, 64, 3, padding=1);  self.bn3 = nn.BatchNorm3d(64);  self.pool3 = nn.MaxPool3d((2, 2, 2))\n        self.conv4 = nn.Conv3d(64, 128, 3, padding=1); self.bn4 = nn.BatchNorm3d(128); self.pool4 = nn.MaxPool3d((2, 2, 2))\n        self.conv5 = nn.Conv3d(128, 256, 3, padding=1);self.bn5 = nn.BatchNorm3d(256) \n        self.relu = nn.LeakyReLU(0.1, inplace=True)\n        # Recon\n        self.rec_d1 = nn.ConvTranspose3d(256, 128, 3, (2,2,2), 1, (1,1,1)); self.rbn1 = nn.BatchNorm3d(128)\n        self.rec_d2 = nn.ConvTranspose3d(128, 64, 3, (2,2,2), 1, (1,1,1));  self.rbn2 = nn.BatchNorm3d(64)\n        self.rec_d3 = nn.ConvTranspose3d(64, 48, 3, (2,2,2), 1, (1,1,1));   self.rbn3 = nn.BatchNorm3d(48)\n        self.rec_d4 = nn.ConvTranspose3d(48, 32, 3, (1,2,2), 1, (0,1,1));   self.rbn4 = nn.BatchNorm3d(32)\n        self.rec_out = nn.Conv3d(32, 3, 3, padding=1)\n        # Pred\n        self.pre_d1 = nn.ConvTranspose3d(256, 128, 3, (2,2,2), 1, (1,1,1)); self.pbn1 = nn.BatchNorm3d(128)\n        self.pre_d2 = nn.ConvTranspose3d(128, 64, 3, (2,2,2), 1, (1,1,1));  self.pbn2 = nn.BatchNorm3d(64)\n        self.pre_d3 = nn.ConvTranspose3d(64, 48, 3, (2,2,2), 1, (1,1,1));   self.pbn3 = nn.BatchNorm3d(48)\n        self.pre_d4 = nn.ConvTranspose3d(48, 32, 3, (1,2,2), 1, (0,1,1));   self.pbn4 = nn.BatchNorm3d(32)\n        self.pre_out = nn.Conv3d(32, 3, 3, padding=1)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        l = self.relu(self.bn5(self.conv5(self.pool4(self.relu(self.bn4(self.conv4(self.pool3(self.relu(self.bn3(self.conv3(self.pool2(self.relu(self.bn2(self.conv2(self.pool1(self.relu(self.bn1(self.conv1(x)))))))))))))))))))\n        r = self.sigmoid(self.rec_out(self.relu(self.rbn4(self.rec_d4(self.relu(self.rbn3(self.rec_d3(self.relu(self.rbn2(self.rec_d2(self.relu(self.rbn1(self.rec_d1(l))))))))))))))\n        p = self.sigmoid(self.pre_out(self.relu(self.pbn4(self.pre_d4(self.relu(self.pbn3(self.pre_d3(self.relu(self.pbn2(self.pre_d2(self.relu(self.pbn1(self.pre_d1(l))))))))))))))\n        return r, p\n\n# --- 2. DATASET ---\nclass InferenceDataset(Dataset):\n    def __init__(self, root_dir, clip_len=16, img_size=256):\n        self.clip_len = clip_len\n        self.samples = []\n        \n        print(\"Indexing Test Videos...\")\n        videos = sorted(os.listdir(root_dir))\n        self.transform = transforms.Compose([transforms.Resize((img_size, img_size)), transforms.ToTensor()])\n        \n        for vid in videos:\n            vid_path = os.path.join(root_dir, vid)\n            if not os.path.isdir(vid_path): continue\n            frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n            if len(frames) < 2 * clip_len: continue\n            \n            for i in range(len(frames) - (2 * clip_len)):\n                in_paths = frames[i : i + clip_len]\n                tgt_paths = frames[i + clip_len : i + (2 * clip_len)]\n                target_frame = frames[i + clip_len]\n                fid = f\"{int(vid) if vid.isdigit() else vid}_{int(os.path.basename(target_frame).split('_')[-1].split('.')[0])}\"\n                self.samples.append((in_paths, tgt_paths, fid))\n\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        in_p, tgt_p, fid = self.samples[idx]\n        vol_in = [self.transform(Image.open(p).convert('RGB')) for p in in_p]\n        vol_tgt = [self.transform(Image.open(p).convert('RGB')) for p in tgt_p]\n        return (torch.stack(vol_in, dim=0).permute(1, 0, 2, 3), \n                torch.stack(vol_tgt, dim=0).permute(1, 0, 2, 3),\n                fid)\n\n# --- 3. EXECUTION ---\ndef generate_submission():\n    torch.cuda.empty_cache(); gc.collect()\n    print(\">>> Initializing Corrected Submission Generator...\")\n    \n    # Load Model\n    model = STAutoEncoder_Optimized()\n    if not os.path.exists(MODEL_PATH): print(\"Model not found!\"); return\n    st = torch.load(MODEL_PATH, map_location=DEVICE)\n    if 'module.' in list(st.keys())[0]: st = {k.replace('module.', ''): v for k, v in st.items()}\n    model.load_state_dict(st)\n    if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n    model.to(DEVICE).eval()\n    \n    # Dataset\n    dataset = InferenceDataset(TEST_DIR, CLIP_LEN, IMG_SIZE)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n    \n    results = []\n    \n    print(\"Running Inference...\")\n    with torch.no_grad():\n        for inp, tgt, fids in tqdm(loader):\n            inp, tgt = inp.to(DEVICE), tgt.to(DEVICE)\n            rec, pred = model(inp)\n            loss_rec = ((rec - inp)**2).mean(dim=(1,2,3,4))\n            loss_pred = ((pred - tgt)**2).mean(dim=(1,2,3,4))\n            scores = (W_REC * loss_rec) + (W_PRED * loss_pred)\n            scores_np = scores.cpu().numpy()\n            \n            for fid, s in zip(fids, scores_np):\n                results.append({'Id': fid, 'Predicted': float(s)})\n\n    # --- 4. CORRECTED NORMALIZATION LOGIC ---\n    print(\"Processing Scores (Normalize BEFORE filling zeros)...\")\n    \n    # Convert actual predictions to DataFrame\n    df_pred = pd.DataFrame(results)\n    \n    # 1. NORMALIZE NOW (While we only have valid scores)\n    # This ensures min_score -> 0.0 and max_score -> 1.0\n    # Crucially, this ignores the missing frames (which will be 0 later)\n    mx = df_pred['Predicted'].max()\n    mn = df_pred['Predicted'].min()\n    print(f\"Raw Score Range: {mn:.6f} to {mx:.6f}\")\n    \n    if mx > mn:\n        df_pred['Predicted'] = (df_pred['Predicted'] - mn) / (mx - mn)\n    else:\n        df_pred['Predicted'] = 0.0\n        \n\n    # 3. FILL GAPS\n    # Now we bring in the missing frames and set them to 0.0\n    all_frames = []\n    for v in sorted(os.listdir(TEST_DIR)):\n        vp = os.path.join(TEST_DIR, v)\n        if not os.path.isdir(vp): continue\n        fs = sorted(glob.glob(os.path.join(vp, '*.jpg')))\n        for f in fs:\n            fid = f\"{int(v) if v.isdigit() else v}_{int(os.path.basename(f).split('_')[-1].split('.')[0])}\"\n            all_frames.append(fid)\n            \n    df_full = pd.DataFrame({'Id': all_frames})\n    \n    # Merge: existing scores stay, missing frames become NaN\n    df_final = pd.merge(df_full, df_pred[['Id', 'Predicted']], on='Id', how='left')\n    \n    # Fill NaN with 0.0 (True Normal)\n    df_final['Predicted'] = df_final['Predicted'].fillna(0.0)\n    \n    # 4. Global Re-Normalization (Safety Check)\n    # Just to ensure smoothing didn't push anything above 1.0 or below 0.0\n    final_vals = df_final['Predicted'].values\n    final_vals = np.clip(final_vals, 0.0, 1.0)\n    df_final['Predicted'] = final_vals\n    \n    # Save\n    sub_df = df_final[['Id', 'Predicted']]\n    sub_df.to_csv(OUTPUT_CSV, index=False)\n    print(f\"Success! Saved to {OUTPUT_CSV}\")\n\nif __name__ == \"__main__\":\n    generate_submission()","metadata":{"_uuid":"3058e25f-f57d-48de-bf6a-2d2abba46521","_cell_guid":"781fb58c-f63b-41be-9ac3-f5cfb291e8e9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ================= CONFIGURATION =================\nINPUT_CSV = '/kaggle/working/submission_videowise_raw.csv'  # Your current best file\nOUTPUT_CSV = 'submission_aggressive.csv'\n\n# AGGRESSIVENESS (Gain)\n# 10 = Strong, 20 = Very Strong (Almost Binary), 5 = Gentle\nGAIN = 10.0  \n\n# CENTER POINT\n# If your anomaly scores usually peak at 0.6, set this slightly lower (e.g., 0.4)\n# to make sure those 0.6s get pushed up to 1.0.\nCENTER = 0.35\n# =================================================\n\ndef sigmoid(x, gain=10, center=0.5):\n    # Standard Logistic Function with Gain and Center control\n    return 1 / (1 + np.exp(-gain * (x - center)))\n\ndef polarize_submission():\n    print(f\"Reading {INPUT_CSV}...\")\n    df = pd.read_csv(INPUT_CSV)\n    \n    raw = df['Predicted'].values\n    \n    # 1. Normalize to [0, 1] first (Critical for Sigmoid to work)\n    if raw.max() > raw.min():\n        raw = (raw - raw.min()) / (raw.max() - raw.min())\n    \n    # 2. Apply Aggressive Sigmoid\n    # This pushes values away from the center\n    aggressive = sigmoid(raw, gain=GAIN, center=CENTER)\n    \n    # 3. Re-Normalize (Just to ensure strict 0-1 range)\n    aggressive = (aggressive - aggressive.min()) / (aggressive.max() - aggressive.min())\n    \n    df['Predicted'] = aggressive\n    \n    # --- VISUALIZATION (To verify polarization) ---\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.hist(raw, bins=50, color='gray', alpha=0.7)\n    plt.title(\"Original Scores (Soft)\")\n    plt.xlabel(\"Score\")\n    \n    plt.subplot(1, 2, 2)\n    plt.hist(aggressive, bins=50, color='red', alpha=0.7)\n    plt.title(f\"Aggressive Scores (Gain={GAIN})\")\n    plt.xlabel(\"Score\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # --- SAVE ---\n    df.to_csv(OUTPUT_CSV, index=False)\n    print(f\"Saved aggressive scores to {OUTPUT_CSV}\")\n    print(df.describe())\n\nif __name__ == \"__main__\":\n    polarize_submission()","metadata":{"_uuid":"7b8a1749-6064-4830-93f6-cc5ac0abbb67","_cell_guid":"0df6249f-fc2b-4631-bd2f-ecb98efea5a3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ================= CONFIGURATION =================\nINPUT_CSV = '/kaggle/input/vlg-0-61/submission_smart_fill(1).csv'  \nOUTPUT_CSV = 'submission_videowise_raw.csv'\n# =================================================\n\ndef fix_videowise_raw():\n    print(f\"Reading {INPUT_CSV}...\")\n    df = pd.read_csv(INPUT_CSV)\n    \n    # Extract Video ID\n    df['VideoID'] = df['Id'].apply(lambda x: x.split('_')[0])\n    \n    final_dfs = []\n    \n    print(\"Applying Per-Video Normalization (NO SMOOTHING)...\")\n    \n    # Process each video independently\n    for vid, group in df.groupby('VideoID', sort=False):\n        group = group.copy()\n        raw_vals = group['Predicted'].values\n        \n        # 1. IDENTIFY VALID FRAMES (Ignore the 0.0 fillers)\n        # Real model output is never exactly 0.0, so 0.0 means \"missing frame\"\n        mask = raw_vals > 1e-9\n        \n        if mask.sum() > 0:\n            valid_scores = raw_vals[mask]\n            \n            # 2. CALCULATE MIN/MAX FOR THIS SPECIFIC VIDEO\n            mn, mx = valid_scores.min(), valid_scores.max()\n            \n            # 3. NORMALIZE (Local Range -> [0, 1])\n            if mx > mn:\n                normalized_valid = (valid_scores - mn) / (mx - mn)\n            else:\n                normalized_valid = np.zeros_like(valid_scores)\n            \n            # Apply back to the valid frames only\n            raw_vals[mask] = normalized_valid\n            \n            # 4. SAFETY: Force gaps to stay 0.0 (No bleeding)\n            raw_vals[~mask] = 0.0\n            \n            group['Predicted'] = np.clip(raw_vals, 0.0, 1.0)\n            \n        final_dfs.append(group)\n        \n    # Reassemble\n    df_final = pd.concat(final_dfs)\n    \n    # --- PLOT CHECK ---\n    plt.figure(figsize=(15, 6))\n    \n    # Plot Video 02 (Running)\n    v02 = df_final[df_final['VideoID'].astype(str).str.contains('02')].sort_values('Id')\n    if not v02.empty:\n        x02 = v02['Id'].apply(lambda x: int(x.split('_')[1]))\n        plt.plot(x02, v02['Predicted'], label='Video 02', color='blue', alpha=0.8)\n        \n    # Plot Video 05 (Bag Throw)\n    v05 = df_final[df_final['VideoID'].astype(str).str.contains('05')].sort_values('Id')\n    if not v05.empty:\n        x05 = v05['Id'].apply(lambda x: int(x.split('_')[1]))\n        plt.plot(x05, v05['Predicted'], label='Video 05', color='red', alpha=0.8)\n\n    plt.title(\"Video-Wise Normalization (Raw Peaks)\")\n    plt.xlabel(\"Frame\")\n    plt.ylabel(\"Anomaly Score\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n    # Save\n    out = df_final[['Id', 'Predicted']]\n    out.to_csv(OUTPUT_CSV, index=False)\n    print(f\"Saved RAW Video-Wise scores to {OUTPUT_CSV}\")\n\nif __name__ == \"__main__\":\n    fix_videowise_raw()","metadata":{"_uuid":"c223b730-a139-4bc9-86b1-95356587c6cd","_cell_guid":"6083a464-4227-4494-a653-2f2644890f31","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# ================= CONFIGURATION =================\nCSV_FILE = '/kaggle/working/submission_aggressive.csv'  # File to check\nTARGET_VIDEO = '20'                     # Video ID to plot\n# =================================================\n\ndef visualize_csv():\n    if not os.path.exists(CSV_FILE):\n        print(f\"Error: {CSV_FILE} not found.\")\n        return\n\n    print(f\"Reading {CSV_FILE}...\")\n    df = pd.read_csv(CSV_FILE)\n    \n    # Filter for Target Video\n    # Matches '02_xxx' or '2_xxx' style IDs\n    df['VideoID'] = df['Id'].apply(lambda x: x.split('_')[0])\n    vid_data = df[df['VideoID'].astype(int) == int(TARGET_VIDEO)].copy()\n    \n    if vid_data.empty:\n        print(f\"No data found for Video {TARGET_VIDEO}\")\n        return\n        \n    # Extract Frame Numbers\n    vid_data['Frame'] = vid_data['Id'].apply(lambda x: int(x.split('_')[1]))\n    vid_data = vid_data.sort_values('Frame')\n    \n    # Plot\n    plt.figure(figsize=(15, 6))\n    plt.plot(vid_data['Frame'], vid_data['Predicted'], color='red', linewidth=2, label='Anomaly Score')\n    \n    plt.title(f\"Aggressive Anomaly Score: Video {TARGET_VIDEO}\", fontsize=14)\n    plt.xlabel(\"Frame Number\")\n    plt.ylabel(\"Score (0 = Normal, 1 = Anomaly)\")\n    plt.ylim(-0.1, 1.1) # Keep Y-axis fixed to see 0-1 clearly\n    plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.3)\n    plt.grid(True, alpha=0.3)\n    plt.legend()\n    plt.show()\n\nif __name__ == \"__main__\":\n    visualize_csv()","metadata":{"_uuid":"52af8c23-c208-4cb4-8deb-ecaa5237b762","_cell_guid":"8bbeea39-9c4e-4531-a563-da45757ea54b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# ================= CONFIGURATION =================\nTRAIN_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos'\nSAVE_PATH = 'multiscale_unet_conditional.pth'\n\nIMG_SIZE = 256\nCLIP_LEN = 4     # 4 frames input\nBATCH_SIZE = 16  # 8 per GPU\nEPOCHS = 50\nLR_G = 2e-4\nLR_D = 2e-5\n\n# [cite_start]Loss Weights [cite: 2419, 2782]\nLAMBDA_INT = 2.0\nLAMBDA_GD = 1.0\nLAMBDA_ADV = 0.05\nLAMBDA_FLOW = 2.0 \n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\n# --- 1. ARCHITECTURE COMPONENTS (Generator) ---\n# [Unchanged from previous robust implementation]\n\nclass AsymmetricConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super(AsymmetricConv, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(kernel_size//2, 0))\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(1, kernel_size), padding=(0, kernel_size//2))\n        self.bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        return self.relu(self.bn(self.conv2(self.relu(self.conv1(x)))))\n\nclass ResidualSkipConnection(nn.Module):\n    def __init__(self, channels):\n        super(ResidualSkipConnection, self).__init__()\n        self.block = nn.Sequential(\n            AsymmetricConv(channels, channels),\n            AsymmetricConv(channels, channels)\n        )\n        self.shortcut = nn.Conv2d(channels, channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        return self.relu(self.block(x) + self.shortcut(x))\n\nclass ShortcutInceptionModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ShortcutInceptionModule, self).__init__()\n        w_6 = out_channels // 6\n        w_3 = out_channels // 3\n        w_2 = out_channels - (w_6 + w_3)\n\n        self.branch1 = AsymmetricConv(in_channels, w_6)\n        self.branch2 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3))\n        self.branch3 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3), AsymmetricConv(w_3, w_2))\n        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        concat = torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1)\n        return self.relu(concat + self.shortcut(x))\n\nclass MultiScaleUNet(nn.Module):\n    def __init__(self, in_channels=12, out_channels=3):\n        super(MultiScaleUNet, self).__init__()\n        # Encoder\n        self.sim1 = ShortcutInceptionModule(in_channels, 96); self.pool1 = nn.MaxPool2d(2)\n        self.sim2 = ShortcutInceptionModule(96, 192);         self.pool2 = nn.MaxPool2d(2)\n        self.sim3 = ShortcutInceptionModule(192, 384);        self.pool3 = nn.MaxPool2d(2)\n        self.sim4 = ShortcutInceptionModule(384, 768)\n\n        # Skip Connections\n        self.rsc1 = nn.Sequential(*[ResidualSkipConnection(96) for _ in range(4)])\n        self.rsc2 = nn.Sequential(*[ResidualSkipConnection(192) for _ in range(3)])\n        self.rsc3 = nn.Sequential(*[ResidualSkipConnection(384) for _ in range(2)])\n\n        # Decoder\n        self.sim5 = ShortcutInceptionModule(768, 384);   self.up1 = nn.ConvTranspose2d(384, 384, 2, 2)\n        self.sim6 = ShortcutInceptionModule(768, 192);   self.up2 = nn.ConvTranspose2d(192, 192, 2, 2)\n        self.sim7 = ShortcutInceptionModule(384, 96);    self.up3 = nn.ConvTranspose2d(96, 96, 2, 2)\n        self.sim8 = ShortcutInceptionModule(192, 96)\n        self.final = nn.Conv2d(96, out_channels, 3, padding=1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        e1 = self.sim1(x);        p1 = self.pool1(e1)\n        e2 = self.sim2(p1);       p2 = self.pool2(e2)\n        e3 = self.sim3(p2);       p3 = self.pool3(e3)\n        e4 = self.sim4(p3)\n\n        d1 = self.sim5(e4);       u1 = self.up1(d1)\n        cat1 = torch.cat([u1, self.rsc3(e3)], dim=1)\n\n        d2 = self.sim6(cat1);     u2 = self.up2(d2)\n        cat2 = torch.cat([u2, self.rsc2(e2)], dim=1)\n\n        d3 = self.sim7(cat2);     u3 = self.up3(d3)\n        cat3 = torch.cat([u3, self.rsc1(e1)], dim=1)\n\n        d4 = self.sim8(cat3)\n        return self.tanh(self.final(d4))\n\n# --- 2. CONDITIONAL PATCH DISCRIMINATOR (FIXED) ---\nclass ConditionalPatchDiscriminator(nn.Module):\n    def __init__(self, in_channels=6): # 3 (Current) + 3 (Past Condition)\n        super(ConditionalPatchDiscriminator, self).__init__()\n        \n        def disc_block(in_f, out_f, bn=True):\n            block = [nn.Conv2d(in_f, out_f, 4, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n            if bn: block.append(nn.BatchNorm2d(out_f))\n            return block\n\n        self.model = nn.Sequential(\n            *disc_block(in_channels, 64, bn=False), # 128x128\n            *disc_block(64, 128),                   # 64x64\n            *disc_block(128, 256),                  # 32x32\n            nn.Conv2d(256, 1, 4, padding=1)         # 32x32 (PatchGAN Map)\n        )\n\n    def forward(self, img_A, img_B):\n        # Concatenate condition (Last Frame) and target (Current Frame)\n        img_input = torch.cat((img_A, img_B), 1)\n        return self.model(img_input)\n\n# --- 3. LOSSES ---\ndef gradient_loss(gen_frames, gt_frames):\n    def gradient(x):\n        h_x = x.size()[-2]\n        w_x = x.size()[-1]\n        x_h = torch.abs(x[:, :, 1:, :] - x[:, :, :h_x-1, :])\n        x_w = torch.abs(x[:, :, :, 1:] - x[:, :, :, :w_x-1])\n        return x_h, x_w\n    gen_h, gen_w = gradient(gen_frames)\n    gt_h, gt_w = gradient(gt_frames)\n    return torch.mean(torch.abs(gen_h - gt_h)) + torch.mean(torch.abs(gen_w - gt_w))\n\ndef flow_loss(gen_frames, gt_frames, prev_frames):\n    flow_gen = torch.abs(gen_frames - prev_frames)\n    flow_gt = torch.abs(gt_frames - prev_frames)\n    return torch.mean(torch.abs(flow_gen - flow_gt)) # L1 Loss for robustness\n\n# --- 4. DATASET ---\nclass AvenueTrainDataset(Dataset):\n    def __init__(self, root_dir, clip_len=4, img_size=256):\n        self.clip_len = clip_len\n        self.samples = []\n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        \n        videos = sorted(os.listdir(root_dir))\n        for vid in videos:\n            path = os.path.join(root_dir, vid)\n            if not os.path.isdir(path): continue\n            frames = sorted(glob.glob(os.path.join(path, '*.jpg')))\n            if len(frames) < clip_len + 1: continue\n            \n            for i in range(len(frames) - clip_len):\n                self.samples.append(frames[i : i + clip_len + 1])\n\n    def __len__(self): return len(self.samples)\n\n    def __getitem__(self, idx):\n        paths = self.samples[idx]\n        imgs = [self.transform(Image.open(p).convert('RGB')) for p in paths]\n        \n        input_seq = torch.cat(imgs[:-1], dim=0) # 12 channels\n        target_frame = imgs[-1]                 # 3 channels (t+1)\n        last_input_frame = imgs[-2]             # 3 channels (t) - For Conditioning\n        \n        return input_seq, target_frame, last_input_frame\n\n# --- 5. TRAINING LOOP (CONDITIONAL GAN) ---\ndef train():\n    print(f\"Initializing Conditional Multi-scale U-Net Training on {DEVICE}...\")\n    \n    # Init Models\n    generator = MultiScaleUNet().to(DEVICE)\n    # Discriminator takes 6 channels: 3 (Condition/Last Frame) + 3 (Target/Generated)\n    discriminator = ConditionalPatchDiscriminator(in_channels=6).to(DEVICE)\n    \n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n        generator = nn.DataParallel(generator)\n        discriminator = nn.DataParallel(discriminator)\n        \n    opt_g = optim.Adam(generator.parameters(), lr=LR_G)\n    opt_d = optim.Adam(discriminator.parameters(), lr=LR_D)\n    \n    criterion_gan = nn.MSELoss() # LSGAN is more stable than BCE\n    criterion_pixel = nn.MSELoss()\n    \n    dataset = AvenueTrainDataset(TRAIN_DIR, CLIP_LEN, IMG_SIZE)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=12, pin_memory=True)\n    \n    try:\n        for epoch in range(EPOCHS):\n            generator.train(); discriminator.train()\n            pbar = tqdm(loader, desc=f\"Ep {epoch+1}/{EPOCHS}\")\n            \n            for inputs, targets, last_frames in pbar:\n                inputs = inputs.to(DEVICE)\n                targets = targets.to(DEVICE)\n                last_frames = last_frames.to(DEVICE) # Condition for D\n                \n                # ==========================\n                #  Train Discriminator (D)\n                # ==========================\n                opt_d.zero_grad()\n                \n                # Real: D(LastFrame, RealTarget) -> 1\n                real_out = discriminator(last_frames, targets)\n                loss_real = criterion_gan(real_out, torch.ones_like(real_out))\n                \n                # Fake: D(LastFrame, FakeTarget) -> 0\n                fake_frame = generator(inputs)\n                fake_out = discriminator(last_frames, fake_frame.detach()) # Detach G\n                loss_fake = criterion_gan(fake_out, torch.zeros_like(fake_out))\n                \n                loss_d = 0.5 * (loss_real + loss_fake)\n                loss_d.backward()\n                opt_d.step()\n                \n                # ==========================\n                #  Train Generator (G)\n                # ==========================\n                opt_g.zero_grad()\n                \n                # 1. Adversarial Loss: D(LastFrame, FakeTarget) -> 1\n                fake_out_g = discriminator(last_frames, fake_frame)\n                l_adv = criterion_gan(fake_out_g, torch.ones_like(fake_out_g))\n                \n                # 2. Pixel Intensity Loss\n                l_int = criterion_pixel(fake_frame, targets)\n                \n                # 3. Gradient Loss\n                l_gd = gradient_loss(fake_frame, targets)\n                \n                # 4. Flow Loss (Temporal Consistency)\n                l_flow = flow_loss(fake_frame, targets, last_frames)\n                \n                # Total Loss\n                loss_g = (LAMBDA_INT * l_int) + \\\n                         (LAMBDA_GD * l_gd) + \\\n                         (LAMBDA_ADV * l_adv) + \\\n                         (LAMBDA_FLOW * l_flow)\n                         \n                loss_g.backward()\n                opt_g.step()\n                \n                pbar.set_postfix({\n                    'D_loss': f\"{loss_d.item():.4f}\",\n                    'G_Adv': f\"{l_adv.item():.4f}\",\n                    'G_Int': f\"{l_int.item():.4f}\",\n                    'G_Flow': f\"{l_flow.item():.4f}\"\n                })\n            \n            torch.save(generator.module.state_dict(), f\"unet_conditional_ep{epoch}.pth\")\n            \n    except KeyboardInterrupt:\n        print(\"\\nTraining Interrupted! Saving checkpoint...\")\n        state = generator.module.state_dict() if hasattr(generator, 'module') else generator.state_dict()\n        torch.save(state, 'INTERRUPTED_conditional.pth')\n        print(\"Saved safely.\")\n\nif __name__ == \"__main__\":\n    train()","metadata":{"_uuid":"90f5140b-ad22-4f24-aaa5-826533c91b3a","_cell_guid":"a9f01a4a-156a-4848-ab95-4b47ed072f28","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport os\n\n# ================= CONFIGURATION =================\n# Check the latest saved epoch file in your directory\nLATEST_MODEL = '/kaggle/input/vlg-testmodel/pytorch/default/1/unet_conditional_ep1.pth' \nTEST_DIR = '/kaggle/working/cleaned_testing_videos'\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\ndef inspect():\n    # Load Model\n    model = MultiScaleUNet().to(DEVICE)\n    \n    if os.path.exists(LATEST_MODEL):\n        print(f\"Loading {LATEST_MODEL}...\")\n        st = torch.load(LATEST_MODEL, map_location=DEVICE)\n        # Fix DataParallel keys if needed\n        if 'module.' in list(st.keys())[0]: st = {k.replace('module.', ''): v for k, v in st.items()}\n        model.load_state_dict(st)\n    else:\n        print(\"Model file not found yet!\")\n        return\n\n    model.eval()\n    \n    # Get 1 Sample\n    ds = AvenueTrainDataset(TEST_DIR, clip_len=4)\n    inputs, target, _ = ds[1229] # Random index\n    \n    # Run Inference\n    with torch.no_grad():\n        inputs = inputs.unsqueeze(0).to(DEVICE) # Add batch dim\n        pred = model(inputs).squeeze(0).cpu()\n    \n    # De-normalize [-1, 1] -> [0, 1] for plotting\n    target = (target * 0.5) + 0.5\n    pred = (pred * 0.5) + 0.5\n    \n    # Plot\n    plt.figure(figsize=(10, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(target.permute(1, 2, 0).clip(0, 1))\n    plt.title(\"Ground Truth (Next Frame)\")\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(pred.permute(1, 2, 0).clip(0, 1))\n    plt.title(f\"Prediction (Epoch {LATEST_MODEL.split('_ep')[1].split('.')[0]})\")\n    plt.axis('off')\n    \n    plt.show()\n\nif __name__ == \"__main__\":\n    inspect()","metadata":{"_uuid":"567977f2-0f5f-46f9-a2fc-09285c927096","_cell_guid":"45680579-ac70-457e-a4e8-5da9a08a451d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"","metadata":{"_uuid":"c211a7b9-2ec3-49f8-9b15-0858cc492194","_cell_guid":"e4ba1989-dcf0-4260-8ac0-a03ce84a192b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# ================= CONFIGURATION =================\n# Update this to match your actual file name (e.g., ep1.pth is Epoch 2 if 0-indexed)\nMODEL_PATH = '/kaggle/input/vlg-testmodel/pytorch/default/1/unet_conditional_ep1.pth' \nTEST_DIR = '/kaggle/working/denoised_frames'\nTARGET_VIDEO = '09' \n\nIMG_SIZE = 256\nCLIP_LEN = 4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\n# --- 1. MODEL ARCHITECTURE (Must be defined to load weights) ---\nclass AsymmetricConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super(AsymmetricConv, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(kernel_size//2, 0))\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(1, kernel_size), padding=(0, kernel_size//2))\n        self.bn = nn.BatchNorm2d(out_channels)\n    def forward(self, x): return self.relu(self.bn(self.conv2(self.relu(self.conv1(x)))))\n\nclass ResidualSkipConnection(nn.Module):\n    def __init__(self, channels):\n        super(ResidualSkipConnection, self).__init__()\n        self.block = nn.Sequential(AsymmetricConv(channels, channels), AsymmetricConv(channels, channels))\n        self.shortcut = nn.Conv2d(channels, channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x): return self.relu(self.block(x) + self.shortcut(x))\n\nclass ShortcutInceptionModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ShortcutInceptionModule, self).__init__()\n        w_6 = out_channels // 6; w_3 = out_channels // 3; w_2 = out_channels - (w_6 + w_3)\n        self.branch1 = AsymmetricConv(in_channels, w_6)\n        self.branch2 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3))\n        self.branch3 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3), AsymmetricConv(w_3, w_2))\n        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.relu(torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1) + self.shortcut(x))\n\nclass MultiScaleUNet(nn.Module):\n    def __init__(self, in_channels=12, out_channels=3):\n        super(MultiScaleUNet, self).__init__()\n        self.sim1 = ShortcutInceptionModule(in_channels, 96); self.pool1 = nn.MaxPool2d(2)\n        self.sim2 = ShortcutInceptionModule(96, 192);         self.pool2 = nn.MaxPool2d(2)\n        self.sim3 = ShortcutInceptionModule(192, 384);        self.pool3 = nn.MaxPool2d(2)\n        self.sim4 = ShortcutInceptionModule(384, 768)\n        self.rsc1 = nn.Sequential(*[ResidualSkipConnection(96) for _ in range(4)])\n        self.rsc2 = nn.Sequential(*[ResidualSkipConnection(192) for _ in range(3)])\n        self.rsc3 = nn.Sequential(*[ResidualSkipConnection(384) for _ in range(2)])\n        self.sim5 = ShortcutInceptionModule(768, 384);   self.up1 = nn.ConvTranspose2d(384, 384, 2, 2)\n        self.sim6 = ShortcutInceptionModule(768, 192);   self.up2 = nn.ConvTranspose2d(192, 192, 2, 2)\n        self.sim7 = ShortcutInceptionModule(384, 96);    self.up3 = nn.ConvTranspose2d(96, 96, 2, 2)\n        self.sim8 = ShortcutInceptionModule(192, 96)\n        self.final = nn.Conv2d(96, out_channels, 3, padding=1)\n        self.tanh = nn.Tanh()\n    def forward(self, x):\n        e1 = self.sim1(x); p1 = self.pool1(e1)\n        e2 = self.sim2(p1); p2 = self.pool2(e2)\n        e3 = self.sim3(p2); p3 = self.pool3(e3)\n        e4 = self.sim4(p3)\n        d1 = self.sim5(e4); u1 = self.up1(d1)\n        d2 = self.sim6(torch.cat([u1, self.rsc3(e3)], dim=1)); u2 = self.up2(d2)\n        d3 = self.sim7(torch.cat([u2, self.rsc2(e2)], dim=1)); u3 = self.up3(d3)\n        d4 = self.sim8(torch.cat([u3, self.rsc1(e1)], dim=1))\n        return self.tanh(self.final(d4))\n\n# --- 2. SINGLE VIDEO DATASET ---\nclass SingleVideoDataset(Dataset):\n    def __init__(self, vid_id, root_dir, clip_len=4, img_size=256):\n        self.clip_len = clip_len\n        self.samples = []\n        \n        # Locate Video\n        vid_path = os.path.join(root_dir, vid_id)\n        if not os.path.exists(vid_path):\n            # Try finding folder with different zero-padding (e.g. '2' vs '02')\n            candidates = [d for d in os.listdir(root_dir) if str(int(d)) == str(int(vid_id))]\n            if candidates: vid_path = os.path.join(root_dir, candidates[0])\n            else: raise ValueError(f\"Video {vid_id} not found in {root_dir}\")\n            \n        print(f\"Loading frames from {vid_path}...\")\n        self.frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n        \n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        \n        # Create sliding windows\n        if len(self.frames) >= clip_len + 1:\n            for i in range(len(self.frames) - clip_len):\n                self.samples.append(i)\n\n    def __len__(self): return len(self.samples)\n\n    def __getitem__(self, idx):\n        # Input: [t, t+1, t+2, t+3]\n        in_paths = self.frames[idx : idx + self.clip_len]\n        # Target: [t+4]\n        tgt_path = self.frames[idx + self.clip_len]\n        \n        imgs = [self.transform(Image.open(p).convert('RGB')) for p in in_paths]\n        input_seq = torch.cat(imgs, dim=0) # (12, H, W)\n        target = self.transform(Image.open(tgt_path).convert('RGB'))\n        \n        return input_seq, target, idx + self.clip_len\n\n# --- 3. PLOT LOGIC ---\ndef visualize():\n    print(f\"Generating Anomaly Graph for Video {TARGET_VIDEO} using {MODEL_PATH}...\")\n    \n    # Load Model\n    model = MultiScaleUNet().to(DEVICE)\n    if not os.path.exists(MODEL_PATH):\n        print(f\"Error: {MODEL_PATH} not found. Please train first or check the path.\")\n        return\n        \n    st = torch.load(MODEL_PATH, map_location=DEVICE)\n    # Handle DataParallel dict keys if needed\n    if 'module.' in list(st.keys())[0]: st = {k.replace('module.', ''): v for k, v in st.items()}\n    model.load_state_dict(st)\n    model.eval()\n    \n    # Data\n    ds = SingleVideoDataset(TARGET_VIDEO, TEST_DIR, CLIP_LEN, IMG_SIZE)\n    loader = DataLoader(ds, batch_size=16, shuffle=False, num_workers=2)\n    \n    frame_indices = []\n    errors = []\n    \n    # Inference\n    with torch.no_grad():\n        for inputs, targets, idxs in tqdm(loader):\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            preds = model(inputs)\n            \n            # Calculate Reconstruction Error (MSE) per frame\n            # (Batch, C, H, W) -> Mean over (C, H, W) -> (Batch)\n            mse = ((preds - targets)**2).mean(dim=(1,2,3)).cpu().numpy()\n            \n            errors.extend(mse)\n            frame_indices.extend(idxs.numpy())\n            \n    # Process Scores\n    scores = np.array(errors)\n    \n    # Normalize [0, 1] for this video (Crucial for AP!)\n    mn, mx = scores.min(), scores.max()\n    if mx > mn:\n        scores_norm = (scores - mn) / (mx - mn)\n    else:\n        scores_norm = scores\n        \n    # Plot\n    plt.figure(figsize=(15, 6))\n    plt.plot(frame_indices, scores_norm, color='red', linewidth=2, label='Anomaly Score (Normalized MSE)')\n    \n    plt.title(f\"Anomaly Score Profile: Video {TARGET_VIDEO} (Epoch 2 Model)\", fontsize=16)\n    plt.xlabel(\"Frame Number\")\n    plt.ylabel(\"Anomaly Score (0=Normal, 1=Anomaly)\")\n    plt.axhline(y=0.5, color='black', linestyle='--', alpha=0.3)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\nif __name__ == \"__main__\":\n    visualize()","metadata":{"_uuid":"9023eb9c-02dd-4352-bb02-330cbfdb28d0","_cell_guid":"9e991589-adcc-4e73-a76c-f064553013bc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# ================= CONFIGURATION =================\nMODEL_PATH = '/kaggle/input/vlg-testmodel/pytorch/default/1/unet_conditional_ep1.pth' # Use your trained model\nTEST_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\nTARGET_VIDEO = '02'  # The video with the running man\nIMG_SIZE = 256\nCLIP_LEN = 4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# DENOISING PARAMS\nBLUR_KERNEL = (5, 5)\nNOISE_THRESH = 0.05\n# =================================================\n\n# --- ARCHITECTURE (Hidden for brevity, same as before) ---\nclass AsymmetricConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super(AsymmetricConv, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(kernel_size//2, 0))\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(1, kernel_size), padding=(0, kernel_size//2))\n        self.bn = nn.BatchNorm2d(out_channels)\n    def forward(self, x): return self.relu(self.bn(self.conv2(self.relu(self.conv1(x)))))\n\nclass ResidualSkipConnection(nn.Module):\n    def __init__(self, channels):\n        super(ResidualSkipConnection, self).__init__()\n        self.block = nn.Sequential(AsymmetricConv(channels, channels), AsymmetricConv(channels, channels))\n        self.shortcut = nn.Conv2d(channels, channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x): return self.relu(self.block(x) + self.shortcut(x))\n\nclass ShortcutInceptionModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ShortcutInceptionModule, self).__init__()\n        w_6 = out_channels // 6; w_3 = out_channels // 3; w_2 = out_channels - (w_6 + w_3)\n        self.branch1 = AsymmetricConv(in_channels, w_6)\n        self.branch2 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3))\n        self.branch3 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3), AsymmetricConv(w_3, w_2))\n        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.relu(torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1) + self.shortcut(x))\n\nclass MultiScaleUNet(nn.Module):\n    def __init__(self, in_channels=12, out_channels=3):\n        super(MultiScaleUNet, self).__init__()\n        self.sim1 = ShortcutInceptionModule(in_channels, 96); self.pool1 = nn.MaxPool2d(2)\n        self.sim2 = ShortcutInceptionModule(96, 192);         self.pool2 = nn.MaxPool2d(2)\n        self.sim3 = ShortcutInceptionModule(192, 384);        self.pool3 = nn.MaxPool2d(2)\n        self.sim4 = ShortcutInceptionModule(384, 768)\n        self.rsc1 = nn.Sequential(*[ResidualSkipConnection(96) for _ in range(4)])\n        self.rsc2 = nn.Sequential(*[ResidualSkipConnection(192) for _ in range(3)])\n        self.rsc3 = nn.Sequential(*[ResidualSkipConnection(384) for _ in range(2)])\n        self.sim5 = ShortcutInceptionModule(768, 384);   self.up1 = nn.ConvTranspose2d(384, 384, 2, 2)\n        self.sim6 = ShortcutInceptionModule(768, 192);   self.up2 = nn.ConvTranspose2d(192, 192, 2, 2)\n        self.sim7 = ShortcutInceptionModule(384, 96);    self.up3 = nn.ConvTranspose2d(96, 96, 2, 2)\n        self.sim8 = ShortcutInceptionModule(192, 96)\n        self.final = nn.Conv2d(96, out_channels, 3, padding=1)\n        self.tanh = nn.Tanh()\n    def forward(self, x):\n        e1 = self.sim1(x); p1 = self.pool1(e1)\n        e2 = self.sim2(p1); p2 = self.pool2(e2)\n        e3 = self.sim3(p2); p3 = self.pool3(e3)\n        e4 = self.sim4(p3)\n        d1 = self.sim5(e4); u1 = self.up1(d1)\n        d2 = self.sim6(torch.cat([u1, self.rsc3(e3)], dim=1)); u2 = self.up2(d2)\n        d3 = self.sim7(torch.cat([u2, self.rsc2(e2)], dim=1)); u3 = self.up3(d3)\n        d4 = self.sim8(torch.cat([u3, self.rsc1(e1)], dim=1))\n        return self.tanh(self.final(d4))\n\n# --- DATASET ---\nclass SingleVideoDataset(Dataset):\n    def __init__(self, vid_id, root_dir, clip_len=4, img_size=256):\n        self.clip_len = clip_len\n        self.samples = []\n        vid_path = os.path.join(root_dir, vid_id)\n        if not os.path.exists(vid_path):\n            candidates = [d for d in os.listdir(root_dir) if str(int(d)) == str(int(vid_id))]\n            vid_path = os.path.join(root_dir, candidates[0])\n        self.frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        if len(self.frames) >= clip_len + 1:\n            for i in range(len(self.frames) - clip_len):\n                self.samples.append(i)\n\n    def __len__(self): return len(self.samples)\n\n    def __getitem__(self, idx):\n        in_paths = self.frames[idx : idx + self.clip_len]\n        tgt_path = self.frames[idx + self.clip_len]\n        imgs = [self.transform(Image.open(p).convert('RGB')) for p in in_paths]\n        input_seq = torch.cat(imgs, dim=0)\n        target = self.transform(Image.open(tgt_path).convert('RGB'))\n        return input_seq, target, idx + self.clip_len\n\ndef show_denoising_effect():\n    print(f\"Analyzing Video {TARGET_VIDEO}...\")\n    \n    model = MultiScaleUNet().to(DEVICE)\n    if not os.path.exists(MODEL_PATH): print(\"Model not found!\"); return\n    st = torch.load(MODEL_PATH, map_location=DEVICE)\n    if 'module.' in list(st.keys())[0]: st = {k.replace('module.', ''): v for k, v in st.items()}\n    model.load_state_dict(st)\n    model.eval()\n    \n    ds = SingleVideoDataset(TARGET_VIDEO, TEST_DIR, CLIP_LEN, IMG_SIZE)\n    loader = DataLoader(ds, batch_size=16, shuffle=False)\n    \n    raw_scores = []\n    denoised_scores = []\n    frame_indices = []\n    \n    # We will capture one \"interesting\" frame to visualize\n    sample_vis = None\n    max_score = 0\n    \n    with torch.no_grad():\n        for inputs, targets, idxs in tqdm(loader):\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            preds = model(inputs)\n            \n            # 1. Calc Squared Error Map (Batch, C, H, W)\n            diff = (preds - targets) ** 2\n            # Mean over channels -> (Batch, H, W)\n            diff_map = diff.mean(dim=1).cpu().numpy()\n            \n            for i in range(len(diff_map)):\n                err_map = diff_map[i]\n                \n                # --- RAW SCORE ---\n                raw_s = np.mean(err_map)\n                raw_scores.append(raw_s)\n                \n                # --- DENOISING ---\n                # Blur\n                blur = cv2.GaussianBlur(err_map, BLUR_KERNEL, 0)\n                # Threshold\n                clean = blur.copy()\n                clean[clean < NOISE_THRESH] = 0.0\n                \n                denoised_s = np.mean(clean)\n                denoised_scores.append(denoised_s)\n                frame_indices.append(idxs[i].item())\n                \n                # Capture the frame with the highest denoised error (The Anomaly)\n                if denoised_s > max_score:\n                    max_score = denoised_s\n                    # Denormalize target for display\n                    tgt_disp = (targets[i].permute(1,2,0).cpu().numpy() * 0.5) + 0.5\n                    sample_vis = (tgt_disp, err_map, clean, idxs[i].item())\n\n    # --- PLOTTING ---\n    plt.figure(figsize=(15, 10))\n    \n    # 1. Timeline Comparison\n    plt.subplot(2, 1, 1)\n    # Normalize for fair visual comparison on graph\n    r_norm = (raw_scores - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores))\n    d_norm = (denoised_scores - np.min(denoised_scores)) / (np.max(denoised_scores) - np.min(denoised_scores))\n    \n    plt.plot(frame_indices, r_norm, color='gray', alpha=0.5, label='Raw Error (Noisy)')\n    plt.plot(frame_indices, d_norm, color='red', linewidth=2, label='Denoised Error (Clean)')\n    plt.title(f\"Video {TARGET_VIDEO}: Raw vs. Denoised Anomaly Score\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # 2. Frame Visualization\n    if sample_vis:\n        img, raw_map, clean_map, fid = sample_vis\n        \n        plt.subplot(2, 3, 4)\n        plt.imshow(img.clip(0, 1))\n        plt.title(f\"Frame {fid} (Real)\")\n        plt.axis('off')\n        \n        plt.subplot(2, 3, 5)\n        plt.imshow(raw_map, cmap='jet', vmin=0, vmax=0.1) # vmin/vmax to see noise\n        plt.title(\"Raw Error Map (Note the static)\")\n        plt.axis('off')\n        \n        plt.subplot(2, 3, 6)\n        plt.imshow(clean_map, cmap='jet', vmin=0, vmax=0.1)\n        plt.title(\"Denoised Map (Signal Isolated)\")\n        plt.axis('off')\n        \n    plt.tight_layout()\n    plt.show()\n\nif __name__ == \"__main__\":\n    show_denoising_effect()","metadata":{"_uuid":"78f2a0b0-2f2c-4050-b8ad-f82bc997fa3e","_cell_guid":"97f3b597-019a-4f03-aef3-5d0bb63f3075","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# ================= CONFIGURATION =================\nMODEL_PATH = '/kaggle/input/vlg-testmodel/pytorch/default/1/unet_conditional_ep1.pth' # Use your trained model\nTEST_DIR = '/kaggle/working/cleaned_testing_videos'\nTARGET_VIDEO = '09' \nIMG_SIZE = 256\nCLIP_LEN = 4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\n# --- 1. SSIM FUNCTION ---\ndef gaussian_window(size, sigma):\n    coords = torch.arange(size, dtype=torch.float)\n    coords -= size // 2\n    g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n    g /= g.sum()\n    return g.unsqueeze(1)\n\ndef ssim_map_calculation(img1, img2, window_size=11):\n    # Returns the full (H, W) SSIM map, not just the mean\n    channel = img1.size(1)\n    _1D_window = gaussian_window(window_size, 1.5).to(img1.device)\n    _1D_window = _1D_window.expand(channel, 1, window_size, 1).contiguous()\n    window = _1D_window.matmul(_1D_window.transpose(2, 3))\n\n    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n\n    mu1_sq = mu1.pow(2); mu2_sq = mu2.pow(2); mu1_mu2 = mu1 * mu2\n\n    sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1*img2, window, padding=window_size//2, groups=channel) - mu1_mu2\n\n    C1 = 0.01**2; C2 = 0.03**2\n\n    # Map of similarity (1=Same, 0=Different)\n    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2)) / ((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n    \n    # We return 1 - SSIM because we want High Value = Anomaly\n    return 1.0 - ssim_map.mean(dim=1, keepdim=True) \n\n# --- 2. ARCHITECTURE (Must match training) ---\nclass AsymmetricConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super(AsymmetricConv, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(kernel_size//2, 0))\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(1, kernel_size), padding=(0, kernel_size//2))\n        self.bn = nn.BatchNorm2d(out_channels)\n    def forward(self, x): return self.relu(self.bn(self.conv2(self.relu(self.conv1(x)))))\n\nclass ResidualSkipConnection(nn.Module):\n    def __init__(self, channels):\n        super(ResidualSkipConnection, self).__init__()\n        self.block = nn.Sequential(AsymmetricConv(channels, channels), AsymmetricConv(channels, channels))\n        self.shortcut = nn.Conv2d(channels, channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x): return self.relu(self.block(x) + self.shortcut(x))\n\nclass ShortcutInceptionModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ShortcutInceptionModule, self).__init__()\n        w_6 = out_channels // 6; w_3 = out_channels // 3; w_2 = out_channels - (w_6 + w_3)\n        self.branch1 = AsymmetricConv(in_channels, w_6)\n        self.branch2 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3))\n        self.branch3 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3), AsymmetricConv(w_3, w_2))\n        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.relu(torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1) + self.shortcut(x))\n\nclass MultiScaleUNet(nn.Module):\n    def __init__(self, in_channels=12, out_channels=3):\n        super(MultiScaleUNet, self).__init__()\n        self.sim1 = ShortcutInceptionModule(in_channels, 96); self.pool1 = nn.MaxPool2d(2)\n        self.sim2 = ShortcutInceptionModule(96, 192);         self.pool2 = nn.MaxPool2d(2)\n        self.sim3 = ShortcutInceptionModule(192, 384);        self.pool3 = nn.MaxPool2d(2)\n        self.sim4 = ShortcutInceptionModule(384, 768)\n        self.rsc1 = nn.Sequential(*[ResidualSkipConnection(96) for _ in range(4)])\n        self.rsc2 = nn.Sequential(*[ResidualSkipConnection(192) for _ in range(3)])\n        self.rsc3 = nn.Sequential(*[ResidualSkipConnection(384) for _ in range(2)])\n        self.sim5 = ShortcutInceptionModule(768, 384);   self.up1 = nn.ConvTranspose2d(384, 384, 2, 2)\n        self.sim6 = ShortcutInceptionModule(768, 192);   self.up2 = nn.ConvTranspose2d(192, 192, 2, 2)\n        self.sim7 = ShortcutInceptionModule(384, 96);    self.up3 = nn.ConvTranspose2d(96, 96, 2, 2)\n        self.sim8 = ShortcutInceptionModule(192, 96)\n        self.final = nn.Conv2d(96, out_channels, 3, padding=1)\n        self.tanh = nn.Tanh()\n    def forward(self, x):\n        e1 = self.sim1(x); p1 = self.pool1(e1)\n        e2 = self.sim2(p1); p2 = self.pool2(e2)\n        e3 = self.sim3(p2); p3 = self.pool3(e3)\n        e4 = self.sim4(p3)\n        d1 = self.sim5(e4); u1 = self.up1(d1)\n        d2 = self.sim6(torch.cat([u1, self.rsc3(e3)], dim=1)); u2 = self.up2(d2)\n        d3 = self.sim7(torch.cat([u2, self.rsc2(e2)], dim=1)); u3 = self.up3(d3)\n        d4 = self.sim8(torch.cat([u3, self.rsc1(e1)], dim=1))\n        return self.tanh(self.final(d4))\n\n# --- 3. DATASET ---\nclass SingleVideoDataset(Dataset):\n    def __init__(self, vid_id, root_dir, clip_len=4, img_size=256):\n        self.clip_len = clip_len\n        vid_path = os.path.join(root_dir, vid_id)\n        if not os.path.exists(vid_path):\n            candidates = [d for d in os.listdir(root_dir) if str(int(d)) == str(int(vid_id))]\n            vid_path = os.path.join(root_dir, candidates[0])\n        self.frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        self.samples = []\n        if len(self.frames) >= clip_len + 1:\n            for i in range(len(self.frames) - clip_len):\n                self.samples.append(i)\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        in_paths = self.frames[idx : idx + self.clip_len]\n        tgt_path = self.frames[idx + self.clip_len]\n        imgs = [self.transform(Image.open(p).convert('RGB')) for p in in_paths]\n        return torch.cat(imgs, dim=0), self.transform(Image.open(tgt_path).convert('RGB')), idx + self.clip_len\n\n# --- 4. VISUALIZE ---\ndef visualize_ssim():\n    print(f\"Analyzing SSIM vs MSE on Video {TARGET_VIDEO}...\")\n    \n    model = MultiScaleUNet().to(DEVICE)\n    if not os.path.exists(MODEL_PATH): print(\"Model not found!\"); return\n    st = torch.load(MODEL_PATH, map_location=DEVICE)\n    if 'module.' in list(st.keys())[0]: st = {k.replace('module.', ''): v for k, v in st.items()}\n    model.load_state_dict(st)\n    model.eval()\n    \n    ds = SingleVideoDataset(TARGET_VIDEO, TEST_DIR, CLIP_LEN, IMG_SIZE)\n    loader = DataLoader(ds, batch_size=16, shuffle=False)\n    \n    mse_scores = []\n    ssim_scores = []\n    frame_indices = []\n    \n    # Capture Sample for Display (Max Anomaly)\n    vis_sample = None\n    max_ssim = 0\n    \n    with torch.no_grad():\n        for inputs, targets, idxs in tqdm(loader):\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            preds = model(inputs)\n            \n            # --- MSE Calculation ---\n            diff_mse = (preds - targets) ** 2\n            mse_map = diff_mse.mean(dim=1) # (B, H, W)\n            mse_vals = mse_map.mean(dim=(1,2)).cpu().numpy()\n            mse_scores.extend(mse_vals)\n            \n            # --- SSIM Calculation ---\n            # Shift to [0, 1]\n            p_01 = (preds * 0.5) + 0.5\n            t_01 = (targets * 0.5) + 0.5\n            \n            # Get Anomaly Map (1 - SSIM)\n            # Returns (B, 1, H, W)\n            anomaly_map_ssim = ssim_map_calculation(p_01, t_01)\n            ssim_vals = anomaly_map_ssim.mean(dim=(1,2,3)).cpu().numpy()\n            ssim_scores.extend(ssim_vals)\n            \n            frame_indices.extend(idxs.numpy())\n            \n            # Capture interesting frame\n            batch_max = ssim_vals.max()\n            if batch_max > max_ssim:\n                max_ssim = batch_max\n                idx_max = ssim_vals.argmax()\n                \n                # Store numpy arrays for plotting\n                img_real = t_01[idx_max].permute(1,2,0).cpu().numpy()\n                map_mse = mse_map[idx_max].cpu().numpy()\n                map_ssim = anomaly_map_ssim[idx_max].squeeze(0).cpu().numpy()\n                fid = idxs[idx_max].item()\n                vis_sample = (img_real, map_mse, map_ssim, fid)\n\n    # --- PLOTTING ---\n    plt.figure(figsize=(15, 12))\n    \n    # 1. GRAPH COMPARISON\n    plt.subplot(3, 1, 1)\n    # Normalize to compare shapes\n    mse_norm = (mse_scores - np.min(mse_scores)) / (np.max(mse_scores) - np.min(mse_scores))\n    ssim_norm = (ssim_scores - np.min(ssim_scores)) / (np.max(ssim_scores) - np.min(ssim_scores))\n    \n    plt.plot(frame_indices, mse_norm, color='gray', alpha=0.5, label='MSE (Pixel Error)')\n    plt.plot(frame_indices, ssim_norm, color='blue', linewidth=2, label='SSIM (Structural Error)')\n    plt.title(f\"Graph Comparison: Does SSIM reduce the noise floor?\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # 2. IMAGE COMPARISON\n    if vis_sample:\n        img, m_mse, m_ssim, fid = vis_sample\n        \n        plt.subplot(3, 3, 4)\n        plt.imshow(img.clip(0, 1))\n        plt.title(f\"Real Frame {fid}\")\n        plt.axis('off')\n        \n        plt.subplot(3, 3, 5)\n        plt.imshow(m_mse, cmap='jet')\n        plt.title(\"MSE Map (Pixel Difference)\\nNote: Background might be noisy\")\n        plt.axis('off')\n        \n        plt.subplot(3, 3, 6)\n        plt.imshow(m_ssim, cmap='jet')\n        plt.title(\"SSIM Map (Structural Difference)\\nIdeally: Dark Background, Bright Object\")\n        plt.axis('off')\n        \n    plt.tight_layout()\n    plt.show()\n\nif __name__ == \"__main__\":\n    visualize_ssim()","metadata":{"_uuid":"e409ba2a-8e76-4caa-b383-728cd731bcb0","_cell_guid":"a5fea46a-2467-4084-8a77-be9a52e70445","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom PIL import Image\n\n# ================= CONFIGURATION =================\nTEST_DIR = '/kaggle/working/cleaned_testing_videos'\nDENOISED_DIR = 'denoised_frames'\n\n# We train on the test video itself (Self-Supervision)\n# 1000 steps is enough to learn the \"noise pattern\" to remove\nTRAIN_STEPS = 1000 \nBATCH_SIZE = 8\nIMG_SIZE = 256\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\n# --- 1. LIGHTWEIGHT DENOISER (DnCNN Style) ---\nclass SimpleDenoiser(nn.Module):\n    def __init__(self):\n        super(SimpleDenoiser, self).__init__()\n        # Input: 3 Channels (RGB)\n        # Output: 3 Channels (Clean RGB)\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(True),\n            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(True),\n            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(True),\n            nn.Conv2d(64, 3, 3, padding=1)\n        )\n    \n    def forward(self, x):\n        # Residual Learning: Predict the NOISE, subtract from Input\n        noise_pred = self.encoder(x)\n        return x - noise_pred\n\n# --- 2. DATASET (Yields Pairs: Frame T and Frame T+1) ---\nclass VideoPairDataset(Dataset):\n    def __init__(self, root_dir, img_size=256):\n        self.pairs = []\n        self.img_size = img_size\n        \n        videos = sorted(os.listdir(root_dir))\n        for vid in videos:\n            vid_path = os.path.join(root_dir, vid)\n            if not os.path.isdir(vid_path): continue\n            frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n            \n            # Create T, T+1 pairs\n            for i in range(len(frames) - 1):\n                self.pairs.append((frames[i], frames[i+1]))\n                \n    def __len__(self): return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        p1, p2 = self.pairs[idx]\n        \n        # Load and Resize\n        img1 = cv2.imread(p1); img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n        img2 = cv2.imread(p2); img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n        \n        img1 = cv2.resize(img1, (self.img_size, self.img_size))\n        img2 = cv2.resize(img2, (self.img_size, self.img_size))\n        \n        # Normalize [0, 1] -> Tensor (C, H, W)\n        t1 = torch.from_numpy(img1 / 255.0).float().permute(2, 0, 1)\n        t2 = torch.from_numpy(img2 / 255.0).float().permute(2, 0, 1)\n        \n        return t1, t2, p1 # Return p1 path to know ID later\n\n# --- 3. OPTICAL FLOW ALIGNMENT (The \"Temporal\" Part) ---\ndef align_frames(curr, next_frame):\n    # curr, next_frame: (B, C, H, W) Torch Tensors\n    # We use CV2 Optical Flow on CPU (simpler than FlowNet)\n    \n    aligned_batch = []\n    \n    curr_np = curr.permute(0, 2, 3, 1).cpu().numpy() # (B, H, W, C)\n    next_np = next_frame.permute(0, 2, 3, 1).cpu().numpy()\n    \n    for i in range(len(curr_np)):\n        prev_gray = cv2.cvtColor((curr_np[i]*255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n        next_gray = cv2.cvtColor((next_np[i]*255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n        \n        # Calculate Flow (Next -> Prev)\n        flow = cv2.calcOpticalFlowFarneback(next_gray, prev_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n        \n        # Warp Next to match Prev\n        h, w = flow.shape[:2]\n        flow_map = np.column_stack((np.repeat(np.arange(h), w), np.tile(np.arange(w), h)))\n        vec = flow.reshape(-1, 2)\n        indices = flow_map + vec\n        \n        map_x = indices[:, 1].reshape(h, w).astype(np.float32)\n        map_y = indices[:, 0].reshape(h, w).astype(np.float32)\n        \n        warped = cv2.remap((next_np[i]*255).astype(np.uint8), map_x, map_y, cv2.INTER_LINEAR)\n        aligned_batch.append(warped / 255.0)\n        \n    # Stack back to Tensor\n    aligned = torch.tensor(np.array(aligned_batch)).float().permute(0, 3, 1, 2).to(curr.device)\n    return aligned\n\n# --- 4. MAIN PIPELINE ---\ndef train_and_denoise():\n    print(\"Initializing Unsupervised Denoiser (Noise2Noise Strategy)...\")\n    \n    os.makedirs(DENOISED_DIR, exist_ok=True)\n    \n    model = SimpleDenoiser().to(DEVICE)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.MSELoss()\n    \n    ds = VideoPairDataset(TEST_DIR, IMG_SIZE)\n    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n    \n    # --- PHASE 1: SELF-SUPERVISED TRAINING ---\n    print(f\"Training on Test Videos for {TRAIN_STEPS} steps...\")\n    model.train()\n    \n    step = 0\n    # Infinite loop wrapper\n    while step < TRAIN_STEPS:\n        for img_curr, img_next, _ in loader:\n            if step >= TRAIN_STEPS: break\n            \n            img_curr = img_curr.to(DEVICE)\n            img_next = img_next.to(DEVICE)\n            \n            # 1. Align Next Frame to Current Frame\n            # (If we don't align, the model learns motion blur)\n            with torch.no_grad():\n                img_next_aligned = align_frames(img_curr, img_next)\n            \n            # 2. Denoise Current Frame\n            # Input: Noisy Current\n            # Target: Aligned Noisy Next (Noise is independent!)\n            cleaned_curr = model(img_curr)\n            \n            # 3. Loss\n            loss = criterion(cleaned_curr, img_next_aligned)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            if step % 100 == 0:\n                print(f\"Step {step}/{TRAIN_STEPS} | Loss: {loss.item():.6f}\")\n            step += 1\n\n    # --- PHASE 2: INFERENCE (SAVE CLEAN FRAMES) ---\n    print(\"Denoising all frames...\")\n    model.eval()\n    \n    # Reload dataset sequentially\n    full_loader = DataLoader(ds, batch_size=1, shuffle=False)\n    \n    with torch.no_grad():\n        for img_curr, _, path in tqdm(full_loader):\n            img_curr = img_curr.to(DEVICE)\n            cleaned = model(img_curr)\n            \n            # Save\n            # Structure: denoised_frames/02/frame_001.jpg\n            orig_path = path[0]\n            vid_id = os.path.basename(os.path.dirname(orig_path))\n            f_name = os.path.basename(orig_path)\n            \n            save_folder = os.path.join(DENOISED_DIR, vid_id)\n            os.makedirs(save_folder, exist_ok=True)\n            \n            # Convert back to Image\n            c_np = cleaned.squeeze(0).permute(1, 2, 0).cpu().numpy()\n            c_np = np.clip(c_np * 255, 0, 255).astype(np.uint8)\n            \n            out_p = os.path.join(save_folder, f_name)\n            Image.fromarray(c_np).save(out_p)\n            \n    print(f\"Done! Clean frames saved to {DENOISED_DIR}\")\n    print(\"Now run your Anomaly Detector on THIS new directory.\")\n\nif __name__ == \"__main__\":\n    train_and_denoise()","metadata":{"_uuid":"5629ddf1-82db-44ca-a56e-7a71cff9a8c3","_cell_guid":"a8c2a4be-f882-4abd-b589-f4e37cc0ebd2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport os\nimport torch\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\n# ================= CONFIGURATION =================\n# 1. WHERE YOUR PYTHON FILES ARE (The dataset you just made)\n# Example: '/kaggle/input/my-tap-code'\nCODE_DATASET_DIR = '/kaggle/input/tap-denoise2' \n\n# 2. PATH TO WEIGHTS (nafnet_srgb.pth)\nMODEL_WEIGHTS = '/kaggle/input/nafnetrgb/pytorch/default/1/nafnet_rgb.pth'\n\n# 3. VIDEO SETTINGS\nINPUT_VIDEO = '/kaggle/working/rendered_videos/video_02.mp4' # Check this path\nOUTPUT_VIDEO = 'cleaned_output.mp4'\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\n# --- 1. SETUP IMPORTS ---\n# Add the dataset directory to Python's search path\nif CODE_DATASET_DIR not in sys.path:\n    sys.path.append(CODE_DATASET_DIR)\n\nprint(f\"Added {CODE_DATASET_DIR} to system path.\")\n\n# Try importing TAP. \n# Since you brought files \"out\" of the folder, they might be at the root.\ntry:\n    # Try direct import first (if files are flat)\n    from network_tap import TAP\n    print(\"✅ Imported TAP from network_tap.py\")\nexcept ImportError:\n    try:\n        # Try package import (if you kept the folder structure 'models/network_tap')\n        from models.network_tap import TAP\n        print(\"✅ Imported TAP from models.network_tap\")\n    except ImportError as e:\n        print(\"❌ CRITICAL ERROR: Could not import TAP.\")\n        print(f\"Python sees these files in {CODE_DATASET_DIR}:\")\n        print(os.listdir(CODE_DATASET_DIR))\n        print(f\"\\nError details: {e}\")\n        # Stop execution if we can't load the model code\n        sys.exit()\n\n# --- 2. INFERENCE LOOP ---\ndef clean_video():\n    print(f\"Loading weights from {MODEL_WEIGHTS}...\")\n    \n    # Initialize TAP Model\n    # sRGB config: in_nc=3, nc=64, frames=5\n    model = TAP(in_nc=3, nc=64, frames=5, pixel_loss_type='L1')\n    \n    # Load Weights\n    if not os.path.exists(MODEL_WEIGHTS):\n        print(\"❌ Error: Weights file not found.\")\n        return\n\n    checkpoint = torch.load(MODEL_WEIGHTS, map_location=DEVICE)\n    if 'params' in checkpoint: \n        checkpoint = checkpoint['params']\n    \n    model.load_state_dict(checkpoint, strict=False)\n    model.to(DEVICE)\n    model.eval()\n\n    # Open Video\n    cap = cv2.VideoCapture(INPUT_VIDEO)\n    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps    = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    if width == 0:\n        print(\"❌ Error: Could not read video. Check INPUT_VIDEO path.\")\n        return\n\n    print(f\"Processing {INPUT_VIDEO}\")\n    print(f\"Dimensions: {width}x{height}, Frames: {total_frames}\")\n\n    # Video Writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (width, height))\n    \n    # Buffer for Sliding Window [t-2, t-1, t, t+1, t+2]\n    # We read all frames into RAM for fastest sliding window access \n    # (Avenue videos are small enough for this)\n    all_frames = []\n    \n    print(\"Reading video into memory...\")\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n        # BGR -> RGB -> Normalize 0-1\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame = frame.astype(np.float32) / 255.0\n        all_frames.append(frame)\n    cap.release()\n    \n    # Convert to Tensor (1, T, C, H, W)\n    video_tensor = torch.tensor(np.array(all_frames)).permute(0, 3, 1, 2).unsqueeze(0)\n    T = video_tensor.shape[1]\n    \n    print(\"Running Denoising...\")\n    \n    with torch.no_grad():\n        for i in tqdm(range(T)):\n            # \n            # Construct indices for window of 5 around 'i'\n            indices = []\n            for offset in range(-2, 3):\n                # Clamp index to [0, T-1] to handle edges\n                idx = np.clip(i + offset, 0, T - 1)\n                indices.append(idx)\n            \n            # Extract Window (1, 5, C, H, W)\n            input_window = video_tensor[:, indices, :, :, :].to(DEVICE)\n            \n            # Inference (Returns Center Frame)\n            clean_tensor = model(input_window) \n            \n            # Save Output\n            clean_frame = clean_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n            clean_frame = np.clip(clean_frame * 255, 0, 255).astype(np.uint8)\n            clean_frame = cv2.cvtColor(clean_frame, cv2.COLOR_RGB2BGR)\n            \n            out.write(clean_frame)\n            \n    out.release()\n    print(f\"✅ Success! Clean video saved to: {OUTPUT_VIDEO}\")\n\nif __name__ == \"__main__\":\n    clean_video()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\n# ================= CONFIGURATION =================\nINPUT_VIDEO = '/kaggle/working/rendered_videos/video_02.mp4'\nOUTPUT_VIDEO = 'video_02_cleaned_NLM.mp4'\n\n# STRENGTH PARAMETERS\n# h: Denoising strength. Higher = smoother but blurrier. \n# 3.0 is conservative, 6.0 is strong. Start with 4.0.\nH_LUMA = 4.0 \nTEMPORAL_WINDOW = 5  # How many frames to look at (odd number)\nSEARCH_WINDOW = 21   # How far to look for matching blocks (pixels)\nBLOCK_SIZE = 7       # Size of blocks to compare\n# =================================================\n\ndef clean_video_nlm():\n    if not os.path.exists(INPUT_VIDEO):\n        print(f\"Error: Video not found at {INPUT_VIDEO}\")\n        return\n\n    cap = cv2.VideoCapture(INPUT_VIDEO)\n    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps    = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    print(f\"Cleaning Video: {width}x{height} @ {fps} FPS ({total_frames} frames)\")\n    \n    # Setup Output\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (width, height))\n    \n    # Buffer: We need to hold 'TEMPORAL_WINDOW' frames in memory\n    # Sliding window logic: [t-2, t-1, t, t+1, t+2] to clean 't'\n    frames_buffer = []\n    \n    # Read ALL frames into RAM first (Avenue videos are small, ~20MB)\n    # This makes the multi-frame logic much simpler\n    all_frames_gray = []\n    all_frames_color = []\n    \n    print(\"Reading video to memory...\")\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n        all_frames_color.append(frame)\n        # NLM works best on Grayscale for the map, but we apply to color\n        # Actually OpenCV's function handles color, but let's stick to the color version\n    cap.release()\n    \n    print(f\"Applying Multi-Frame Non-Local Means (Strength {H_LUMA})...\")\n    \n    # Pad video with border frames to handle start/end\n    pad = TEMPORAL_WINDOW // 2\n    padded_frames = [all_frames_color[0]]*pad + all_frames_color + [all_frames_color[-1]]*pad\n    \n    for i in tqdm(range(len(all_frames_color))):\n        # Extract the window of frames\n        # Index in padded array is i + pad\n        # We want window centered there\n        center_idx = i + pad\n        start = center_idx - pad\n        end = center_idx + pad + 1\n        \n        current_window = padded_frames[start:end]\n        \n        # The function expects:\n        # imgs: list of frames\n        # imgToDenoiseIndex: index of the frame in the list to clean (middle one)\n        # temporalWindowSize: number of frames\n        \n        clean = cv2.fastNlMeansDenoisingMulti(\n            srcImgs=current_window,\n            imgToDenoiseIndex=pad, # The middle frame\n            temporalWindowSize=TEMPORAL_WINDOW,\n            h=H_LUMA,\n            templateWindowSize=BLOCK_SIZE,\n            searchWindowSize=SEARCH_WINDOW\n        )\n        \n        out.write(clean)\n        \n    out.release()\n    print(f\"✅ Cleaned video saved as: {OUTPUT_VIDEO}\")\n\nif __name__ == \"__main__\":\n    clean_video_nlm()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom torchvision import transforms\n\n# ================= CONFIGURATION =================\nTRAIN_VIDEO_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos'\nOUTPUT_DIR = 'cleaned_check'\nTARGET_VIDEO = '02'  # The specific video ID to test\nMODEL_PATH = '/kaggle/input/vlg-testmodel/pytorch/default/1/unet_conditional_ep1.pth' \n\nIMG_SIZE = 256\nCLIP_LEN = 4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\n# --- 1. MODEL ARCHITECTURE (Must match training) ---\nclass AsymmetricConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super(AsymmetricConv, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(kernel_size//2, 0))\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(1, kernel_size), padding=(0, kernel_size//2))\n        self.bn = nn.BatchNorm2d(out_channels)\n    def forward(self, x): return self.relu(self.bn(self.conv2(self.relu(self.conv1(x)))))\n\nclass ResidualSkipConnection(nn.Module):\n    def __init__(self, channels):\n        super(ResidualSkipConnection, self).__init__()\n        self.block = nn.Sequential(AsymmetricConv(channels, channels), AsymmetricConv(channels, channels))\n        self.shortcut = nn.Conv2d(channels, channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x): return self.relu(self.block(x) + self.shortcut(x))\n\nclass ShortcutInceptionModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ShortcutInceptionModule, self).__init__()\n        w_6 = out_channels // 6; w_3 = out_channels // 3; w_2 = out_channels - (w_6 + w_3)\n        self.branch1 = AsymmetricConv(in_channels, w_6)\n        self.branch2 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3))\n        self.branch3 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3), AsymmetricConv(w_3, w_2))\n        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.relu(torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1) + self.shortcut(x))\n\nclass MultiScaleUNet(nn.Module):\n    def __init__(self, in_channels=12, out_channels=3):\n        super(MultiScaleUNet, self).__init__()\n        self.sim1 = ShortcutInceptionModule(in_channels, 96); self.pool1 = nn.MaxPool2d(2)\n        self.sim2 = ShortcutInceptionModule(96, 192);         self.pool2 = nn.MaxPool2d(2)\n        self.sim3 = ShortcutInceptionModule(192, 384);        self.pool3 = nn.MaxPool2d(2)\n        self.sim4 = ShortcutInceptionModule(384, 768)\n        self.rsc1 = nn.Sequential(*[ResidualSkipConnection(96) for _ in range(4)])\n        self.rsc2 = nn.Sequential(*[ResidualSkipConnection(192) for _ in range(3)])\n        self.rsc3 = nn.Sequential(*[ResidualSkipConnection(384) for _ in range(2)])\n        self.sim5 = ShortcutInceptionModule(768, 384);   self.up1 = nn.ConvTranspose2d(384, 384, 2, 2)\n        self.sim6 = ShortcutInceptionModule(768, 192);   self.up2 = nn.ConvTranspose2d(192, 192, 2, 2)\n        self.sim7 = ShortcutInceptionModule(384, 96);    self.up3 = nn.ConvTranspose2d(96, 96, 2, 2)\n        self.sim8 = ShortcutInceptionModule(192, 96)\n        self.final = nn.Conv2d(96, out_channels, 3, padding=1)\n        self.tanh = nn.Tanh()\n    def forward(self, x):\n        e1 = self.sim1(x); p1 = self.pool1(e1)\n        e2 = self.sim2(p1); p2 = self.pool2(e2)\n        e3 = self.sim3(p2); p3 = self.pool3(e3)\n        e4 = self.sim4(p3)\n        d1 = self.sim5(e4); u1 = self.up1(d1)\n        d2 = self.sim6(torch.cat([u1, self.rsc3(e3)], dim=1)); u2 = self.up2(d2)\n        d3 = self.sim7(torch.cat([u2, self.rsc2(e2)], dim=1)); u3 = self.up3(d3)\n        d4 = self.sim8(torch.cat([u3, self.rsc1(e1)], dim=1))\n        return self.tanh(self.final(d4))\n\n# --- 2. SINGLE VIDEO LOGIC ---\ndef clean_single_video():\n    print(f\"Loading Model from {MODEL_PATH}...\")\n    model = MultiScaleUNet().to(DEVICE)\n    if not os.path.exists(MODEL_PATH):\n        print(\"❌ Model not found! Train it first.\")\n        return\n        \n    st = torch.load(MODEL_PATH, map_location=DEVICE)\n    if 'module.' in list(st.keys())[0]: st = {k.replace('module.', ''): v for k, v in st.items()}\n    model.load_state_dict(st)\n    model.eval()\n    \n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    # Locate target video\n    vid_path = os.path.join(TRAIN_VIDEO_DIR, TARGET_VIDEO)\n    if not os.path.exists(vid_path):\n        # Fallback for folder naming inconsistencies\n        candidates = [d for d in os.listdir(TRAIN_VIDEO_DIR) if str(int(d)) == str(int(TARGET_VIDEO))]\n        if candidates: vid_path = os.path.join(TRAIN_VIDEO_DIR, candidates[0])\n        else: print(f\"❌ Video {TARGET_VIDEO} not found.\"); return\n\n    frames = sorted(glob.glob(os.path.join(vid_path, '*.jpg')))\n    if len(frames) < CLIP_LEN + 1:\n        print(\"Video too short.\"); return\n        \n    print(f\"Cleaning Video {TARGET_VIDEO} ({len(frames)} frames)...\")\n\n    # Setup Video Writer\n    save_path = os.path.join(OUTPUT_DIR, f\"train_{TARGET_VIDEO}_cleaned.mp4\")\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(save_path, fourcc, 25.0, (IMG_SIZE, IMG_SIZE))\n    \n    # Transforms\n    tf = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    # Load all frames to RAM\n    loaded_frames = []\n    for f in frames:\n        img = Image.open(f).convert('RGB')\n        loaded_frames.append(tf(img))\n    \n    vid_tensor = torch.stack(loaded_frames)\n    \n    with torch.no_grad():\n        # Iterate through the sequence\n        for i in range(len(loaded_frames) - CLIP_LEN):\n            # Input: [i, i+1, i+2, i+3] -> Predicts i+4\n            input_seq = vid_tensor[i : i+CLIP_LEN]\n            input_seq = input_seq.view(-1, IMG_SIZE, IMG_SIZE).unsqueeze(0).to(DEVICE)\n            \n            pred_frame = model(input_seq)\n            \n            # Post-process\n            img_out = pred_frame.squeeze(0).permute(1, 2, 0).cpu().numpy()\n            img_out = (img_out * 0.5) + 0.5\n            img_out = np.clip(img_out * 255, 0, 255).astype(np.uint8)\n            img_out = cv2.cvtColor(img_out, cv2.COLOR_RGB2BGR)\n            \n            out.write(img_out)\n            \n    out.release()\n    print(f\"✅ Cleaned video saved to: {save_path}\")\n\nif __name__ == \"__main__\":\n    clean_single_video()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\n# ================= CONFIGURATION =================\n# We train on the TEST videos to adapt to their specific noise patterns\nTRAIN_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\nSAVE_NAME = 'resnet_denoiser.pth'\n\nIMG_SIZE = 256\nBATCH_SIZE = 16\nEPOCHS = 15  # Fast training\nLR = 1e-4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\n# --- 1. LIGHTWEIGHT RESNET AUTOENCODER ---\nclass ResBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1),\n            nn.BatchNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, channels, 3, padding=1),\n            nn.BatchNorm2d(channels)\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        return self.relu(x + self.conv(x)) # Residual Connection\n\nclass ResNetDenoiser(nn.Module):\n    def __init__(self):\n        super(ResNetDenoiser, self).__init__()\n        \n        # Encoder (Downsampling)\n        self.enc1 = nn.Sequential(nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)) # 128\n        self.enc2 = nn.Sequential(nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)) # 64\n        self.enc3 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)) # 32\n        \n        # Bottleneck (Clean Structure Learning)\n        # 3 ResBlocks to refine features without losing resolution\n        self.bottleneck = nn.Sequential(\n            ResBlock(128),\n            ResBlock(128),\n            ResBlock(128)\n        )\n        \n        # Decoder (Upsampling)\n        self.dec3 = nn.Sequential(nn.ConvTranspose2d(128, 64, 2, 2), nn.ReLU())\n        self.dec2 = nn.Sequential(nn.ConvTranspose2d(64, 32, 2, 2), nn.ReLU())\n        self.dec1 = nn.Sequential(nn.ConvTranspose2d(32, 16, 2, 2), nn.ReLU())\n        \n        self.final = nn.Conv2d(16, 3, 3, padding=1)\n        self.tanh = nn.Tanh() # Output -1 to 1\n\n    def forward(self, x):\n        # No Long Skip Connections! This kills the noise.\n        e1 = self.enc1(x)\n        e2 = self.enc2(e1)\n        e3 = self.enc3(e2)\n        \n        b = self.bottleneck(e3)\n        \n        d3 = self.dec3(b)\n        d2 = self.dec2(d3)\n        d1 = self.dec1(d2)\n        \n        return self.tanh(self.final(d1))\n\n# --- 2. DATASET (Single Frame Reconstruction) ---\nclass FrameDataset(Dataset):\n    def __init__(self, root_dir, img_size=256):\n        self.frames = []\n        videos = sorted(os.listdir(root_dir))\n        for vid in videos:\n            vid_path = os.path.join(root_dir, vid)\n            if not os.path.isdir(vid_path): continue\n            self.frames.extend(glob.glob(os.path.join(vid_path, '*.jpg')))\n            \n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n\n    def __len__(self): return len(self.frames)\n    def __getitem__(self, idx):\n        img = Image.open(self.frames[idx]).convert('RGB')\n        return self.transform(img)\n\n# --- 3. TRAINING ---\ndef train_denoiser():\n    print(\"Initializing ResNet Denoiser...\")\n    model = ResNetDenoiser().to(DEVICE)\n    opt = optim.Adam(model.parameters(), lr=LR)\n    criterion = nn.L1Loss() # L1 is better for Sharpness than MSE\n    \n    ds = FrameDataset(TRAIN_DIR, IMG_SIZE)\n    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    \n    print(f\"Training on {len(ds)} frames from Test Set...\")\n    \n    for epoch in range(EPOCHS):\n        model.train()\n        loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n        epoch_loss = 0\n        \n        for img in loop:\n            img = img.to(DEVICE)\n            \n            # Reconstruction Task: Input = Img, Target = Img\n            # The bottleneck naturally filters high-frequency noise\n            recon = model(img)\n            \n            loss = criterion(recon, img)\n            \n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            \n            epoch_loss += loss.item()\n            loop.set_postfix(loss=loss.item())\n            \n        # Save usually at the end\n    \n    torch.save(model.state_dict(), SAVE_NAME)\n    print(f\"Denoiser saved to {SAVE_NAME}\")\n\nif __name__ == \"__main__\":\n    train_denoiser()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\n# ================= CONFIGURATION =================\n# 1. CORRECT INPUT PATH (Point to the Avenue Dataset Test Video 02)\nINPUT_VIDEO_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos/02'\n\n# 2. OUTPUT FILE\nOUTPUT_VIDEO = 'video_02_resnet_clean.mp4'\n\n# 3. MODEL WEIGHTS\nMODEL_PATH = 'resnet_denoiser.pth'\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\n# --- 1. RESNET CLASSES (Must match training) ---\nclass ResBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1),\n            nn.BatchNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, channels, 3, padding=1),\n            nn.BatchNorm2d(channels)\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        return self.relu(x + self.conv(x))\n\nclass ResNetDenoiser(nn.Module):\n    def __init__(self):\n        super(ResNetDenoiser, self).__init__()\n        self.enc1 = nn.Sequential(nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2))\n        self.enc2 = nn.Sequential(nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2))\n        self.enc3 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2))\n        \n        self.bottleneck = nn.Sequential(ResBlock(128), ResBlock(128), ResBlock(128))\n        \n        self.dec3 = nn.Sequential(nn.ConvTranspose2d(128, 64, 2, 2), nn.ReLU())\n        self.dec2 = nn.Sequential(nn.ConvTranspose2d(64, 32, 2, 2), nn.ReLU())\n        self.dec1 = nn.Sequential(nn.ConvTranspose2d(32, 16, 2, 2), nn.ReLU())\n        \n        self.final = nn.Conv2d(16, 3, 3, padding=1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        e1 = self.enc1(x); e2 = self.enc2(e1); e3 = self.enc3(e2)\n        b = self.bottleneck(e3)\n        d3 = self.dec3(b); d2 = self.dec2(d3); d1 = self.dec1(d2)\n        return self.tanh(self.final(d1))\n\n# --- 2. CLEANING FUNCTION ---\ndef clean_and_save_video():\n    print(f\"Checking for frames in: {INPUT_VIDEO_DIR}\")\n    \n    # Robust path finding (handle if folder is named '02' or '2')\n    if not os.path.exists(INPUT_VIDEO_DIR):\n        print(f\"❌ Error: Path not found. Check if '/kaggle/input/...' path is correct.\")\n        return\n\n    frames = sorted(glob.glob(os.path.join(INPUT_VIDEO_DIR, '*.jpg')))\n    \n    if len(frames) == 0:\n        print(\"❌ Error: No .jpg frames found in directory!\")\n        print(\"Files found:\", os.listdir(INPUT_VIDEO_DIR)[:5]) # Debug print\n        return\n\n    print(f\"✅ Found {len(frames)} frames. Loading Model...\")\n\n    model = ResNetDenoiser().to(DEVICE)\n    if not os.path.exists(MODEL_PATH):\n        print(f\"❌ Error: {MODEL_PATH} not found. Did you train it?\")\n        return\n        \n    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n    model.eval()\n    \n    # Setup Video Writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, 25.0, (256, 256))\n    \n    tf = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    print(\"Processing...\")\n    with torch.no_grad():\n        for f in tqdm(frames):\n            img = Image.open(f).convert('RGB')\n            inp = tf(img).unsqueeze(0).to(DEVICE)\n            \n            clean = model(inp)\n            \n            # Post-process\n            clean_np = clean.squeeze(0).permute(1, 2, 0).cpu().numpy()\n            clean_np = (clean_np * 0.5) + 0.5\n            clean_np = np.clip(clean_np * 255, 0, 255).astype(np.uint8)\n            clean_np = cv2.cvtColor(clean_np, cv2.COLOR_RGB2BGR)\n            \n            out.write(clean_np)\n            \n    out.release()\n    print(f\"✅ Success! Saved: {OUTPUT_VIDEO}\")\n\nif __name__ == \"__main__\":\n    clean_and_save_video()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T13:56:35.060074Z","iopub.execute_input":"2025-12-30T13:56:35.060940Z","iopub.status.idle":"2025-12-30T13:56:47.815659Z","shell.execute_reply.started":"2025-12-30T13:56:35.060880Z","shell.execute_reply":"2025-12-30T13:56:47.814925Z"}},"outputs":[{"name":"stdout","text":"Checking for frames in: /kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos/02\n✅ Found 1211 frames. Loading Model...\nProcessing...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1211/1211 [00:12<00:00, 95.39it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Success! Saved: video_02_resnet_clean.mp4\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport cv2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# ================= CONFIGURATION =================\nINPUT_DIR = '/kaggle/working/cleaned_testing_videos/02'\nOUTPUT_VIDEO = 'unet_interpolation_sharp_02.mp4'\nMODEL_SAVE_PATH = 'unet_interpolator.pth'\n\nIMG_SIZE = 256\nBATCH_SIZE = 16\nEPOCHS = 40        # Interpolation learns faster\nLR = 0.0002\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# =================================================\n\n# --- 1. ARCHITECTURE (Same U-Net, Modified Input) ---\nclass AsymmetricConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super(AsymmetricConv, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(kernel_size//2, 0))\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(1, kernel_size), padding=(0, kernel_size//2))\n        self.bn = nn.BatchNorm2d(out_channels)\n    def forward(self, x): return self.relu(self.bn(self.conv2(self.relu(self.conv1(x)))))\n\nclass ResidualSkipConnection(nn.Module):\n    def __init__(self, channels):\n        super(ResidualSkipConnection, self).__init__()\n        self.block = nn.Sequential(AsymmetricConv(channels, channels), AsymmetricConv(channels, channels))\n        self.shortcut = nn.Conv2d(channels, channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x): return self.relu(self.block(x) + self.shortcut(x))\n\nclass ShortcutInceptionModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ShortcutInceptionModule, self).__init__()\n        w_6 = out_channels // 6; w_3 = out_channels // 3; w_2 = out_channels - (w_6 + w_3)\n        self.branch1 = AsymmetricConv(in_channels, w_6)\n        self.branch2 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3))\n        self.branch3 = nn.Sequential(AsymmetricConv(in_channels, w_6), AsymmetricConv(w_6, w_3), AsymmetricConv(w_3, w_2))\n        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.relu(torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1) + self.shortcut(x))\n\nclass MultiScaleUNet(nn.Module):\n    def __init__(self, in_channels=6, out_channels=3): # Input: 6 (Prev+Next), Output: 3 (Current)\n        super(MultiScaleUNet, self).__init__()\n        self.sim1 = ShortcutInceptionModule(in_channels, 96); self.pool1 = nn.MaxPool2d(2)\n        self.sim2 = ShortcutInceptionModule(96, 192);         self.pool2 = nn.MaxPool2d(2)\n        self.sim3 = ShortcutInceptionModule(192, 384);        self.pool3 = nn.MaxPool2d(2)\n        self.sim4 = ShortcutInceptionModule(384, 768)\n        self.rsc1 = nn.Sequential(*[ResidualSkipConnection(96) for _ in range(4)])\n        self.rsc2 = nn.Sequential(*[ResidualSkipConnection(192) for _ in range(3)])\n        self.rsc3 = nn.Sequential(*[ResidualSkipConnection(384) for _ in range(2)])\n        self.sim5 = ShortcutInceptionModule(768, 384);   self.up1 = nn.ConvTranspose2d(384, 384, 2, 2)\n        self.sim6 = ShortcutInceptionModule(768, 192);   self.up2 = nn.ConvTranspose2d(192, 192, 2, 2)\n        self.sim7 = ShortcutInceptionModule(384, 96);    self.up3 = nn.ConvTranspose2d(96, 96, 2, 2)\n        self.sim8 = ShortcutInceptionModule(192, 96)\n        self.final = nn.Conv2d(96, out_channels, 3, padding=1)\n        self.tanh = nn.Tanh()\n    def forward(self, x):\n        e1 = self.sim1(x); p1 = self.pool1(e1)\n        e2 = self.sim2(p1); p2 = self.pool2(e2)\n        e3 = self.sim3(p2); p3 = self.pool3(e3)\n        e4 = self.sim4(p3)\n        d1 = self.sim5(e4); u1 = self.up1(d1)\n        d2 = self.sim6(torch.cat([u1, self.rsc3(e3)], dim=1)); u2 = self.up2(d2)\n        d3 = self.sim7(torch.cat([u2, self.rsc2(e2)], dim=1)); u3 = self.up3(d3)\n        d4 = self.sim8(torch.cat([u3, self.rsc1(e1)], dim=1))\n        return self.tanh(self.final(d4))\n\n# --- 2. DATASET (The Sandwich) ---\nclass InterpolationDataset(Dataset):\n    def __init__(self, root_dir, img_size=256):\n        if not os.path.exists(root_dir):\n            parent = os.path.dirname(root_dir)\n            if os.path.exists(parent):\n                possible = [d for d in os.listdir(parent) if '02' in d]\n                if possible: root_dir = os.path.join(parent, possible[0])\n\n        self.frames = sorted(glob.glob(os.path.join(root_dir, '*.jpg')))\n        print(f\"Found {len(self.frames)} frames.\")\n        \n        self.transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        \n        # Pre-load to RAM\n        self.loaded_frames = []\n        for f in tqdm(self.frames, desc=\"Loading\"):\n            img = Image.open(f).convert('RGB')\n            self.loaded_frames.append(self.transform(img))\n            \n    def __len__(self):\n        # We need neighbors, so we skip first and last frame\n        return len(self.frames) - 2\n\n    def __getitem__(self, idx):\n        # We want to predict frame 'i' using 'i-1' and 'i+1'\n        # Dataset idx 0 corresponds to frame 1 (middle of 0,1,2)\n        center_idx = idx + 1\n        \n        prev_frame = self.loaded_frames[center_idx - 1]\n        next_frame = self.loaded_frames[center_idx + 1]\n        target_frame = self.loaded_frames[center_idx]\n        \n        # Stack inputs: (6, H, W)\n        input_stack = torch.cat([prev_frame, next_frame], dim=0)\n        \n        return input_stack, target_frame\n\n# --- 3. EXECUTION ---\ndef run_interpolation_denoising():\n    ds = InterpolationDataset(INPUT_DIR, IMG_SIZE)\n    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    \n    # Input: 6 channels (Prev RGB + Next RGB)\n    model = MultiScaleUNet(in_channels=6, out_channels=3).to(DEVICE)\n    \n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n        \n    optimizer = optim.Adam(model.parameters(), lr=LR)\n    criterion = nn.L1Loss() # L1 is sharper than MSE\n    \n    print(f\"Starting Interpolation Training ({EPOCHS} Epochs)...\")\n    \n    try:\n        for epoch in range(EPOCHS):\n            model.train()\n            loop = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n            epoch_loss = 0\n            \n            for inputs, targets in loop:\n                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n                \n                preds = model(inputs)\n                loss = criterion(preds, targets)\n                \n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                \n                epoch_loss += loss.item()\n                loop.set_postfix(loss=loss.item())\n            \n            # Uncomment to print loss if needed\n            # print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(loader):.5f}\")\n            \n    except KeyboardInterrupt:\n        print(\"\\n🛑 Interrupted! Generating video now...\")\n\n    # --- SAVE ---\n    state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n    torch.save(state_dict, MODEL_SAVE_PATH)\n    \n    # --- GENERATE & SHARPEN ---\n    print(\"Generating & Sharpening Video...\")\n    model.eval()\n    \n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, 25.0, (IMG_SIZE, IMG_SIZE))\n    \n    seq_loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False)\n    \n    with torch.no_grad():\n        for inputs, _ in tqdm(seq_loader, desc=\"Rendering\"):\n            inputs = inputs.to(DEVICE)\n            preds = model(inputs)\n            \n            for i in range(preds.size(0)):\n                img = preds[i].permute(1, 2, 0).cpu().numpy()\n                img = (img * 0.5) + 0.5\n                img = np.clip(img * 255, 0, 255).astype(np.uint8)\n                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n                \n                # --- SHARPENING TRICK ---\n                # 1. Unsharp Mask: enhance edges\n                gaussian = cv2.GaussianBlur(img, (0, 0), 2.0)\n                sharp = cv2.addWeighted(img, 1.5, gaussian, -0.5, 0)\n                \n                # 2. Detail Enhance (Optional, subtle boost)\n                # sharp = cv2.detailEnhance(sharp, sigma_s=10, sigma_r=0.15)\n                \n                out.write(sharp)\n                \n    out.release()\n    print(f\"✅ DONE! Sharp & Clean video saved to: {OUTPUT_VIDEO}\")\n\nif __name__ == \"__main__\":\n    run_interpolation_denoising()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T14:29:50.214939Z","iopub.execute_input":"2025-12-30T14:29:50.215747Z","iopub.status.idle":"2025-12-30T14:35:01.851996Z","shell.execute_reply.started":"2025-12-30T14:29:50.215715Z","shell.execute_reply":"2025-12-30T14:35:01.851267Z"}},"outputs":[{"name":"stdout","text":"Found 1211 frames.\n","output_type":"stream"},{"name":"stderr","text":"Loading: 100%|██████████| 1211/1211 [00:05<00:00, 236.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Starting Interpolation Training (40 Epochs)...\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"\n🛑 Interrupted! Generating video now...\nGenerating & Sharpening Video...\n","output_type":"stream"},{"name":"stderr","text":"Rendering: 100%|██████████| 76/76 [01:04<00:00,  1.18it/s]","output_type":"stream"},{"name":"stdout","text":"✅ DONE! Sharp & Clean video saved to: unet_interpolation_sharp_02.mp4\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nimport shutil\nimport sys\n\n# 1. SETUP WRITABLE DIRECTORIES\nWORK_DIR = \"/kaggle/working\"\nMODELS_DIR = os.path.join(WORK_DIR, \"models\")\n\n# Clean slate: Remove existing models dir if it exists to avoid conflicts\nif os.path.exists(MODELS_DIR):\n    shutil.rmtree(MODELS_DIR)\nos.makedirs(MODELS_DIR)\n\n# 2. FIND AND COPY ALL PYTHON FILES\nprint(\"🔍 Hunting for Python files in /kaggle/input...\")\n\npy_files_found = []\n\nfor root, dirs, files in os.walk(\"/kaggle/input/tap-denoise2\"):\n    for file in files:\n        if file.endswith(\".py\"):\n            source_path = os.path.join(root, file)\n            \n            # Copy to /kaggle/working/models/ (Satisfies 'from models import x')\n            dest_path_models = os.path.join(MODELS_DIR, file)\n            shutil.copy2(source_path, dest_path_models)\n            \n            # ALSO Copy to /kaggle/working/ (Satisfies direct imports if needed)\n            dest_path_root = os.path.join(WORK_DIR, file)\n            shutil.copy2(source_path, dest_path_root)\n            \n            py_files_found.append(file)\n\n# 3. CREATE __init__.py\n# This makes Python treat the directory as a package\nwith open(os.path.join(MODELS_DIR, \"__init__.py\"), \"w\") as f:\n    f.write(\"\")\n\nprint(f\"✅ Success! Moved {len(py_files_found)} files to {MODELS_DIR}\")\nprint(f\"Files: {py_files_found}\")\n\n# 4. FIX SYSTEM PATH\n# Now Python will look in /kaggle/working first\nsys.path.insert(0, WORK_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:10:32.355248Z","iopub.execute_input":"2025-12-30T18:10:32.355896Z","iopub.status.idle":"2025-12-30T18:10:32.432098Z","shell.execute_reply.started":"2025-12-30T18:10:32.355865Z","shell.execute_reply":"2025-12-30T18:10:32.431525Z"}},"outputs":[{"name":"stdout","text":"🔍 Hunting for Python files in /kaggle/input...\n✅ Success! Moved 18 files to /kaggle/working/models\nFiles: ['option_finetune_crvd.py', 'main_test_tap_crvd_outdoor.py', 'option_pretrain.py', 'module_pcd_alignment.py', 'network_isp.py', 'main_test_tap_crvd_indoor.py', 'pretrain_im_dataset.py', 'main_finetune_crvd_tap.py', 'network_nafnet.py', 'module_dcn.py', 'basicblocks.py', 'option_finetune_rgb.py', 'utils_image.py', 'utils_network.py', 'finetune_vid_dataset.py', 'utils_logger.py', 'main_finetune_rgb_tapt.py', 'main_finetune_rgb_tap.py']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:04:31.206297Z","iopub.execute_input":"2025-12-30T18:04:31.207037Z","iopub.status.idle":"2025-12-30T18:04:31.254728Z","shell.execute_reply.started":"2025-12-30T18:04:31.207006Z","shell.execute_reply":"2025-12-30T18:04:31.253847Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1395402695.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule_pcd_alignment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCDAlignment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCDAlignment_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAvgPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/tap-denoise2/module_pcd_alignment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_dcn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModulatedDeformableConv2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPCDAlignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"],"ename":"ModuleNotFoundError","evalue":"No module named 'models'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport os\nimport sys\nfrom tqdm import tqdm\n\n# --- 1. SETUP PATHS (Kaggle Uploads are usually in /kaggle/input) ---\n# We add the directory containing network_nafnet.py to python path\nsys.path.append(\"/kaggle/working\") \n\n# Try to import\ntry:\n    from network_nafnet import Baseline\nexcept ImportError:\n    # Fallback: if you uploaded raw files, they might be in current dir\n    sys.path.append(os.getcwd())\n    from network_nafnet import Baseline\n\n# --- 2. CONFIGURATION (Your fixed values) ---\nWIDTH = 64\nENC_BLKS = [2, 2, 4]\nDEC_BLKS = [2, 2, 2]\nMIDDLE_BLK = 6\nDW_EXPAND = 2\nDEVICE = torch.device('cuda') # We use the free GPU!\n\n# --- 3. PATHS ---\n# UPDATE THESE TO MATCH YOUR KAGGLE RIGHT SIDEBAR\nINPUT_VIDEO = \"/kaggle/working/rendered_videos/video_02.mp4\"\n\nWEIGHTS = \"/kaggle/input/nafnetrgb/pytorch/default/1/nafnet_rgb.pth\"\nOUTPUT_VIDEO = \"cleaned_02_gpu.mp4\"\n\ndef run_gpu_inference():\n    print(f\"🚀 powering up GPU: {torch.cuda.get_device_name(0)}\")\n    \n    model = Baseline(\n        img_channel=3, width=WIDTH, middle_blk_num=MIDDLE_BLK,\n        enc_blk_nums=ENC_BLKS, dec_blk_nums=DEC_BLKS, dw_expand=DW_EXPAND\n    )\n    \n    # Load weights\n    ckpt = torch.load(WEIGHTS, map_location=DEVICE)\n    if 'params' in ckpt: ckpt = ckpt['params']\n    elif 'state_dict' in ckpt: ckpt = ckpt['state_dict']\n    model.load_state_dict(ckpt, strict=True)\n    model.to(DEVICE).eval()\n    \n    # Process\n    cap = cv2.VideoCapture(INPUT_VIDEO)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    out = cv2.VideoWriter(OUTPUT_VIDEO, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n    \n    print(f\"Cleaning {total} frames...\")\n    \n    with torch.no_grad():\n        for _ in tqdm(range(total)):\n            ret, frame = cap.read()\n            if not ret: break\n            \n            # GPU Processing\n            img = torch.from_numpy(frame).permute(2,0,1).float().div(255.0).unsqueeze(0).to(DEVICE)\n            \n            # Since video is BGR, and model expects RGB usually, we flip\n            # But NAFNet might be agnostic. Let's stick to standard BGR->RGB flow if trained on RGB.\n            img = img.flip(1) # BGR to RGB\n            \n            output = model(img)\n            \n            output = output.squeeze(0).permute(1,2,0).cpu().numpy()\n            output = output[:, :, ::-1] # Flip back RGB to BGR\n            output = np.clip(output * 255, 0, 255).astype(np.uint8)\n            \n            out.write(output)\n            \n    cap.release()\n    out.release()\n    print(\"✅ Done! Download your video from the Output tab.\")\n\nif __name__ == \"__main__\":\n    run_gpu_inference()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:12:12.031790Z","iopub.execute_input":"2025-12-30T18:12:12.032403Z","iopub.status.idle":"2025-12-30T18:17:55.285698Z","shell.execute_reply.started":"2025-12-30T18:12:12.032377Z","shell.execute_reply":"2025-12-30T18:17:55.284914Z"}},"outputs":[{"name":"stdout","text":"🚀 powering up GPU: Tesla T4\nCleaning 1211 frames...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1211/1211 [05:42<00:00,  3.53it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Done! Download your video from the Output tab.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport cv2\nimport numpy as np\nimport os\nimport sys\nfrom tqdm import tqdm\n\n# --- CONFIGURATION ---\nINPUT_VIDEO = \"/kaggle/working/rendered_videos/video_02.mp4\"       # <--- CHECK PATH\nWEIGHTS_PATH = \"/kaggle/input/nafnetrgb/pytorch/default/1/nafnet_rgb.pth\" # <--- CHECK PATH\nOUTPUT_VIDEO = \"/kaggle/working/final_temporal_clean.mp4\"\n\n# NAFNet Config (From your weights)\nWIDTH = 64\nENC_BLKS = [2, 2, 4]\nDEC_BLKS = [2, 2, 2]\nMIDDLE_BLK = 6\nDW_EXPAND = 2\n\n# Temporal Strength (0.0 = No history, 0.5 = Strong smoothing)\n# 0.3 is a good balance to kill flicker without ghosting\nTEMPORAL_BLEND = 0.3 \n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- MODEL DEFINITION (NAFNet Baseline) ---\nclass LayerNormFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, eps):\n        ctx.eps = eps\n        N, C, H, W = x.size()\n        mu = x.mean(1, keepdim=True)\n        var = (x - mu).pow(2).mean(1, keepdim=True)\n        y = (x - mu) / (var + eps).sqrt()\n        ctx.save_for_backward(y, var, weight)\n        y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)\n        return y\n    @staticmethod\n    def backward(ctx, grad_output):\n        eps = ctx.eps\n        N, C, H, W = grad_output.size()\n        y, var, weight = ctx.saved_variables\n        g = grad_output * weight.view(1, C, 1, 1)\n        mean_g = g.mean(dim=1, keepdim=True)\n        mean_gy = (g * y).mean(dim=1, keepdim=True)\n        gx = 1. / torch.sqrt(var + eps) * (g - y * mean_gy - mean_g)\n        return gx, (grad_output * y).sum(dim=3).sum(dim=2).sum(dim=0), grad_output.sum(dim=3).sum(dim=2).sum(dim=0), None\n\nclass LayerNorm2d(nn.Module):\n    def __init__(self, channels, eps=1e-6):\n        super(LayerNorm2d, self).__init__()\n        self.register_parameter('weight', nn.Parameter(torch.ones(channels)))\n        self.register_parameter('bias', nn.Parameter(torch.zeros(channels)))\n        self.eps = eps\n    def forward(self, x):\n        return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)\n\nclass BaselineBlock(nn.Module):\n    def __init__(self, c, DW_Expand=1, FFN_Expand=2, drop_out_rate=0.):\n        super().__init__()\n        dw_channel = c * DW_Expand\n        self.conv1 = nn.Conv2d(in_channels=c, out_channels=dw_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n        self.conv2 = nn.Conv2d(in_channels=dw_channel, out_channels=dw_channel, kernel_size=3, padding=1, stride=1, groups=dw_channel, bias=True)\n        self.conv3 = nn.Conv2d(in_channels=dw_channel, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels=dw_channel, out_channels=dw_channel // 2, kernel_size=1, padding=0, stride=1, groups=1, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=dw_channel // 2, out_channels=dw_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True),\n            nn.Sigmoid()\n        )\n        self.gelu = nn.GELU()\n        ffn_channel = FFN_Expand * c\n        self.conv4 = nn.Conv2d(in_channels=c, out_channels=ffn_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n        self.conv5 = nn.Conv2d(in_channels=ffn_channel, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n        self.norm1 = LayerNorm2d(c)\n        self.norm2 = LayerNorm2d(c)\n        self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n        self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n    def forward(self, inp):\n        x = inp\n        x = self.norm1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.gelu(x)\n        x = x * self.se(x)\n        x = self.conv3(x)\n        x = self.dropout1(x)\n        y = inp + x * self.beta\n        x = self.conv4(self.norm2(y))\n        x = self.gelu(x)\n        x = self.conv5(x)\n        x = self.dropout2(x)\n        return y + x * self.gamma\n\nclass Baseline(nn.Module):\n    def __init__(self, img_channel=3, width=16, middle_blk_num=1, enc_blk_nums=[], dec_blk_nums=[], dw_expand=1, ffn_expand=2):\n        super().__init__()\n        self.intro = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1, bias=True)\n        self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1, bias=True)\n        self.encoders = nn.ModuleList()\n        self.decoders = nn.ModuleList()\n        self.middle_blks = nn.ModuleList()\n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList()\n        chan = width\n        for num in enc_blk_nums:\n            self.encoders.append(nn.Sequential(*[BaselineBlock(chan, dw_expand, ffn_expand) for _ in range(num)]))\n            self.downs.append(nn.Conv2d(chan, 2*chan, 2, 2))\n            chan = chan * 2\n        self.middle_blks = nn.Sequential(*[BaselineBlock(chan, dw_expand, ffn_expand) for _ in range(middle_blk_num)])\n        for num in dec_blk_nums:\n            self.ups.append(nn.Sequential(nn.Conv2d(chan, chan * 2, 1, bias=False), nn.PixelShuffle(2)))\n            chan = chan // 2\n            self.decoders.append(nn.Sequential(*[BaselineBlock(chan, dw_expand, ffn_expand) for _ in range(num)]))\n        self.padder_size = 2 ** len(self.encoders)\n    def forward(self, inp):\n        B, C, H, W = inp.shape\n        inp = self.check_image_size(inp)\n        x = self.intro(inp)\n        encs = []\n        for encoder, down in zip(self.encoders, self.downs):\n            x = encoder(x)\n            encs.append(x)\n            x = down(x)\n        x = self.middle_blks(x)\n        for decoder, up, enc_skip in zip(self.decoders, self.ups, encs[::-1]):\n            x = up(x)\n            x = x + enc_skip\n            x = decoder(x)\n        x = self.ending(x)\n        x = x + inp\n        return x[:, :, :H, :W]\n    def check_image_size(self, x):\n        _, _, h, w = x.size()\n        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n        return x\n\n# --- EXECUTION ---\ndef run_temporal_denoising():\n    print(\"--- Motion-Compensated NAFNet Cleaning ---\")\n    \n    # 1. Load Model\n    model = Baseline(\n        img_channel=3, width=WIDTH, middle_blk_num=MIDDLE_BLK,\n        enc_blk_nums=ENC_BLKS, dec_blk_nums=DEC_BLKS, dw_expand=DW_EXPAND\n    )\n    \n    if not os.path.exists(WEIGHTS_PATH):\n        print(\"❌ Error: Weights not found.\")\n        return\n        \n    st = torch.load(WEIGHTS_PATH, map_location=DEVICE)\n    if 'params' in st: st = st['params']\n    elif 'state_dict' in st: st = st['state_dict']\n    \n    model.load_state_dict(st, strict=True)\n    model.to(DEVICE).eval()\n    print(\"✅ Model Loaded\")\n    \n    # 2. Video IO\n    cap = cv2.VideoCapture(INPUT_VIDEO)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    out = cv2.VideoWriter(OUTPUT_VIDEO, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n    \n    # 3. Processing Loop\n    prev_clean = None\n    prev_gray = None\n    \n    print(f\"Processing {total} frames...\")\n    \n    with torch.no_grad():\n        for i in tqdm(range(total)):\n            ret, frame = cap.read()\n            if not ret: break\n            \n            # A. Spatial Denoise (NAFNet)\n            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            img_tensor = torch.from_numpy(img_rgb.astype(np.float32) / 255.0)\n            img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n            \n            clean_tensor = model(img_tensor)\n            \n            # To numpy (H, W, 3)\n            curr_clean = clean_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n            curr_clean = np.clip(curr_clean * 255, 0, 255).astype(np.uint8)\n            curr_clean_bgr = cv2.cvtColor(curr_clean, cv2.COLOR_RGB2BGR)\n            \n            # B. Temporal Fusion (Optical Flow)\n            curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            \n            if prev_clean is not None:\n                # 1. Calculate Flow (Where did pixels move?)\n                flow = cv2.calcOpticalFlowFarneback(\n                    prev_gray, curr_gray, None, \n                    pyr_scale=0.5, levels=3, winsize=15, \n                    iterations=3, poly_n=5, poly_sigma=1.2, flags=0\n                )\n                \n                # 2. Warp Previous Clean Frame to Current Position\n                # (Create grid for remapping)\n                h, w = flow.shape[:2]\n                flow_map = np.column_stack((np.repeat(np.arange(h), w), np.tile(np.arange(w), h)))\n                flow_map = flow_map.reshape(h, w, 2).astype(np.float32)\n                \n                # Add flow to grid (Remember: flow is dx, dy)\n                map_x = flow_map[..., 1] + flow[..., 0]\n                map_y = flow_map[..., 0] + flow[..., 1]\n                \n                warped_prev = cv2.remap(prev_clean, map_x, map_y, cv2.INTER_LINEAR)\n                \n                # 3. Blend (Remove Flicker)\n                # If flow error is high (occlusion), trust current frame more\n                # For simplicity, we use fixed alpha which works well for grain\n                final_frame = cv2.addWeighted(curr_clean_bgr, 1.0 - TEMPORAL_BLEND, warped_prev, TEMPORAL_BLEND, 0)\n            else:\n                final_frame = curr_clean_bgr\n            \n            # Save for next iter\n            prev_clean = final_frame\n            prev_gray = curr_gray\n            \n            out.write(final_frame)\n            \n    cap.release()\n    out.release()\n    print(f\"✅ Finished! Saved to {OUTPUT_VIDEO}\")\n\nif __name__ == \"__main__\":\n    run_temporal_denoising()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:31:26.574939Z","iopub.execute_input":"2025-12-30T18:31:26.575765Z","iopub.status.idle":"2025-12-30T18:38:25.626320Z","shell.execute_reply.started":"2025-12-30T18:31:26.575734Z","shell.execute_reply":"2025-12-30T18:38:25.625741Z"}},"outputs":[{"name":"stdout","text":"--- Motion-Compensated NAFNet Cleaning ---\n✅ Model Loaded\nProcessing 1211 frames...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1211/1211 [06:58<00:00,  2.89it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Finished! Saved to /kaggle/working/final_temporal_clean.mp4\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# 1. Clone the Repo\n!git clone https://github.com/m-tassano/fastdvdnet.git\n%cd fastdvdnet\n\n# 2. Install Dependencies (Kaggle usually has these, but just in case)\n!pip install -q torch torchvision opencv-python\n\n# 3. Download Pre-trained Weights (Model.pth)\n# We use the 'denoising' weights trained on high noise\n!mkdir -p model\n!wget -O model/model.pth https://github.com/m-tassano/fastdvdnet/raw/master/model.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T18:44:59.618244Z","iopub.execute_input":"2025-12-30T18:44:59.618785Z","iopub.status.idle":"2025-12-30T18:45:06.855041Z","shell.execute_reply.started":"2025-12-30T18:44:59.618756Z","shell.execute_reply":"2025-12-30T18:45:06.854321Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'fastdvdnet'...\nremote: Enumerating objects: 145, done.\u001b[K\nremote: Counting objects: 100% (36/36), done.\u001b[K\nremote: Compressing objects: 100% (27/27), done.\u001b[K\nremote: Total 145 (delta 21), reused 12 (delta 9), pack-reused 109 (from 1)\u001b[K\nReceiving objects: 100% (145/145), 34.97 MiB | 42.38 MiB/s, done.\nResolving deltas: 100% (71/71), done.\n/kaggle/working/fastdvdnet\n--2025-12-30 18:45:05--  https://github.com/m-tassano/fastdvdnet/raw/master/model.pth\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/m-tassano/fastdvdnet/master/model.pth [following]\n--2025-12-30 18:45:06--  https://raw.githubusercontent.com/m-tassano/fastdvdnet/master/model.pth\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9971551 (9.5M) [application/octet-stream]\nSaving to: ‘model/model.pth’\n\nmodel/model.pth     100%[===================>]   9.51M  --.-KB/s    in 0.1s    \n\n2025-12-30 18:45:06 (94.8 MB/s) - ‘model/model.pth’ saved [9971551/9971551]\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nimport cv2\nimport shutil\nimport glob\nimport re\n\n# ================= CONFIGURATION =================\n# Path to your messy frames (e.g. 1.jpg, 10.jpg...)\nINPUT_FRAMES_DIR = \"/kaggle/working/cleaned_testing_videos/02\"  \n\nNOISE_SIGMA = 50 \nMAX_FRAMES = 2000\nOUTPUT_VIDEO_PATH = \"/kaggle/working/cleaned_fastdvdnet_ordered.mp4\"\n# =================================================\n\ndef natural_sort_key(s):\n    \"\"\"Sorts string with embedded numbers correctly (1, 2, ... 10) instead of (1, 10, 2)\"\"\"\n    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n\ndef prepare_ordered_input(src_dir, dest_dir):\n    \"\"\"Copies frames to a new folder and renames them 00001.jpg, 00002.jpg...\"\"\"\n    if os.path.exists(dest_dir): shutil.rmtree(dest_dir)\n    os.makedirs(dest_dir)\n    \n    # Get all images\n    files = glob.glob(os.path.join(src_dir, \"*\"))\n    files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    \n    # CRITICAL: Sort them naturally (1, 2, 3...)\n    files.sort(key=lambda f: natural_sort_key(os.path.basename(f)))\n    \n    print(f\"🧹 Re-ordering {len(files)} frames...\")\n    print(f\"   First 3: {[os.path.basename(f) for f in files[:3]]}\")\n    \n    for i, file_path in enumerate(files):\n        # Rename to 00001.jpg, 00002.jpg, etc.\n        ext = os.path.splitext(file_path)[1]\n        new_name = f\"{i:05d}{ext}\" \n        shutil.copy2(file_path, os.path.join(dest_dir, new_name))\n        \n    return dest_dir\n\ndef install_dependencies():\n    print(\"📦 Installing dependencies (tensorboardX)...\")\n    os.system(\"pip install tensorboardX\")\n\ndef patch_fastdvdnet_code():\n    utils_path = \"fastdvdnet/utils.py\"\n    if not os.path.exists(utils_path): return\n    with open(utils_path, \"r\") as f: code = f.read()\n    new_code = code.replace(\"from skimage.measure.simple_metrics import compare_psnr\", \"from skimage.metrics import peak_signal_noise_ratio as compare_psnr\")\n    with open(utils_path, \"w\") as f: f.write(new_code)\n    print(\"✅ Patch applied.\")\n\ndef run_pipeline():\n    # 1. SETUP\n    install_dependencies()\n    if not os.path.exists(\"fastdvdnet\"):\n        print(\"📥 Cloning FastDVDnet...\")\n        os.system(\"git clone https://github.com/m-tassano/fastdvdnet.git\")\n    patch_fastdvdnet_code()\n    \n    if not os.path.exists(\"fastdvdnet/model/model.pth\"):\n        print(\"⬇️ Downloading weights...\")\n        os.makedirs(\"fastdvdnet/model\", exist_ok=True)\n        os.system(\"wget -O fastdvdnet/model/model.pth https://github.com/m-tassano/fastdvdnet/raw/master/model.pth\")\n\n    # 2. PREPARE INPUT (THE FIX)\n    # We copy frames to a 'sorted_input' folder first\n    sorted_input_dir = \"/kaggle/working/temp_sorted_input\"\n    prepare_ordered_input(INPUT_FRAMES_DIR, sorted_input_dir)\n\n    # 3. RUN INFERENCE\n    print(f\"🚀 Running FastDVDnet...\")\n    temp_out = \"/kaggle/working/temp_results\"\n    if os.path.exists(temp_out): shutil.rmtree(temp_out)\n    os.makedirs(temp_out)\n\n    cmd = (\n        f\"cd fastdvdnet && \"\n        f\"python test_fastdvdnet.py \"\n        f\"--test_path {sorted_input_dir} \" # Use the SORTED folder\n        f\"--noise_sigma {NOISE_SIGMA} \"\n        f\"--save_path {temp_out} \"\n        f\"--max_num_fr_per_seq {MAX_FRAMES}\"\n    )\n    \n    if os.system(cmd) != 0:\n        print(\"❌ FastDVDnet failed.\")\n        return\n\n    # 4. STITCH VIDEO\n    print(\"🎬 Stitching frames...\")\n    \n    # Find output (FastDVDnet creates a subdir named 'temp_sorted_input')\n    processed_dir = os.path.join(temp_out, \"temp_sorted_input\")\n    if not os.path.exists(processed_dir): processed_dir = temp_out\n        \n    frames = sorted(glob.glob(os.path.join(processed_dir, \"*\")))\n    # Since we renamed inputs to 00001.jpg, the output is guaranteed to sort correctly now!\n    \n    if not frames:\n        print(\"❌ No output frames found.\")\n        return\n        \n    sample = cv2.imread(frames[0])\n    h, w, _ = sample.shape\n    out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, cv2.VideoWriter_fourcc(*'mp4v'), 25.0, (w, h))\n    \n    for f in frames:\n        out.write(cv2.imread(f))\n    out.release()\n    \n    # Cleanup\n    shutil.rmtree(temp_out)\n    shutil.rmtree(sorted_input_dir)\n    print(f\"✅ SUCCESS! Correctly ordered video: {OUTPUT_VIDEO_PATH}\")\n\nif __name__ == \"__main__\":\n    run_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:09:00.310609Z","iopub.execute_input":"2025-12-30T19:09:00.311257Z","iopub.status.idle":"2025-12-30T19:12:00.732791Z","shell.execute_reply.started":"2025-12-30T19:09:00.311230Z","shell.execute_reply":"2025-12-30T19:12:00.731886Z"}},"outputs":[{"name":"stdout","text":"📦 Installing dependencies (tensorboardX)...\nRequirement already satisfied: tensorboardX in /usr/local/lib/python3.12/dist-packages (2.6.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (2.0.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (25.0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (5.29.5)\n✅ Patch applied.\n🧹 Re-ordering 1211 frames...\n   First 3: ['frame_00000.jpg', 'frame_00001.jpg', 'frame_00002.jpg']\n🚀 Running FastDVDnet...\n","output_type":"stream"},{"name":"stderr","text":"INFO:testlog:Finished denoising /kaggle/working/temp_sorted_input\nINFO:testlog:\tDenoised 1211 frames in 119.965s, loaded seq in 7.591s\nINFO:testlog:\tPSNR noisy 14.1514dB, PSNR result 30.0900dB\n","output_type":"stream"},{"name":"stdout","text":"\n### Testing FastDVDnet model ###\n> Parameters:\n\tmodel_file: ./model.pth\n\ttest_path: /kaggle/working/temp_sorted_input\n\tsuffix: \n\tmax_num_fr_per_seq: 2000\n\tnoise_sigma: 0.19607843137254902\n\tdont_save_results: False\n\tsave_noisy: False\n\tno_gpu: False\n\tsave_path: /kaggle/working/temp_results\n\tgray: False\n\tcuda: True\n\n\nLoading models ...\n\tOpen sequence in folder:  /kaggle/working/temp_sorted_input\n🎬 Stitching frames...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2515115298.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_55/2515115298.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_VIDEO_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoWriter_fourcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m'mp4v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"],"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'shape'","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"import cv2\nimport glob\nimport os\nimport re\n\n# ================= CONFIGURATION =================\n# Path where FastDVDnet dumped the results\n# Based on your logs, it should be here:\nRESULTS_DIR = \"/kaggle/working/temp_results/temp_sorted_input\" \nOUTPUT_VIDEO = \"/kaggle/working/cleaned_fastdvdnet_final.mp4\"\n# =================================================\n\ndef natural_sort_key(s):\n    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n\ndef repair_video():\n    print(f\"🕵️ Looking for frames in: {RESULTS_DIR}\")\n    \n    if not os.path.exists(RESULTS_DIR):\n        print(f\"❌ Error: Folder not found. Did the previous step delete it?\")\n        # Fallback check\n        alt_path = \"/kaggle/working/temp_results\"\n        print(f\"   Checking fallback: {alt_path}\")\n        if os.path.exists(alt_path):\n            files = glob.glob(os.path.join(alt_path, \"*.png\"))\n            if len(files) > 0:\n                print(\"   Found frames in root temp_results!\")\n                process_stitching(alt_path)\n                return\n        return\n\n    process_stitching(RESULTS_DIR)\n\ndef process_stitching(folder_path):\n    # 1. Strict Filtering (Only PNG/JPG)\n    # This prevents the error you just got\n    search_path = os.path.join(folder_path, \"*\")\n    all_files = glob.glob(search_path)\n    \n    frames = [f for f in all_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    \n    # 2. Sort Correctly\n    frames.sort(key=lambda f: natural_sort_key(os.path.basename(f)))\n    \n    if not frames:\n        print(\"❌ No image files found to stitch.\")\n        return\n\n    print(f\"✅ Found {len(frames)} valid frames.\")\n    print(f\"   First: {os.path.basename(frames[0])}\")\n    print(f\"   Last:  {os.path.basename(frames[-1])}\")\n\n    # 3. Initialize Video Writer\n    first_frame = cv2.imread(frames[0])\n    if first_frame is None:\n        print(f\"❌ Critical: Could not read first frame: {frames[0]}\")\n        return\n        \n    height, width, _ = first_frame.shape\n    fps = 25.0\n    \n    print(f\"🎬 Stitching {width}x{height} video...\")\n    \n    out = cv2.VideoWriter(OUTPUT_VIDEO, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n    \n    count = 0\n    for f_path in frames:\n        img = cv2.imread(f_path)\n        if img is None:\n            print(f\"⚠️ Warning: Skipping unreadable file: {f_path}\")\n            continue\n        out.write(img)\n        count += 1\n        \n    out.release()\n    print(f\"🎉 SUCCESS! Video saved to: {OUTPUT_VIDEO}\")\n\nif __name__ == \"__main__\":\n    repair_video()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:16:33.265550Z","iopub.execute_input":"2025-12-30T19:16:33.265976Z","iopub.status.idle":"2025-12-30T19:16:42.519365Z","shell.execute_reply.started":"2025-12-30T19:16:33.265947Z","shell.execute_reply":"2025-12-30T19:16:42.518616Z"}},"outputs":[{"name":"stdout","text":"🕵️ Looking for frames in: /kaggle/working/temp_results/temp_sorted_input\n❌ Error: Folder not found. Did the previous step delete it?\n   Checking fallback: /kaggle/working/temp_results\n   Found frames in root temp_results!\n✅ Found 1211 valid frames.\n   First: n50_FastDVDnet_0.png\n   Last:  n50_FastDVDnet_1210.png\n🎬 Stitching 640x360 video...\n🎉 SUCCESS! Video saved to: /kaggle/working/cleaned_fastdvdnet_final.mp4\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}